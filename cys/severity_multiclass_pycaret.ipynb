{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28551a76",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Multiclass Classification PyCaret Model for Panic Severity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc63ef",
   "metadata": {},
   "source": [
    "author:  `@cyshin971`  \n",
    "\n",
    "date:    `2025-06-xx`  \n",
    "\n",
    "version: `1-0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a49b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1-0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fec94e",
   "metadata": {},
   "source": [
    "# üìö | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "from library.pandas_utils import move_column, remove_columns, create_empty_df, read_csv\n",
    "from library.text_utils import save_as_csv\n",
    "from library.json_utils import save_dict_to_file, load_dict_from_file\n",
    "from library.path_utils import get_file_path\n",
    "\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bf1ba",
   "metadata": {},
   "source": [
    "# üìÅ | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61873106",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./_data\"\n",
    "TMP_PATH = \"./cys/_tmp\"\n",
    "OUTPUT_PATH = \"./cys/_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4473c",
   "metadata": {},
   "source": [
    "# üåê | Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUTPUT:\n",
    "    num_classes = 3\n",
    "    class_names = ['Mild', 'Moderate', 'Severe']\n",
    "    \n",
    "    label2name = dict(enumerate(class_names))\n",
    "    name2label = {v: k for k, v in label2name.items()}\n",
    "    \n",
    "    output_dict = {\n",
    "\t\t1: 'Mild',\n",
    "\t\t2: 'Mild',\n",
    "\t\t3: 'Moderate',\n",
    "\t\t4: 'Severe',\n",
    "\t\t5: 'Severe'\n",
    "\t}\n",
    "    \n",
    "    output_dict_inv = {v: k for k, v in output_dict.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_label_name(label):\n",
    "        return OUTPUT.label2name[label]\n",
    "    @staticmethod\n",
    "    def get_label_from_name(name):\n",
    "        return OUTPUT.name2label[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172b789",
   "metadata": {},
   "source": [
    "# ‚öíÔ∏è | Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4319fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\tfeatures_dict = load_dict_from_file(OUTPUT_PATH, 'panic_features_dict')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {get_file_path(OUTPUT_PATH, 'panic_features_dict')}. Please run data_analysis.ipynb first.\")\n",
    "print(f\"Loaded features dict with {len(features_dict)} keys:\")\n",
    "scraped_data_filename = None\n",
    "for k, v in features_dict.items():\n",
    "    if k == 'scraped_data_filename':\n",
    "        print(f\"  {k}: {v}.csv\")\n",
    "        scraped_data_filename = v\n",
    "    elif k == 'preproc_version':\n",
    "        preproc_version = v\n",
    "    elif k == 'analysis_version':\n",
    "        analysis_version = v\n",
    "    else:\n",
    "        print(f'{k}: {features_dict[k]}')\n",
    "\n",
    "if scraped_data_filename is None:\n",
    "\traise ValueError(\"scraped_data_filename not found in features_dict\")\n",
    "\n",
    "pre_data = read_csv(get_file_path(OUTPUT_PATH, f'panic_pre_data_filled_{analysis_version}({scraped_data_filename}).csv'))\n",
    "display(pre_data.head(5))\n",
    "metadata = read_csv(get_file_path(OUTPUT_PATH, f'panic_metadata_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(metadata.head(5))\n",
    "demography_data = read_csv(get_file_path(OUTPUT_PATH, f'panic_demography_data_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(demography_data.head(5))\n",
    "patient_data = read_csv(get_file_path(OUTPUT_PATH, f'panic_patient_analysis_{analysis_version}({scraped_data_filename}).csv'))\n",
    "display(patient_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f554d4",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d373b39",
   "metadata": {},
   "source": [
    "# üîÑÔ∏è | Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852adcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbp_param = 3\n",
    "\n",
    "filtered_metadata = create_empty_df()\n",
    "filtered_pre_data = create_empty_df()\n",
    "proc_data_init = create_empty_df()\n",
    "\n",
    "filtered_panic_metadata = metadata[metadata['panic_label'] == 1].copy()\n",
    "print(f\"Found {len(filtered_panic_metadata)} entries with panic label.\")\n",
    "filtered_panic_metadata_entry_ids = filtered_panic_metadata[filtered_panic_metadata[f'valid_entry_{dbp_param}'] == 1]['entry_id'].unique()\n",
    "\n",
    "proc_data_init = metadata[metadata['entry_id'].isin(filtered_panic_metadata_entry_ids)].copy()\n",
    "\n",
    "print(f\"Found {len(filtered_panic_metadata_entry_ids)} entries with panic label and at least {dbp_param} days of prior data.\")\n",
    "filtered_metadata = metadata[metadata['ref_event_id'].isin(filtered_panic_metadata_entry_ids)].copy()\n",
    "print(f\"Filtered metadata contains {len(filtered_metadata)} entries with panic label and at least {dbp_param} days of prior data.\")\n",
    "unique_dbp = filtered_metadata['dbp'].unique()\n",
    "if len(unique_dbp) != 3:\n",
    "\traise ValueError(f\"Expected 3 unique DBP values, found {len(unique_dbp)}: {unique_dbp}\")\n",
    "del filtered_panic_metadata, filtered_panic_metadata_entry_ids, unique_dbp\n",
    "\n",
    "filtered_entry_ids = filtered_metadata['entry_id'].unique()\n",
    "filtered_panic_entry_ids = filtered_metadata['ref_event_id'].unique()\n",
    "filtered_pre_data = pre_data[pre_data['entry_id'].isin(filtered_entry_ids)].copy()\n",
    "if len(filtered_pre_data) != len(filtered_metadata):\n",
    "\traise ValueError(f\"Filtered pre_data length {len(filtered_pre_data)} does not match filtered_metadata length {len(filtered_metadata)}\")\n",
    "print(f\"Filtered data contains {len(filtered_panic_entry_ids)} unique panic events and {len(filtered_entry_ids)} unique entry IDs.\")\n",
    "print(f\"Filtered pre_data contains {len(filtered_pre_data['ID'].unique())} unique IDs.\")\n",
    "del filtered_entry_ids\n",
    "\n",
    "proc_data_init = proc_data_init[features_dict['id']+features_dict['label']].copy()\n",
    "print(f\"Initial processed data contains {len(proc_data_init)} entries with {len(proc_data_init.columns)} columns.\")\n",
    "display(proc_data_init.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data_int = create_empty_df()\n",
    "proc_data_int = proc_data_init.copy()\n",
    "\n",
    "# remove 'severity' from features_dict['dailylog]\n",
    "features_dict['dailylog'] = [f for f in features_dict['dailylog'] if f != 'severity']\n",
    "\n",
    "# use demography data to add demographic features to proc_data using ID (multiple entries per ID)\n",
    "proc_data_int = pd.merge(proc_data_int, demography_data, on='ID', how='left')\n",
    "print(f\"Processed data after merging with demography data contains {len(proc_data_int)} entries with {len(proc_data_int.columns)} columns.\"\t)\n",
    "\n",
    "for i in range(1, dbp_param + 1):\n",
    "    # make a dictionary of 'entry_id' : 'ref_event_id' for the current dbp\n",
    "\tdbp_dict = filtered_metadata[filtered_metadata['dbp'] == i].set_index('entry_id')['ref_event_id'].to_dict()\n",
    "\tprint(f\"Processing data for {i} days before panic.\")\n",
    "\n",
    "\tentry_ids = dbp_dict.keys()\n",
    "\tfiltered_pre_data_i = filtered_pre_data[filtered_pre_data['entry_id'].isin(entry_ids)].copy()\n",
    "\tif len(filtered_pre_data_i) != len(dbp_dict.keys()):\n",
    "\t\traise ValueError(f\"Filtered pre_data length {len(filtered_pre_data_i)} does not match filtered_metadata length {len(dbp_dict.keys())} for {i} days before panic\")\n",
    "  \t# Update 'entry_id' in filtered_pre_data_i to the corresponding 'ref_event_id' from dbp_dict\n",
    "\tfiltered_pre_data_i['entry_id'] = filtered_pre_data_i['entry_id'].map(dbp_dict)\n",
    "\t\n",
    "\tfeatures_list = ['entry_id']+features_dict['dailylog']+features_dict['lifelog']\n",
    "\tif i == dbp_param:\n",
    "\t\tfeatures_list += features_dict['questionnaire']\n",
    "\tfiltered_pre_data_i = filtered_pre_data_i[features_list].copy()\n",
    "\t# rename ALL non-ID columns to include the suffix\n",
    "\tcols_to_rename = [c for c in filtered_pre_data_i.columns if c != 'entry_id']\n",
    "\trename_map = {c: f\"{c}_{i}\" for c in cols_to_rename}\n",
    "\tfiltered_pre_data_i.rename(columns=rename_map, inplace=True)\n",
    "\t\n",
    "\tproc_data_int = pd.merge(proc_data_int, filtered_pre_data_i, on='entry_id', how='left', suffixes=('', f'_{i}'))\n",
    "\n",
    "# Use OUTPUT.output_dict to map severity labels\n",
    "proc_data_int['severity'] = proc_data_int['severity'].map(OUTPUT.output_dict)\n",
    "\n",
    "# save_as_csv(proc_data_int, TMP_PATH, f'proc_data_{dbp_param}days', index=False)\n",
    "# display(proc_data_int.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data = create_empty_df()\n",
    "proc_data = proc_data_int.copy()\n",
    "\n",
    "r_cols = ['panic',\n",
    "          'dbp',\n",
    "          'panic_label']\n",
    "remove_columns(proc_data, r_cols)\n",
    "move_column(proc_data, 'severity', -1)\n",
    "display(proc_data.head(5))\n",
    "save_as_csv(proc_data, OUTPUT_PATH, f'panic_severity_multi_proc_data_{dbp_param}days_{version}({scraped_data_filename})', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49957b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(proc_data['severity'], proc_data['dataset'], margins=True, margins_name='Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01af17",
   "metadata": {},
   "source": [
    "# ü§ñ | Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = proc_data.copy()\n",
    "remove_columns(data, features_dict['id'])\n",
    "display(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249402fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize PyCaret setup\n",
    "clf = setup(\n",
    "    data=data,\n",
    "    target='severity',           # replace with your target column name\n",
    "    session_id=123,              # for reproducibility\n",
    "    normalize=True,              # scale numeric features\n",
    "    transformation=False,        # turn off power transformation\n",
    "    train_size=0.8,              # 80/20 split\n",
    "    fold=5,                      # 5-fold cross-validation\n",
    "    fold_strategy='stratifiedkfold',\n",
    "    numeric_imputation='mean',\n",
    "    remove_multicollinearity=True,   # for small datasets, this is often helpful\n",
    "\tmulticollinearity_threshold=0.9, # threshold for removing multicollinear features\n",
    "\t# html=False,                # do not generate HTML report (use plain-text output)\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4eea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compare baseline models and select the best by Accuracy\n",
    "best_model = compare_models(sort='Accuracy')\n",
    "\n",
    "results = pull()  # Get the latest output table as a DataFrame\n",
    "# Cross-Validation results\n",
    "print(\"Cross-Validation Results:\")\n",
    "display(results)  # Jupyter display (can further style if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48196744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate on hold-out set (20% test split)\n",
    "holdout_results = predict_model(best_model)\n",
    "for i in range(1, dbp_param + 1):\n",
    "\tfor col in features_dict['dailylog']+ features_dict['lifelog']:\n",
    "\t\tremove_columns(holdout_results, [f\"{col}_{i}\"])\n",
    "\tif i == dbp_param:\n",
    "\t\tfor col in features_dict['questionnaire']:\n",
    "\t\t\tremove_columns(holdout_results, [f\"{col}_{i}\"])\n",
    "print(\"Hold-out set performance:\")\n",
    "display(holdout_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b468b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a specific model (e.g., LightGBM)\n",
    "model = create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Tune the model hyperparameters\n",
    "tuned_model = tune_model(model, optimize='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Ensemble models (optional)\n",
    "blended_model = blend_models([tuned_model, best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eddbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Finalize the model for deployment\n",
    "final_model = finalize_model(blended_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b591b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Use the finalized model on brand-new data\n",
    "# new_data = pd.read_csv('new_data.csv')\n",
    "# new_predictions = predict_model(final_model, data=new_data)\n",
    "\n",
    "# 10. Save the finalized model for later use\n",
    "# save_model(final_model, 'final_pycaret_multiclass_model')\n",
    "\n",
    "# To load the saved model:\n",
    "# loaded_model = load_model('final_pycaret_multiclass_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef324a",
   "metadata": {},
   "source": [
    "# üöÇ | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c65797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = setup(data, target = target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cc8b6",
   "metadata": {},
   "source": [
    "# üìã | Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = compare_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhlab-panic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
