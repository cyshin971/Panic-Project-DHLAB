{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f4fd25",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5634a08",
   "metadata": {},
   "source": [
    "author:  `@cyshin971`  \n",
    "\n",
    "date:    `2025-07-02`  \n",
    "\n",
    "version: `2.3`\n",
    " \n",
    "> version `2.0`: imputation separated from `data_analysis.ipynb` (version `1.3`)  \n",
    "> version `2.1`: Questionnaire forward filling added (previously implemented in scraping stage) \n",
    "\n",
    "> version `2.2`: Replaced previous imputation methods with methods agreed on `20250625`  \n",
    ">  - Questionnaire 'first come method' imputation entirely moved from scraping to this notebook (`data_imputation.ipynb`)\n",
    ">  - Added \"Growing Average Imputation\" for `lifelog`, `mood` features\n",
    ">  - Implemented zero fill imputation for `dailylog_life` features   \n",
    "\n",
    "> version `2.3`: Added `use_growing_avg` and `null_default_zero` to switch b/w growing average to group average imputation and switch b/w zero fill and average fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '2-3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42654e1",
   "metadata": {},
   "source": [
    "# üìö | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "from library.pandas_utils import move_column, remove_columns, aggregate_by_column, create_empty_df, read_csv\n",
    "from library.text_utils import save_as_csv\n",
    "from library.json_utils import save_dict_to_file, load_dict_from_file\n",
    "from library.path_utils import get_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a73f4",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è | Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_scraped_data_filename = None # Keep as None if you don't want to manually specify a file\n",
    "\n",
    "use_growing_avg = True\n",
    "# True: will use a growing average for the patient, i.e. the average of all previous entries\n",
    "# False: will use a fixed average for the patient, i.e. the average of all entries for that patient\n",
    "\n",
    "null_default_zero = True\n",
    "# True: will replace null values with no existing entries for that patient with 0\n",
    "# False: will replace null values with no existing entries for that patient with the patient average or global average\n",
    "\n",
    "# NOTE: 20250625 consensus\n",
    "# use_growing_avg = True\n",
    "# null_default_zero = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa403ca",
   "metadata": {},
   "source": [
    "# üìÅ | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./_data\"\n",
    "TMP_PATH = \"./cys/_tmp\"\n",
    "OUT_PATH = \"./cys/_output\"\n",
    "\n",
    "try:\n",
    "\tfeatures_dict = load_dict_from_file(OUT_PATH, 'panic_features_dict')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {get_file_path(OUT_PATH, 'panic_features_dict')}. Please run data_analysis.ipynb first.\")\n",
    "print(f\"Loaded features dict with {len(features_dict)} keys:\")\n",
    "scraped_data_filename = None\n",
    "for k, v in features_dict.items():\n",
    "    if k == 'scraped_data_filename':\n",
    "        print(f\"  {k}: {v}.csv\")\n",
    "        scraped_data_filename = v\n",
    "    elif k == 'preproc_version':\n",
    "        preproc_version = v\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "if scraped_data_filename is None:\n",
    "\traise ValueError(\"scraped_data_filename not found in features_dict\")\n",
    "if manual_scraped_data_filename is not None:\n",
    "    logging.warning(f\"Using manually specified scraped_data_filename: {manual_scraped_data_filename}. If this is not intended, please set it to None.\")\n",
    "    scraped_data_filename = manual_scraped_data_filename\n",
    "\n",
    "features_dict['imputation_version'] = version\n",
    "save_dict_to_file(features_dict, OUT_PATH, 'panic_features_dict')\n",
    "\n",
    "PREPROC_PATH = f\"{OUT_PATH}/{scraped_data_filename}/preprocessed\"\n",
    "OUTPUT_PATH = f\"{OUT_PATH}/{scraped_data_filename}/imputed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024277b",
   "metadata": {},
   "source": [
    "# ‚öíÔ∏è | Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1f06",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = read_csv(get_file_path(PREPROC_PATH, f'panic_pre_data_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(pre_data.head(5))\n",
    "metadata = read_csv(get_file_path(PREPROC_PATH, f'panic_metadata_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(metadata.head(5))\n",
    "demography_data = read_csv(get_file_path(PREPROC_PATH, f'panic_demography_data_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(demography_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdce151",
   "metadata": {},
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9355d",
   "metadata": {},
   "source": [
    "## Questionnaire\n",
    "\n",
    "- The null values were forward filled from the first value in the scraped data\n",
    "- the entries with null values prior to the first value was backward filled\n",
    "- Subsequent entries were forward filled\n",
    "- other null values were set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_filled_questionnaire = create_empty_df()\n",
    "pre_data_filled_questionnaire = pre_data.copy()\n",
    "\n",
    "pre_data_filled_questionnaire.sort_values(by=['ID', 'date'], inplace=True)\n",
    "\n",
    "for qcol in features_dict['questionnaire']:\n",
    "    pre_data_filled_questionnaire[qcol] = (\n",
    "        pre_data_filled_questionnaire.groupby('ID')[qcol]\n",
    "            .apply(lambda s: s.ffill().bfill())\n",
    "            .fillna(0)\n",
    "            .values\n",
    "    )\n",
    "\n",
    "pre_data_filled_questionnaire.sort_values(by=['ID', 'date'], inplace=True)\n",
    "disp_df = pre_data_filled_questionnaire[pre_data_filled_questionnaire['ID'] == 'SYM1-1-380'].copy()\n",
    "display(disp_df[features_dict['id']+features_dict['questionnaire']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d45e2",
   "metadata": {},
   "source": [
    "## Growing Average Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53ac46",
   "metadata": {},
   "source": [
    "- Initial fill: All NaNs get 0.\n",
    "- By ID: For each ID, process their entries in chronological order (date).\n",
    "- Running average: Each time a valid value is encountered, add it to the running sum/count.\n",
    "- For every NaN (now 0, but you detect it with the original mask), fill with running average so far.\n",
    "- No values for that ID: Remain at 0 (since running_count stays at 0).\n",
    "- After each value: The average is updated so the imputation logic matches your requirements.\n",
    "- No backward fill: Only forward imputation based on past data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def growing_average_impute(\n",
    "    df,\n",
    "    group_col,\n",
    "    target_cols,\n",
    "    default_fill_zero=True,\n",
    "    prefill_value=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Impute NaN values in target columns by growing average within each group.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame to impute.\n",
    "        group_col (str): Grouping column (e.g., patient ID).\n",
    "        target_cols (list): Columns to impute.\n",
    "        default_fill_zero (bool): If True, use 0.0 as prefill before first value; else use group mean or prefill_value.\n",
    "        prefill_value (float, optional): Custom value to use as prefill (takes precedence over group mean if provided).\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.sort_values(by=[group_col, 'date'], inplace=True)\n",
    "\n",
    "    # Precompute group means and global means if needed\n",
    "    if not default_fill_zero:\n",
    "        group_means = df.groupby(group_col)[target_cols].mean()\n",
    "    global_means = df[target_cols].mean()\n",
    "\n",
    "    for col in target_cols:\n",
    "        for pid, sub in df.groupby(group_col):\n",
    "            vals = sub[col].values\n",
    "            mask = ~np.isnan(vals)\n",
    "\n",
    "            running_sum = 0.0\n",
    "            running_count = 0\n",
    "\n",
    "            # Determine fill value for \"pre-first\" NaNs\n",
    "            if default_fill_zero:\n",
    "                fill_val = 0.0\n",
    "            elif prefill_value is not None:\n",
    "                fill_val = prefill_value\n",
    "            else:\n",
    "                group_mean = group_means.loc[pid, col] if pid in group_means.index else np.nan\n",
    "                # If group mean is nan or missing, fallback to global mean, else 0.0\n",
    "                if not np.isnan(group_mean):\n",
    "                    fill_val = group_mean\n",
    "                elif not np.isnan(global_means[col]):\n",
    "                    fill_val = global_means[col]\n",
    "                else:\n",
    "                    fill_val = 0.0\n",
    "\n",
    "            # Growing average imputation\n",
    "            for i in range(len(vals)):\n",
    "                if mask[i]:\n",
    "                    running_sum += vals[i]\n",
    "                    running_count += 1\n",
    "                else:\n",
    "                    if running_count == 0:\n",
    "                        vals[i] = fill_val\n",
    "                    else:\n",
    "                        vals[i] = running_sum / running_count\n",
    "\n",
    "            # Write back to DataFrame\n",
    "            df.loc[sub.index, col] = vals\n",
    "\n",
    "        # After group loop, fill any remaining NaNs with global mean (if not default_fill_zero)\n",
    "        if not default_fill_zero:\n",
    "            df[col].fillna(global_means[col], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242073ac",
   "metadata": {},
   "source": [
    "## Average Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3281c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_filled_avg = create_empty_df()\n",
    "pre_data_filled_avg = pre_data_filled_questionnaire.copy()\n",
    "\n",
    "avg_features = features_dict['lifelog'] + features_dict['mood']\n",
    "\n",
    "if use_growing_avg:\n",
    "\tlogging.info(\"Using growing average imputation.\")\n",
    "\tpre_data_filled_avg = growing_average_impute(\n",
    "\t\tpre_data_filled_avg, 'ID', avg_features, default_fill_zero=null_default_zero\n",
    "\t)\n",
    "else: \n",
    "\tlogging.info(\"Using fixed average imputation.\")\n",
    "\tfor col in avg_features:\n",
    "\t\tpre_data_filled_avg[col] = pre_data_filled_avg.groupby('ID')[col].transform(\n",
    "\t\t\tlambda x: x.fillna(x.mean())\n",
    "\t\t)\n",
    "\t\tif null_default_zero:\n",
    "\t\t\tpre_data_filled_avg[col].fillna(0, inplace=True)\n",
    "\t\telse:\n",
    "\t\t\tpre_data_filled_avg[col].fillna(pre_data_filled_avg[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d404d",
   "metadata": {},
   "source": [
    "## Null Value 0\n",
    "\n",
    "- Null values set to `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d609e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_null_features = features_dict['dailylog_life']\n",
    "\n",
    "pre_data_filled_zero_null = create_empty_df()\n",
    "pre_data_filled_zero_null = pre_data_filled_avg.copy()\n",
    "\n",
    "# Fill Null Values\n",
    "pre_data_filled_zero_null.loc[:, zero_null_features] = pre_data_filled_zero_null.loc[:, zero_null_features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c8ada",
   "metadata": {},
   "source": [
    "## üíæ | Save Filled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c34026",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_filled = create_empty_df()\n",
    "pre_data_filled = pre_data_filled_zero_null.copy()\n",
    "\n",
    "save_as_csv(pre_data_filled, OUTPUT_PATH, f\"panic_pre_data_filled_{version}({scraped_data_filename})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb7bee",
   "metadata": {},
   "source": [
    "## Filled Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df884ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filled = create_empty_df()\n",
    "metadata_filled = metadata.copy()\n",
    "if len(pre_data_filled) != len(metadata):\n",
    "    raise ValueError(f\"Length of pre_data_filled ({len(pre_data_filled)}) does not match length of metadata ({len(metadata)}). Please check the data consistency.\")\n",
    "none_columns = ['dailylog_data', 'lifelog_data', 'questionnaire_data', 'dtype_n']\n",
    "for col in none_columns:\n",
    "    metadata_filled[col] = None\n",
    "\n",
    "metadata_filled['dailylog_data'] = pre_data_filled[features_dict['dailylog']].notnull().any(axis=1).astype(int)\n",
    "metadata_filled['lifelog_data'] = pre_data_filled[features_dict['lifelog']].notnull().any(axis=1).astype(int)\n",
    "metadata_filled['questionnaire_data'] = pre_data_filled[features_dict['questionnaire']].notnull().any(axis=1).astype(int)\n",
    "\n",
    "# TODO: Diary data is not used in the current analysis, but can be useful for future reference\n",
    "metadata_filled['dtype_n'] = metadata_filled['dailylog_data'] + metadata_filled['lifelog_data'] + metadata_filled['questionnaire_data']\n",
    "\n",
    "save_as_csv(metadata_filled, OUTPUT_PATH, f\"panic_metadata_filled_{version}({scraped_data_filename})\")\n",
    "display(metadata_filled.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdba2a",
   "metadata": {},
   "source": [
    "# üìí | Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f58181",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filled_ids =  pre_data_filled['ID'].unique()\n",
    "print(f\"Total number of unique IDs in pre_data_filled: {len(unique_filled_ids)}\")\n",
    "print(f\"Total number of entries in pre_data_filled: {len(pre_data_filled)}\")\n",
    "panic_entries = pre_data_filled[pre_data_filled['panic_label'] == 1]\n",
    "print(f\"Total number of panic entries in pre_data_filled: {len(panic_entries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_entry_ids = pre_data_filled['entry_id'].unique()\n",
    "print(f\"Total number of daily log entries: {metadata_filled[metadata_filled['dailylog_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata_filled[metadata_filled['dailylog_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of life log entries: {metadata_filled[metadata_filled['lifelog_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata_filled[metadata_filled['lifelog_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of questionnaire entries: {metadata_filled[metadata_filled['questionnaire_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata_filled[metadata_filled['questionnaire_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")\n",
    "# print(f\"Total number of panic diary entries: {metadata_filled[metadata_filled['diary_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata_filled[metadata_filled['diary_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "panic_patients = metadata_filled[metadata_filled['panic_label'] == 1]['ID'].unique()\n",
    "print(f\"Total number of patients with panic events: {len(panic_patients)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhlab-panic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
