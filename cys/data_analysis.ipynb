{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f4fd25",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5634a08",
   "metadata": {},
   "source": [
    "author:  `@cyshin971`  \n",
    "\n",
    "date:    `2025-06-12`  \n",
    "\n",
    "version: `1.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42654e1",
   "metadata": {},
   "source": [
    "# üìö | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from library.pandas_utils import move_column, remove_columns, aggregate_by_column, create_empty_df, find_unique_row\n",
    "from library.text_utils import save_as_csv\n",
    "# from library.path_utils import get_file_path\n",
    "from library.matplotlib_utils import plot_histogram_of_counts\n",
    "from library.json_utils import save_dict_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa403ca",
   "metadata": {},
   "source": [
    "# üìÅ | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../_data\"\n",
    "TMP_PATH = \"./cys/_tmp\"\n",
    "OUTPUT_PATH = \"./cys/_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbe0e9",
   "metadata": {},
   "source": [
    "# ‚õèÔ∏è | Scraped Data\n",
    "\n",
    "load preprocessed data (by `junyeol_lee`)\n",
    "- Each entry are the datapoints for a patient (`ID`) on a specific date (`date`)\n",
    "- If there were multiple datapoints for a specific date (`date`) for a specific patient (`ID`), the values were processed (`sum`, `avg`, etc.) to a representation for the day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecdc15",
   "metadata": {},
   "source": [
    "**Demography**\n",
    "| Feature     | Source |  Data Type |  Range         |  Description           |  Data Type |  Description      | Change         |\n",
    "|:------------|:-------|:--------------|:------------------|:-------------------------|:------------------|:------------------------|:---------------|\n",
    "|             |        | **raw**       |          |           | **scraped**       |         |     |\n",
    "| `gender`    |        | `boolean`       |        | Male (?), Female (?)     | `boolean`           | Male (?), Female (?)    |   |\n",
    "| `age`       |        | `int`           | 0‚Äì120?            | Age in years             | `int`               | Age in years            |      |\n",
    "| `marriage`  |        |               |                   |                         |                   |                        |                |\n",
    "| `smkHx`     |        |               |                   |                         |                   |                        |                |\n",
    "| `drinkHx`   |        |               |                   |                         |                   |                        |                |\n",
    "| `suicideHx` |        |               |                   |                         |                   |                        |                |\n",
    "\n",
    "Daily Log:\n",
    "- `panic`\n",
    "- `severity`\n",
    "- `exercise`\n",
    "- `alcohol`\n",
    "- `coffee`\n",
    "- `menstruation`\n",
    "- `smoking`: (`boolean`) **-> change to (``int``)**\n",
    "- `positive_feeling`\n",
    "- `negative_feeling`\n",
    "- `positive_E`\n",
    "- `negative_E`\n",
    "- `anxiety`\n",
    "- `annoying`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbe912",
   "metadata": {},
   "source": [
    "## Scraped Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filename = \"final_result_20250612\"\n",
    "\n",
    "features_dict = {\n",
    "\t\"demography\": [\n",
    "\t\t'gender', 'age', 'marriage', 'job', 'smkHx', 'drinkHx', 'suicideHx'\n",
    "\t],\n",
    "\t\"dailylog\": [\n",
    "\t\t'panic', 'severity', 'exercise', 'alcohol', 'coffee', 'menstruation',\n",
    "\t\t'smoking', 'positive_feeling', 'negative_feeling', 'positive_E', 'negative_E',\n",
    "\t\t'anxiety', 'annoying'\n",
    "\t],\n",
    "\t\"lifelog\": [\n",
    "\t\t'HR_var', 'HR_max', 'HR_mean', 'HR_hvar_mean', 'HR_acrophase', 'HR_amplitude',\n",
    "\t\t'HR_mesor','HR_acrophase_difference', 'HR_acrophase_difference_2d', 'HR_amplitude_difference',\n",
    "\t\t'HR_amplitude_difference_2d', 'HR_mesor_difference', 'HR_mesor_difference_2d',\n",
    "\t\t'bandpower(0.001-0.0005Hz)', 'bandpower(0.0005-0.0001Hz)', 'bandpower(0.0001-0.00005Hz)', 'bandpower(0.00005-0.00001Hz)',\n",
    "\t\t'steps', 'SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep'\n",
    "\t],\n",
    "\t\"questionnaire\": [\n",
    "\t\t'PHQ_9', 'STAI_X2', 'CSM', 'CTQ_1', 'CTQ_2', 'CTQ_3', 'CTQ_4', 'CTQ_5', 'KRQ', 'MDQ',\n",
    "\t\t'ACQ', 'APPQ_1', 'APPQ_2', 'APPQ_3', 'BSQ', 'GAD_7', 'BRIAN'\n",
    "\t],\n",
    "    \"id\": [\n",
    "        'ID', 'date'\n",
    "    ],\n",
    "    \"label\": [\n",
    "        'severity'\n",
    "    ],\n",
    "    \"metadata\": []\n",
    "}\n",
    "\n",
    "demo_vars = features_dict[\"demography\"]\n",
    "dailylog_vars = features_dict[\"dailylog\"]\n",
    "lifelog_vars = features_dict[\"lifelog\"]\n",
    "questionnaire_vars = features_dict[\"questionnaire\"]\n",
    "\n",
    "state_vars = demo_vars\n",
    "trait_vars = dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "all_vars = state_vars + dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "\n",
    "print(f'Number of variables: {len(all_vars)}')\n",
    "print(f'   Demographic variables: {len(state_vars)}')\n",
    "print(f'   Daily log variables: {len(dailylog_vars)}')\n",
    "print(f'   Life log variables: {len(lifelog_vars)}')\n",
    "print(f'   Questionnaire variables: {len(questionnaire_vars)}')\n",
    "\n",
    "save_dict_to_file(features_dict, TMP_PATH, \"scraped_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81218b4a",
   "metadata": {},
   "source": [
    "## Load Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = pd.read_csv(os.path.join(DATA_PATH, f\"{metadata_filename}.csv\"))\n",
    "\n",
    "# check if all columns are present\n",
    "missing_cols = [col for col in all_vars if col not in scraped_data.columns]\n",
    "if missing_cols:\n",
    "    logging.warning(f\"Missing columns in scraped_data: {missing_cols}\")\n",
    "else:\n",
    "\tlogging.info(\"All expected columns are present in scraped_data.\")\n",
    "# convert date column to datetime format\n",
    "scraped_data['date'] = pd.to_datetime(scraped_data['date'], format='%Y-%m-%d')\n",
    "remove_columns(scraped_data, ['Unnamed: 0'])\n",
    "\n",
    "print(f\"Number of rows: {scraped_data.shape[0]}\")\n",
    "print(f\"Number of columns: {scraped_data.shape[1]}\")\n",
    "display(scraped_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024277b",
   "metadata": {},
   "source": [
    "# ‚öíÔ∏è | Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1f06",
   "metadata": {},
   "source": [
    "## Initialize Preprocessed Data\n",
    "\n",
    "- add `entry_id` to identify each entry: `'ID'_'date'`\n",
    "- add dataset to identify source: `SYM1`, `SYM2`, `PXPN`\n",
    "- convert `panic` (`0`, `1`, `2` = panic) to days befor panic (`dbp`) (panic = `0`, `1`, `2`)\n",
    "- convert `0` values in `exercise`, `alcohol`, `coffee`, `menstruation`, `smoking` to null values `np.nan` (`NaN`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_init = create_empty_df()\n",
    "data_pre_init = scraped_data.copy()\n",
    "\n",
    "# Add 'entry_id' column: unique identifier for each row\n",
    "data_pre_init['entry_id'] = data_pre_init['ID'] + '_' + data_pre_init['date'].astype(str)\n",
    "instance_id_unique = data_pre_init['entry_id'].unique()\n",
    "move_column(data_pre_init, 'entry_id', 0)\n",
    "print(\"Number of unique entry IDs:\", len(instance_id_unique))\n",
    "# Check if 'entry_id' is unique\n",
    "if data_pre_init['entry_id'].duplicated().any():\n",
    "\t# return the rows with duplicate 'entry_id'\n",
    "\tduplicates = data_pre_init[data_pre_init['entry_id'].duplicated(keep=False)]\n",
    "\tprint(f\"Duplicate entry_id found [{len(duplicates)}]:\")\n",
    "\tdisplay(duplicates.head(5))\n",
    "\tsave_as_csv(duplicates, TMP_PATH, f\"duplicates_{metadata_filename}\")\n",
    "\n",
    "# Add 'dataset' column: source of data\n",
    "data_pre_init['dataset'] = data_pre_init['ID'].str.split('_').str[0]\n",
    "data_pre_init['dataset'] = data_pre_init['dataset'].str.split('-').str[0]\n",
    "move_column(data_pre_init, 'dataset', 1)\n",
    "\n",
    "# Convert 'panic' column to Days Before Panic (dbp)\n",
    "data_pre_init['dbp'] = data_pre_init.apply(\n",
    "\tlambda row: np.nan if row['panic'] == 0\n",
    " \t\t\t\telse 0 if row['panic'] == 2 else row['panic'],\n",
    "\taxis=1\n",
    ")\n",
    "remove_columns(data_pre_init, ['panic'])\n",
    "\n",
    "# Convert 'daily_log' variables = 0 to NaN\n",
    "data_pre_init['exercise'] = data_pre_init['exercise'].replace(0, np.nan)\n",
    "data_pre_init['alcohol'] = data_pre_init['alcohol'].replace(0, np.nan)\n",
    "data_pre_init['coffee'] = data_pre_init['coffee'].replace(0, np.nan)\n",
    "data_pre_init['menstruation'] = data_pre_init['menstruation'].replace(0, np.nan)\n",
    "data_pre_init['smoking'] = data_pre_init['smoking'].replace(0, np.nan)\n",
    "\n",
    "# Update the features_dict\n",
    "features_dict['id'] = ['entry_id'] + features_dict['id'] + ['dataset']\n",
    "features_dict['label'] = ['dbp'] + features_dict['label']\n",
    "features_dict['dailylog'].remove('panic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_pre_init.head(5))\n",
    "print(\"Unique sources in metadata_ljy: \", data_pre_init['dataset'].unique())\n",
    "print(\"Number of entries in metadata_ljy:\", data_pre_init.shape[0])\n",
    "sym1_n = data_pre_init[data_pre_init['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre_init[data_pre_init['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre_init[data_pre_init['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in metadata_ljy:\", len(data_pre_init['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre_init[data_pre_init['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre_init[data_pre_init['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre_init[data_pre_init['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data_pre_init[data_pre_init['dbp'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cac54",
   "metadata": {},
   "source": [
    "## Initialize Metadata\n",
    "\n",
    "initialize `metadata` by adding\n",
    "- `demography_data` : whether demography data exists in the entry (`boolean`)\n",
    "- `dailylog_data`, `lifelog_data`, `questionnaire_data` : whether each data group exists in the entry (`boolean`)\n",
    "- `dtype_n` : how many of the 3 `state` groups exists in the entry (`int`)\n",
    "- `panic_label` : whether a panic occured in the entry (`boolean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed181f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_init = create_empty_df()\n",
    "metadata_init = data_pre_init.copy()\n",
    "\n",
    "metadata_init['demography_data'] = metadata_init[features_dict['demography']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['dailylog_data'] = metadata_init[features_dict['dailylog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['lifelog_data'] = metadata_init[features_dict['lifelog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['questionnaire_data'] = metadata_init[features_dict['questionnaire']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['dtype_n'] = metadata_init['dailylog_data'] + metadata_init['lifelog_data'] + metadata_init['questionnaire_data']\n",
    "move_column(metadata_init, 'dtype_n', 8)\n",
    "metadata_init['panic_label'] = metadata_init['dbp'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "add_list = ['dailylog_data', 'lifelog_data', 'questionnaire_data', 'dtype_n']\n",
    "for item in add_list:\n",
    "\tif item not in features_dict['metadata']:\n",
    "\t\tfeatures_dict['metadata'].append(item)\n",
    "del add_list\n",
    "if 'panic_label' not in features_dict['label']:\n",
    "\tfeatures_dict['label'].append('panic_label')\n",
    "\n",
    "check_metadata = False\n",
    "if check_metadata:\n",
    "    check_type = 'dailylog' # demography, dailylog, lifelog, questionnaire\n",
    "    check_for = 1\n",
    "    test = metadata_init[metadata_init[check_type+'_data'] == check_for].copy()\n",
    "    test = test[features_dict['id']+features_dict['metadata']+features_dict[check_type]]\n",
    "    print(f\"--------- TEST {test.shape[0]} ENTRIES WITH {check_type} = {check_for} ---------\")\n",
    "    display(test.head(10))\n",
    "    save_as_csv(test, TMP_PATH, f\"metadata_{check_type}_{check_for}\")\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    del test, check_type, check_for\n",
    "\n",
    "#metadata_init = metadata_init[features_dict['id'] + features_dict['metadata'] + features_dict['demography'] + features_dict['label']]\n",
    "display(metadata_init.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccf3dc",
   "metadata": {},
   "source": [
    "## Extract Demography Data\n",
    "\n",
    "- All patients within the scraped data were confirmed to have demographic data (`demography_data` = `True`)\n",
    "- as such demography_data will not be included in the `metadata`\n",
    "- Demography data was extracted and saved as `demography.csv` to the `output` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('gender_n', 'gender', 'nunique'),\n",
    "\t('age_n', 'age', 'nunique'),\n",
    "\t('marriage_n', 'marriage', 'nunique'),\n",
    "\t('job_n', 'job', 'nunique'),\n",
    "\t('smkHx_n', 'smkHx', 'nunique'),\n",
    "\t('drinkHx_n', 'drinkHx', 'nunique'),\n",
    "\t('suicideHx_n', 'suicideHx', 'nunique'),\n",
    "    ('gender', 'gender', 'first'),\n",
    "\t('age', 'age', 'first'),\n",
    "\t('marriage', 'marriage', 'first'),\n",
    "\t('job', 'job', 'first'),\n",
    "\t('smkHx', 'smkHx', 'first'),\n",
    "\t('drinkHx', 'drinkHx', 'first'),\n",
    "\t('suicideHx', 'suicideHx', 'first')\n",
    "]\n",
    "demo_data = create_empty_df()\n",
    "demo_data = aggregate_by_column(metadata_init, 'ID', agg_matrix)\n",
    "# check if the length of each unique value is 1\n",
    "non_unique_cols = []\n",
    "for col in features_dict['demography']:\n",
    "\tif demo_data[col+'_n'].apply(lambda x: x > 1).any():\n",
    "\t\tnon_unique_cols.append(col)\n",
    "if non_unique_cols:\n",
    "\traise ValueError(f\"Demographic columns {non_unique_cols} are not unique for each ID in demo_data.\")\n",
    "else:\n",
    "\tprint(\"All demographic columns are unique for each ID in demo_data.\")\n",
    "\n",
    "for col in features_dict['demography']:\n",
    "\tremove_columns(demo_data, [col+'_n'])\n",
    "print(f\"Number of rows in demo_data: {demo_data.shape[0]}\")\n",
    "display(demo_data.head(5))\n",
    "\n",
    "save_as_csv(demo_data, OUTPUT_PATH, f\"demography_({metadata_filename})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52f179",
   "metadata": {},
   "source": [
    "## Construct Intermediate Metadata\n",
    "- the current `metadata` (`metadata_init`) was filtered to include only columns for identification, added columns for metadata, and labels\n",
    "- the `metadata` was also filtered to get rid of all entries that only have demography data (`dtype_n` = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b509aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_int = create_empty_df()\n",
    "metadata_int = metadata_init.copy()\n",
    "\n",
    "metadata_int = metadata_int[features_dict['id'] + features_dict['metadata'] + features_dict['label']]\n",
    "move_column(metadata_int, 'severity', -1)\n",
    "metadata_int = metadata_int[metadata_int['dtype_n'] > 0]\n",
    "display(metadata_int.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c967c9",
   "metadata": {},
   "source": [
    "## Filter Preprocessed Data\n",
    "\n",
    "- demographic features were removed from preprocessed data (`data_pre`)\n",
    "- the data was filtered to remove entries with only demgraphic data\n",
    "- the removed IDs were checked to see if no relevant entries were discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = create_empty_df()\n",
    "data_pre = data_pre_init.copy()\n",
    "# Remove demographic features from data_proc\n",
    "remove_columns(data_pre, features_dict['demography'])\n",
    "# Filter data_proc to keep only rows with entry IDs present in metadata_int\n",
    "metadata_int_unique_ids = metadata_int['entry_id'].unique()\n",
    "data_pre = data_pre[data_pre['entry_id'].isin(metadata_int_unique_ids)]\n",
    "\n",
    "# remove rows with null dates\n",
    "data_pre = data_pre[data_pre['date'].notnull()]\n",
    "\n",
    "# Find IDs present in unfiltered_data but missing in filtered_data (i.e., lost after filtering)\n",
    "check_missing_ids = False\n",
    "if check_missing_ids:\n",
    "\tmissing_ids = np.setdiff1d(data_pre_init['ID'].unique(), data_pre['ID'].unique())\n",
    "\tmissing_data = data_pre_init[data_pre_init['ID'].isin(missing_ids)]\n",
    "\tprint(f\"Number of IDs lost after filtering: {len(missing_ids)}\")\n",
    "\t_ = save_as_csv(missing_data, TMP_PATH, f\"missing_{metadata_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc84e1",
   "metadata": {},
   "source": [
    "## üíæ | Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_pre to CSV\n",
    "save_as_csv(data_pre, OUTPUT_PATH, f\"panic_pre_({metadata_filename})\")\n",
    "\n",
    "display(data_pre.head(3))\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Total entries in original: \", data_pre_init.shape[0])\n",
    "sym1_n = data_pre_init[data_pre_init['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre_init[data_pre_init['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre_init[data_pre_init['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in original:\", len(data_pre_init['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre_init[data_pre_init['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre_init[data_pre_init['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre_init[data_pre_init['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data_pre_init[data_pre_init['dbp'] == 0].shape[0])\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Total entries in filtered: \", data_pre['dataset'].unique())\n",
    "sym1_n = data_pre[data_pre['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre[data_pre['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre[data_pre['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in filtered:\", len(data_pre['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre[data_pre['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre[data_pre['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre[data_pre['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data_pre[data_pre['dbp'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e6270",
   "metadata": {},
   "source": [
    "# üìñ | Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e7f97",
   "metadata": {},
   "source": [
    "### Calculate Days Before Panic Features for Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Not working\n",
    "\n",
    "metadata_calc = create_empty_df()\n",
    "metadata_calc = metadata_int.copy()\n",
    "\n",
    "# Test\n",
    "p_id = 'PXPN_10006'\n",
    "\n",
    "metadata_calc['n_prior_data']    = None\n",
    "metadata_calc['ref_event_id']    = None\n",
    "move_column(metadata_calc, 'panic_label', -1)\n",
    "move_column(metadata_calc, 'severity', -1)\n",
    "metadata_calc.sort_values(by=['ID', 'date'], ascending=False, inplace=True)\n",
    "\n",
    "delta_days = 3\n",
    "lookback_limit = 7\n",
    "\n",
    "def calculate_days_before_panic(patient_id):\n",
    "    patient_data = metadata_calc[metadata_calc['ID'] == patient_id]\n",
    "    entry_dates_series = patient_data['date']\n",
    "    if len(set(entry_dates_series)) != len(entry_dates_series):\n",
    "        raise ValueError(f\"Duplicate dates found for patient {patient_id}. Please check the data.\")\n",
    "    panic_dates_series = patient_data[patient_data['dbp'] == 0]['date']\n",
    "    if len(set(panic_dates_series)) != len(panic_dates_series):\n",
    "        raise ValueError(f\"Duplicate panic dates found for patient {patient_id}. Please check the data.\")\n",
    "    \n",
    "    entry_dates = set(entry_dates_series)\n",
    "    panic_dates = set(panic_dates_series)\n",
    "\n",
    "    for panic_date in sorted(panic_dates, reverse=True): # Sort from latest to earliest\n",
    "        for j in range(1, delta_days + 1):\n",
    "            prior_date = panic_date - pd.Timedelta(days=j)\n",
    "            if prior_date in entry_dates:\n",
    "                index = patient_data[patient_data['date'] == prior_date].index[0]\n",
    "                metadata_calc.loc[index, 'dbp'] = j\n",
    "                metadata_calc.loc[index, 'ref_event_id'] = patient_data.loc[index, 'entry_id']\n",
    "    \n",
    "    for entry_date in sorted(entry_dates, reverse=True):\n",
    "        for j in range(1, lookback_limit + 1):\n",
    "            if j == lookback_limit:\n",
    "                metadata_calc.loc[index, 'n_prior_data'] = j\n",
    "                break\n",
    "            prior_date = entry_date - pd.Timedelta(days=j)\n",
    "            if prior_date not in entry_dates:\n",
    "                break\n",
    "            index = patient_data[patient_data['date'] == prior_date].index[0]\n",
    "            if metadata_calc.loc[index, 'panic_label'] == 1:\n",
    "                break\n",
    "            index = patient_data[patient_data['date'] == entry_date].index[0]\n",
    "            metadata_calc.loc[index, 'n_prior_data'] = j\n",
    "\n",
    "# Test\n",
    "calculate_days_before_panic(p_id)\n",
    "metadata_calc = metadata_calc[metadata_calc['ID'] == p_id]\n",
    "\n",
    "display(metadata_calc.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_calc.sort_values(by=['ID', 'date'], inplace=True)\n",
    "display(metadata_calc.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da162d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99402307",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'entry_id', 'count'),\n",
    "\t('n_panic_2', 'panic', lambda x: (x == 2).sum())\n",
    "]\n",
    "\n",
    "metadata_ljy_agg = aggregate_by_column(metadata, 'ID', agg_matrix)\n",
    "display(metadata_ljy_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68554311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all IDs that ever had panic==2\n",
    "panic_ids = metadata_ljy_agg.loc[\n",
    "    metadata_ljy_agg['n_panic_2'] > 0, 'ID'\n",
    "].unique()\n",
    "print(\"Unique IDs with panic events (panic=2):\", len(panic_ids))\n",
    "print(f\"Number of panic events (panic=2): {n_panic_2}\")\n",
    "print(\"--------------------------------------\")\n",
    "plot_histogram_of_counts(metadata_ljy_agg['n_panic_2'], title=\"Histogram of Panic Events per ID\", xlabel=\"Number of Panic Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "metadata_agg = aggregate_by_column(metadata, 'ref_event_id', agg_matrix)\n",
    "#display(metadata_agg.head(5))\n",
    "\n",
    "check = metadata_agg[metadata_agg['n_entries'] != metadata_agg['n_dates']]\n",
    "#print(\"Entries where n_entries != n_dates:\")\n",
    "#display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only those rows we marked as panic events\n",
    "panic_data = metadata[['ID', 'date', 'event_id', 'last_panic_days', 'n_prior_data', 'severity']].copy()\n",
    "panic_data = panic_data[panic_data['event_id'].notnull()]\n",
    "\n",
    "panic_data = panic_data[\n",
    "    (panic_data['last_panic_days'] > delta_days) &\n",
    "    (panic_data['n_prior_data'] >= delta_days) &\n",
    "    (panic_data['severity'].notnull())\n",
    "]\n",
    "print(f\"-------- last_panic_days > {delta_days} & n_prior_data ‚â• {delta_days} --------\")\n",
    "print(f\"Number of qualifying panic events: {panic_data.shape[0]} out of {n_panic_2} ({panic_data.shape[0] / n_panic_2:.2%})\")\n",
    "print(f\"Unique IDs with panic events: {len(panic_data['ID'].unique())} out of {len(panic_ids)} ({len(panic_data['ID'].unique()) / len(panic_ids):.2%})\")\n",
    "\n",
    "display(panic_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = metadata[(metadata['ref_event_id'].notnull()) | (metadata['event_id'].notnull())].copy()\n",
    "qulifying_event_ids = panic_data['event_id'].unique()\n",
    "qualifying_metadata = filtered_metadata[filtered_metadata['ref_event_id'].isin(qulifying_event_ids)]\n",
    "display(qualifying_metadata.head(10))\n",
    "print(f\"Expected number of columns: {len(all_cols) * 3}\")\n",
    "\n",
    "disp_data = filtered_metadata[['ID', 'date', 'event_id', 'ref_event_id', 'last_panic_days', 'n_prior_data']].copy()\n",
    "display(disp_data.head(50))\n",
    "del disp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "qualifying_metadata_agg = aggregate_by_column(qualifying_metadata, 'ref_event_id', agg_matrix)\n",
    "#display(filtered_metadata_agg.head(5))\n",
    "plot_histogram_of_counts(qualifying_metadata_agg['n_entries'], title=\"Histogram of Entries per Ref Event ID\", xlabel=\"Number of Entries\", bins_step=1)\n",
    "check = qualifying_metadata_agg[qualifying_metadata_agg['n_entries'] != qualifying_metadata_agg['n_dates']]\n",
    "print(\"Entries where n_entries != n_dates:\")\n",
    "display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_panic = metadata['severity'].unique()\n",
    "print(f\"\\nUnique values in 'panic': {unique_panic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhlab-panic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
