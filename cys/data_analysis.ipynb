{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_LJY_PATH = \"../_data/all_data_with_severity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cols = [\n",
    "    'gender', 'age', 'marriage', 'job', 'smkHx', 'drinkHx', 'suicideHx'\n",
    "]\n",
    "dailylog_cols = [\n",
    "    'panic', 'severity', 'exercise', 'alcohol', 'coffee', 'menstruation',\n",
    "\t'smoking', 'positive_feeling', 'negative', 'positive_E', 'negative_E',\n",
    "\t'anxiety', 'annoying'\n",
    "]\n",
    "lifelog_cols = [\n",
    "\t'HR_var', 'HR_max', 'HR_mean', 'HR_hvar_mean', 'HR_acrophase', 'HR_amplitude',\n",
    " \t'HR_mesor','HR_acrophase_difference', 'HR_acrophase_difference_2d', 'HR_amplitude_difference',\n",
    "  \t'HR_amplitude_difference_2d', 'HR_mesor_difference', 'HR_mesor_difference_2d',\n",
    "\t'bandpower(0.001-0.0005Hz)', 'bandpower(0.0005-0.0001Hz)', 'bandpower(0.0001-0.00005Hz)', 'bandpower(0.00005-0.00001Hz)',\n",
    "\t'steps', 'SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep'\n",
    "]\n",
    "questionnaire_cols = [\n",
    "\t'PHQ_9', 'STAI_X2', 'CSM', 'CTQ_1', 'CTQ_2', 'CTQ_3', 'CTQ_4', 'CTQ_5', 'KRQ', 'MDQ',\n",
    " \t'ACQ', 'APPQ_1', 'APPQ_2', 'APPQ_3', 'BSQ', 'GAD_7', 'BRIAN'\n",
    "]\n",
    "all_cols = demo_cols + dailylog_cols + lifelog_cols + questionnaire_cols\n",
    "print(f'Number of columns: {len(all_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_ljy = pd.read_csv(METADATA_LJY_PATH)\n",
    "\n",
    "# check if all columns are present\n",
    "missing_cols = [col for col in all_cols if col not in metadata_ljy.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Missing columns in metadata_ljy: {missing_cols}\")\n",
    "# convert date column to datetime format\n",
    "metadata_ljy['date'] = pd.to_datetime(metadata_ljy['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09964c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {metadata_ljy.shape[0]}\")\n",
    "print(f\"Number of columns: {metadata_ljy.shape[1]}\")\n",
    "display(metadata_ljy.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481804a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'dataset' column: source of data\n",
    "metadata_ljy['dataset'] = metadata_ljy['ID'].str.split('_').str[0]\n",
    "metadata_ljy['dataset'] = metadata_ljy['dataset'].str.split('-').str[0]\n",
    "unique_dataset = metadata_ljy['dataset'].unique()\n",
    "print(\"Unique sources in metadata_ljy:\")\n",
    "\n",
    "print(unique_dataset)\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Total number of entries:\", metadata_ljy.shape[0])\n",
    "pxpn_n = metadata_ljy[metadata_ljy['dataset'] == 'PXPN'].shape[0]\n",
    "print(\"    PXPN dataset:\", pxpn_n)\n",
    "sym1_n = metadata_ljy[metadata_ljy['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = metadata_ljy[metadata_ljy['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM dataset:\", sym1_n+sym2_n)\n",
    "print(\"--------------------------------------\")\n",
    "unique_ids = metadata_ljy['ID'].unique()\n",
    "print(\"Number of unique IDs in metadata_ljy:\", len(unique_ids))\n",
    "n_panic_2 = metadata_ljy[metadata_ljy['panic'] == 2].shape[0]\n",
    "print(\"Number of panic events (panic=2):\", n_panic_2)\n",
    "\n",
    "display(metadata_ljy.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99402307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.pandas_utils import aggregate_by_column\n",
    "\n",
    "agg_matrix = [\n",
    "\t('n_entries', 'ID', 'count'),\n",
    "\t('n_dates', 'date', 'nunique'),\n",
    "\t('n_panic_2', 'panic', lambda x: (x == 2).sum())\n",
    "]\n",
    "\n",
    "metadata_ljy_agg = aggregate_by_column(metadata_ljy, 'ID', agg_matrix)\n",
    "#display(metadata_ljy_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88877b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_of_counts(column, title=None, xlabel=None, ylabel=\"Frequency\", bins_step=5):\n",
    "\t\"\"\"\n",
    "\tPlot a histogram of counts for a given pandas Series (column).\n",
    "\tOnly nonzero values are plotted.\n",
    "\t\"\"\"\n",
    "\timport matplotlib.pyplot as plt\n",
    "\n",
    "\tdata = column[column > 0]\n",
    "\tif data.empty:\n",
    "\t\tprint(\"No nonzero values to plot.\")\n",
    "\t\treturn\n",
    "\n",
    "\tmin_val = int(data.min())\n",
    "\tmax_val = int(data.max())\n",
    "\tbins = np.arange(min_val, max_val + 2)  # +2 to include max in bin edges\n",
    "\n",
    "\tplt.figure(figsize=(10, 4))\n",
    "\tplt.hist(data, bins=bins, color='blue', alpha=0.7)\n",
    "\tplt.title(title or f'Histogram of {column.name}')\n",
    "\tplt.xlabel(xlabel or column.name)\n",
    "\tplt.ylabel(ylabel)\n",
    "\tplt.grid(axis='y', alpha=0.75)\n",
    "\tplt.xticks(np.arange(0, max_val + 1, bins_step))\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68554311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all IDs that ever had panic==2\n",
    "panic_ids = metadata_ljy_agg.loc[\n",
    "    metadata_ljy_agg['n_panic_2'] > 0, 'ID'\n",
    "].unique()\n",
    "print(\"Unique IDs with panic events (panic=2):\", len(panic_ids))\n",
    "print(f\"Number of panic events (panic=2): {n_panic_2}\")\n",
    "print(\"--------------------------------------\")\n",
    "plot_histogram_of_counts(metadata_ljy_agg['n_panic_2'], title=\"Histogram of Panic Events per ID\", xlabel=\"Number of Panic Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_days = 3\n",
    "\n",
    "# Make a fresh copy of metadata_ljy and add four new columns\n",
    "metadata = metadata_ljy.copy()\n",
    "metadata['last_panic_days'] = None\n",
    "metadata['n_prior_data']    = None\n",
    "metadata['event_id']        = None\n",
    "metadata['ref_event_id']    = None\n",
    "\n",
    "# Loop over each subject‐ID that has at least one panic==2\n",
    "for panic_id in panic_ids:\n",
    "    # Extract only rows for this subject, sorted by date ascending\n",
    "    df = metadata.loc[metadata['ID'] == panic_id].sort_values('date', ascending=True)\n",
    "\n",
    "    # Build a set of all observed dates for quick membership tests\n",
    "    date_set = set(df['date'])\n",
    "\n",
    "    last_panic_date = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['panic'] != 2:\n",
    "            # Only process rows where panic==2\n",
    "            continue\n",
    "\n",
    "        # Create and store event_id\n",
    "        event_id = f\"{panic_id}_{row['date'].date()}\"\n",
    "        metadata.loc[idx, 'event_id'] = event_id\n",
    "\n",
    "        # Compute last_panic_days\n",
    "        if last_panic_date is None:\n",
    "            # This is the very first panic for this subject\n",
    "            metadata.loc[idx, 'last_panic_days'] = 100\n",
    "        else:\n",
    "            days_diff = (row['date'] - last_panic_date).days\n",
    "            metadata.loc[idx, 'last_panic_days'] = days_diff\n",
    "\n",
    "        # Count how many consecutive prior days (up to 100) exist before this panic\n",
    "        # We look backward one day at a time. If we hit another panic, or a missing date, we break.\n",
    "        current_date = row['date']\n",
    "        found_n_prior = False\n",
    "\n",
    "        for j in range(1, 101):  # j = 1, 2, …, 100\n",
    "            look_date = current_date - pd.Timedelta(days=j)\n",
    "\n",
    "            if look_date not in date_set:\n",
    "                # We encountered a missing day. That means only (j-1) consecutive prior days exist.\n",
    "                metadata.loc[idx, 'n_prior_data'] = j - 1\n",
    "                found_n_prior = True\n",
    "                break\n",
    "\n",
    "            # There *is* at least one row on look_date—grab it (or them)\n",
    "            rows_on_that_date = df[df['date'] == look_date]\n",
    "            if len(rows_on_that_date) > 1:\n",
    "                print(f\"Warning: More than one row for date {look_date} for ID {panic_id}\")\n",
    "            # If ANY of those rows had panic==2, we stop and record (j-1):\n",
    "            if (rows_on_that_date['panic'] == 2).any():\n",
    "                metadata.loc[idx, 'n_prior_data'] = j - 1\n",
    "                found_n_prior = True\n",
    "                break\n",
    "\n",
    "            if j in range(1, delta_days + 1):\n",
    "                # Set the 'ref_event_id' to event_id for all the rows for rows_on_that_date if we are within the delta_days\n",
    "                metadata.loc[rows_on_that_date.index, 'ref_event_id'] = event_id\n",
    "  \n",
    "        if not found_n_prior:\n",
    "            # We never hit a prior panic or missing day within 100 days → cap at 100\n",
    "            metadata.loc[idx, 'n_prior_data'] = 100\n",
    "\n",
    "        # update last_panic_date so that the next panic (if any) calculates correctly\n",
    "        last_panic_date = row['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "metadata_agg = aggregate_by_column(metadata, 'ref_event_id', agg_matrix)\n",
    "#display(metadata_agg.head(5))\n",
    "\n",
    "check = metadata_agg[metadata_agg['n_entries'] != metadata_agg['n_dates']]\n",
    "#print(\"Entries where n_entries != n_dates:\")\n",
    "#display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only those rows we marked as panic events\n",
    "panic_data = metadata[['ID', 'date', 'event_id', 'last_panic_days', 'n_prior_data', 'severity']].copy()\n",
    "panic_data = panic_data[panic_data['event_id'].notnull()]\n",
    "\n",
    "panic_data = panic_data[\n",
    "    (panic_data['last_panic_days'] > delta_days) &\n",
    "    (panic_data['n_prior_data'] >= delta_days) &\n",
    "    (panic_data['severity'].notnull())\n",
    "]\n",
    "print(f\"-------- last_panic_days > {delta_days} & n_prior_data ≥ {delta_days} --------\")\n",
    "print(f\"Number of qualifying panic events: {panic_data.shape[0]} out of {n_panic_2} ({panic_data.shape[0] / n_panic_2:.2%})\")\n",
    "print(f\"Unique IDs with panic events: {len(panic_data['ID'].unique())} out of {len(panic_ids)} ({len(panic_data['ID'].unique()) / len(panic_ids):.2%})\")\n",
    "\n",
    "display(panic_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = metadata[(metadata['ref_event_id'].notnull()) | (metadata['event_id'].notnull())].copy()\n",
    "qulifying_event_ids = panic_data['event_id'].unique()\n",
    "qualifying_metadata = filtered_metadata[filtered_metadata['ref_event_id'].isin(qulifying_event_ids)]\n",
    "display(qualifying_metadata.head(10))\n",
    "print(f\"Expected number of columns: {len(all_cols) * 3}\")\n",
    "\n",
    "disp_data = filtered_metadata[['ID', 'date', 'event_id', 'ref_event_id', 'last_panic_days', 'n_prior_data']].copy()\n",
    "display(disp_data.head(50))\n",
    "del disp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "qualifying_metadata_agg = aggregate_by_column(qualifying_metadata, 'ref_event_id', agg_matrix)\n",
    "#display(filtered_metadata_agg.head(5))\n",
    "plot_histogram_of_counts(qualifying_metadata_agg['n_entries'], title=\"Histogram of Entries per Ref Event ID\", xlabel=\"Number of Entries\", bins_step=1)\n",
    "check = qualifying_metadata_agg[qualifying_metadata_agg['n_entries'] != qualifying_metadata_agg['n_dates']]\n",
    "print(\"Entries where n_entries != n_dates:\")\n",
    "display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_panic = metadata_ljy['severity'].unique()\n",
    "print(f\"\\nUnique values in 'panic': {unique_panic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owlie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
