{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f4fd25",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5634a08",
   "metadata": {},
   "source": [
    "author:  `@cyshin971`  \n",
    "\n",
    "date:    `2025-06-12`  \n",
    "\n",
    "version: `1.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42654e1",
   "metadata": {},
   "source": [
    "# 📚 | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from library.pandas_utils import move_column, remove_columns, aggregate_by_column, create_empty_df, find_unique_row\n",
    "from library.text_utils import save_as_csv\n",
    "# from library.path_utils import get_file_path\n",
    "from library.matplotlib_utils import plot_histogram_of_counts\n",
    "from library.json_utils import save_dict_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa403ca",
   "metadata": {},
   "source": [
    "# 📁 | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../_data\"\n",
    "TMP_PATH = \"./cys/_tmp\"\n",
    "OUTPUT_PATH = \"./cys/_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbe0e9",
   "metadata": {},
   "source": [
    "# ⛏️ | Scraped Data\n",
    "\n",
    "load preprocessed data (by `junyeol_lee`)\n",
    "- Each entry are the datapoints for a patient (`ID`) on a specific date (`date`)\n",
    "- If there were multiple datapoints for a specific date (`date`) for a specific patient (`ID`), the values were processed (`sum`, `avg`, etc.) to a representation for the day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecdc15",
   "metadata": {},
   "source": [
    "**Demography**\n",
    "| Feature     | Source |  Data Type |  Range         |  Description           |  Data Type |  Description      | Change         |\n",
    "|:------------|:-------|:--------------|:------------------|:-------------------------|:------------------|:------------------------|:---------------|\n",
    "|             |        | **raw**       |          |           | **scraped**       |         |     |\n",
    "| `gender`    |        | `boolean`       |        | Male (?), Female (?)     | `boolean`           | Male (?), Female (?)    |   |\n",
    "| `age`       |        | `int`           | 0–120?            | Age in years             | `int`               | Age in years            |      |\n",
    "| `marriage`  |        |               |                   |                         |                   |                        |                |\n",
    "| `smkHx`     |        |               |                   |                         |                   |                        |                |\n",
    "| `drinkHx`   |        |               |                   |                         |                   |                        |                |\n",
    "| `suicideHx` |        |               |                   |                         |                   |                        |                |\n",
    "\n",
    "Daily Log:\n",
    "- `panic`\n",
    "- `severity`\n",
    "- `exercise`\n",
    "- `alcohol`\n",
    "- `coffee`\n",
    "- `menstruation`\n",
    "- `smoking`: (`boolean`) **-> change to (``int``)**\n",
    "- `positive_feeling`\n",
    "- `negative_feeling`\n",
    "- `positive_E`\n",
    "- `negative_E`\n",
    "- `anxiety`\n",
    "- `annoying`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbe912",
   "metadata": {},
   "source": [
    "## Scraped Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab7722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - (json_utils.py) save_dict_to_file: Dictionary saved successfully to C:\\Users\\cyshi\\OneDrive\\Documents\\GitHub\\Panic-Project-CYS\\cys\\_tmp\\scraped_features.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 62\n",
      "   Demographic variables: 7\n",
      "   Daily log variables: 13\n",
      "   Life log variables: 25\n",
      "   Questionnaire variables: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cyshi/OneDrive/Documents/GitHub/Panic-Project-CYS/cys/_tmp/scraped_features.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_filename = \"final_result_20250612\"\n",
    "\n",
    "features_dict = {\n",
    "\t\"demography\": [\n",
    "\t\t'gender', 'age', 'marriage', 'job', 'smkHx', 'drinkHx', 'suicideHx'\n",
    "\t],\n",
    "\t\"dailylog\": [\n",
    "\t\t'panic', 'severity', 'exercise', 'alcohol', 'coffee', 'menstruation',\n",
    "\t\t'smoking', 'positive_feeling', 'negative_feeling', 'positive_E', 'negative_E',\n",
    "\t\t'anxiety', 'annoying'\n",
    "\t],\n",
    "\t\"lifelog\": [\n",
    "\t\t'HR_var', 'HR_max', 'HR_mean', 'HR_hvar_mean', 'HR_acrophase', 'HR_amplitude',\n",
    "\t\t'HR_mesor','HR_acrophase_difference', 'HR_acrophase_difference_2d', 'HR_amplitude_difference',\n",
    "\t\t'HR_amplitude_difference_2d', 'HR_mesor_difference', 'HR_mesor_difference_2d',\n",
    "\t\t'bandpower(0.001-0.0005Hz)', 'bandpower(0.0005-0.0001Hz)', 'bandpower(0.0001-0.00005Hz)', 'bandpower(0.00005-0.00001Hz)',\n",
    "\t\t'steps', 'SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep'\n",
    "\t],\n",
    "\t\"questionnaire\": [\n",
    "\t\t'PHQ_9', 'STAI_X2', 'CSM', 'CTQ_1', 'CTQ_2', 'CTQ_3', 'CTQ_4', 'CTQ_5', 'KRQ', 'MDQ',\n",
    "\t\t'ACQ', 'APPQ_1', 'APPQ_2', 'APPQ_3', 'BSQ', 'GAD_7', 'BRIAN'\n",
    "\t],\n",
    "    \"id\": [\n",
    "        'ID', 'date'\n",
    "    ],\n",
    "    \"label\": [\n",
    "        'severity'\n",
    "    ],\n",
    "    \"metadata\": []\n",
    "}\n",
    "\n",
    "demo_vars = features_dict[\"demography\"]\n",
    "dailylog_vars = features_dict[\"dailylog\"]\n",
    "lifelog_vars = features_dict[\"lifelog\"]\n",
    "questionnaire_vars = features_dict[\"questionnaire\"]\n",
    "\n",
    "state_vars = demo_vars\n",
    "trait_vars = dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "all_vars = state_vars + dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "\n",
    "print(f'Number of variables: {len(all_vars)}')\n",
    "print(f'   Demographic variables: {len(state_vars)}')\n",
    "print(f'   Daily log variables: {len(dailylog_vars)}')\n",
    "print(f'   Life log variables: {len(lifelog_vars)}')\n",
    "print(f'   Questionnaire variables: {len(questionnaire_vars)}')\n",
    "\n",
    "save_dict_to_file(features_dict, TMP_PATH, \"scraped_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81218b4a",
   "metadata": {},
   "source": [
    "## Load Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a279cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (1965066665.py) <module>: All expected columns are present in scraped_data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 24370\n",
      "Number of columns: 85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>panic</th>\n",
       "      <th>gender</th>\n",
       "      <th>marriage</th>\n",
       "      <th>job</th>\n",
       "      <th>smkHx</th>\n",
       "      <th>drinkHx</th>\n",
       "      <th>suicideHx</th>\n",
       "      <th>suicide_need</th>\n",
       "      <th>...</th>\n",
       "      <th>STAI_X1</th>\n",
       "      <th>SLT1</th>\n",
       "      <th>SLT2</th>\n",
       "      <th>SLT3</th>\n",
       "      <th>SLT4</th>\n",
       "      <th>SLT5</th>\n",
       "      <th>SLT6</th>\n",
       "      <th>total_sleep</th>\n",
       "      <th>smoking</th>\n",
       "      <th>menstruation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.52</td>\n",
       "      <td>7.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYM1-1-102</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID       date  panic  gender  marriage  job  smkHx  drinkHx  \\\n",
       "0  SYM1-1-100 2021-03-02    2.0       1       0.0  0.0    0.0      0.0   \n",
       "1  SYM1-1-100 2021-03-03    2.0       1       0.0  0.0    0.0      0.0   \n",
       "2  SYM1-1-100 2021-03-19    0.0       1       0.0  0.0    0.0      0.0   \n",
       "3  SYM1-1-100 2021-08-12    0.0       1       0.0  0.0    0.0      0.0   \n",
       "4  SYM1-1-102        NaT    0.0       0       0.0  0.0    0.0      0.0   \n",
       "\n",
       "   suicideHx  suicide_need  ...  STAI_X1  SLT1  SLT2  SLT3  SLT4  SLT5  SLT6  \\\n",
       "0        0.0           0.0  ...      NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1        0.0           0.0  ...      NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2        0.0           0.0  ...      NaN   0.0   8.0   0.0  0.00  0.00  0.00   \n",
       "3        0.0           0.0  ...      NaN   0.9   0.0   0.0  3.68  1.23  1.52   \n",
       "4        0.0           0.0  ...      NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   total_sleep  smoking  menstruation  \n",
       "0          NaN      NaN           NaN  \n",
       "1          NaN      NaN           NaN  \n",
       "2         8.00      NaN           NaN  \n",
       "3         7.33      NaN           NaN  \n",
       "4          NaN      NaN           NaN  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scraped_data = pd.read_csv(os.path.join(DATA_PATH, f\"{metadata_filename}.csv\"))\n",
    "\n",
    "# check if all columns are present\n",
    "missing_cols = [col for col in all_vars if col not in scraped_data.columns]\n",
    "if missing_cols:\n",
    "    logging.warning(f\"Missing columns in scraped_data: {missing_cols}\")\n",
    "else:\n",
    "\tlogging.info(\"All expected columns are present in scraped_data.\")\n",
    "# convert date column to datetime format\n",
    "scraped_data['date'] = pd.to_datetime(scraped_data['date'], format='%Y-%m-%d')\n",
    "remove_columns(scraped_data, ['Unnamed: 0'])\n",
    "\n",
    "print(f\"Number of rows: {scraped_data.shape[0]}\")\n",
    "print(f\"Number of columns: {scraped_data.shape[1]}\")\n",
    "display(scraped_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024277b",
   "metadata": {},
   "source": [
    "# ⚒️ | Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1f06",
   "metadata": {},
   "source": [
    "## Initialize Preprocessed Data\n",
    "\n",
    "- add `entry_id` to identify each entry: `'ID'_'date'`\n",
    "- add dataset to identify source: `SYM1`, `SYM2`, `PXPN`\n",
    "- convert `panic` (`0`, `1`, `2` = panic) to days befor panic (`dbp`) (panic = `0`, `1`, `2`)\n",
    "- convert `0` values in `exercise`, `alcohol`, `coffee`, `menstruation`, `smoking` to null values `np.nan` (`NaN`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5a6ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entry IDs: 24370\n"
     ]
    }
   ],
   "source": [
    "data_pre_init = create_empty_df()\n",
    "data_pre_init = scraped_data.copy()\n",
    "\n",
    "# Add 'entry_id' column: unique identifier for each row\n",
    "data_pre_init['entry_id'] = data_pre_init['ID'] + '_' + data_pre_init['date'].astype(str)\n",
    "instance_id_unique = data_pre_init['entry_id'].unique()\n",
    "move_column(data_pre_init, 'entry_id', 0)\n",
    "print(\"Number of unique entry IDs:\", len(instance_id_unique))\n",
    "# Check if 'entry_id' is unique\n",
    "if data_pre_init['entry_id'].duplicated().any():\n",
    "\t# return the rows with duplicate 'entry_id'\n",
    "\tduplicates = data_pre_init[data_pre_init['entry_id'].duplicated(keep=False)]\n",
    "\tprint(f\"Duplicate entry_id found [{len(duplicates)}]:\")\n",
    "\tdisplay(duplicates.head(5))\n",
    "\tsave_as_csv(duplicates, TMP_PATH, f\"duplicates_{metadata_filename}\")\n",
    "\n",
    "# Add 'dataset' column: source of data\n",
    "data_pre_init['dataset'] = data_pre_init['ID'].str.split('_').str[0]\n",
    "data_pre_init['dataset'] = data_pre_init['dataset'].str.split('-').str[0]\n",
    "move_column(data_pre_init, 'dataset', 1)\n",
    "\n",
    "# Convert 'panic' column to Days Before Panic (dbp)\n",
    "data_pre_init['dbp'] = data_pre_init.apply(\n",
    "\tlambda row: np.nan if row['panic'] == 0\n",
    " \t\t\t\telse 0 if row['panic'] == 2 else row['panic'],\n",
    "\taxis=1\n",
    ")\n",
    "remove_columns(data_pre_init, ['panic'])\n",
    "\n",
    "# Convert 'daily_log' variables = 0 to NaN\n",
    "data_pre_init['exercise'] = data_pre_init['exercise'].replace(0, np.nan)\n",
    "data_pre_init['alcohol'] = data_pre_init['alcohol'].replace(0, np.nan)\n",
    "data_pre_init['coffee'] = data_pre_init['coffee'].replace(0, np.nan)\n",
    "data_pre_init['menstruation'] = data_pre_init['menstruation'].replace(0, np.nan)\n",
    "data_pre_init['smoking'] = data_pre_init['smoking'].replace(0, np.nan)\n",
    "\n",
    "# Update the features_dict\n",
    "features_dict['id'] = ['entry_id'] + features_dict['id'] + ['dataset']\n",
    "features_dict['label'] = ['dbp'] + features_dict['label']\n",
    "features_dict['dailylog'].remove('panic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276b12c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>gender</th>\n",
       "      <th>marriage</th>\n",
       "      <th>job</th>\n",
       "      <th>smkHx</th>\n",
       "      <th>drinkHx</th>\n",
       "      <th>suicideHx</th>\n",
       "      <th>...</th>\n",
       "      <th>SLT1</th>\n",
       "      <th>SLT2</th>\n",
       "      <th>SLT3</th>\n",
       "      <th>SLT4</th>\n",
       "      <th>SLT5</th>\n",
       "      <th>SLT6</th>\n",
       "      <th>total_sleep</th>\n",
       "      <th>smoking</th>\n",
       "      <th>menstruation</th>\n",
       "      <th>dbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYM1-1-100_2021-03-02</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SYM1-1-100_2021-03-03</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SYM1-1-100_2021-03-19</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SYM1-1-100_2021-08-12</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.52</td>\n",
       "      <td>7.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYM1-1-102_NaT</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-102</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                entry_id dataset          ID       date  gender  marriage  \\\n",
       "0  SYM1-1-100_2021-03-02    SYM1  SYM1-1-100 2021-03-02       1       0.0   \n",
       "1  SYM1-1-100_2021-03-03    SYM1  SYM1-1-100 2021-03-03       1       0.0   \n",
       "2  SYM1-1-100_2021-03-19    SYM1  SYM1-1-100 2021-03-19       1       0.0   \n",
       "3  SYM1-1-100_2021-08-12    SYM1  SYM1-1-100 2021-08-12       1       0.0   \n",
       "4         SYM1-1-102_NaT    SYM1  SYM1-1-102        NaT       0       0.0   \n",
       "\n",
       "   job  smkHx  drinkHx  suicideHx  ...  SLT1  SLT2  SLT3  SLT4  SLT5  SLT6  \\\n",
       "0  0.0    0.0      0.0        0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1  0.0    0.0      0.0        0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2  0.0    0.0      0.0        0.0  ...   0.0   8.0   0.0  0.00  0.00  0.00   \n",
       "3  0.0    0.0      0.0        0.0  ...   0.9   0.0   0.0  3.68  1.23  1.52   \n",
       "4  0.0    0.0      0.0        0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   total_sleep  smoking  menstruation  dbp  \n",
       "0          NaN      NaN           NaN  0.0  \n",
       "1          NaN      NaN           NaN  0.0  \n",
       "2         8.00      NaN           NaN  NaN  \n",
       "3         7.33      NaN           NaN  NaN  \n",
       "4          NaN      NaN           NaN  NaN  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sources in metadata_ljy:  ['SYM1' 'SYM2' 'PXPN']\n",
      "Number of entries in metadata_ljy: 24370\n",
      "    SYM entries: 23531\n",
      "    PXPN entries: 839\n",
      "Number of unique IDs in metadata_ljy: 429\n",
      "    SYM IDs:  400\n",
      "    PXPN IDs:  29\n",
      "Number of panic events (dbp=0): 802\n"
     ]
    }
   ],
   "source": [
    "display(data_pre_init.head(5))\n",
    "print(\"Unique sources in metadata_ljy: \", data_pre_init['dataset'].unique())\n",
    "print(\"Number of entries in metadata_ljy:\", data_pre_init.shape[0])\n",
    "sym1_n = data_pre_init[data_pre_init['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre_init[data_pre_init['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre_init[data_pre_init['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in metadata_ljy:\", len(data_pre_init['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre_init[data_pre_init['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre_init[data_pre_init['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre_init[data_pre_init['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data_pre_init[data_pre_init['dbp'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cac54",
   "metadata": {},
   "source": [
    "## Initialize Metadata\n",
    "\n",
    "initialize `metadata` by adding\n",
    "- `demography_data` : whether demography data exists in the entry (`boolean`)\n",
    "- `dailylog_data`, `lifelog_data`, `questionnaire_data` : whether each data group exists in the entry (`boolean`)\n",
    "- `dtype_n` : how many of the 3 `state` groups exists in the entry (`int`)\n",
    "- `panic_label` : whether a panic occured in the entry (`boolean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed181f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>gender</th>\n",
       "      <th>marriage</th>\n",
       "      <th>job</th>\n",
       "      <th>smkHx</th>\n",
       "      <th>dtype_n</th>\n",
       "      <th>drinkHx</th>\n",
       "      <th>...</th>\n",
       "      <th>SLT6</th>\n",
       "      <th>total_sleep</th>\n",
       "      <th>smoking</th>\n",
       "      <th>menstruation</th>\n",
       "      <th>dbp</th>\n",
       "      <th>demography_data</th>\n",
       "      <th>dailylog_data</th>\n",
       "      <th>lifelog_data</th>\n",
       "      <th>questionnaire_data</th>\n",
       "      <th>panic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYM1-1-100_2021-03-02</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SYM1-1-100_2021-03-03</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SYM1-1-100_2021-03-19</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SYM1-1-100_2021-08-12</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.52</td>\n",
       "      <td>7.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYM1-1-102_NaT</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-102</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                entry_id dataset          ID       date  gender  marriage  \\\n",
       "0  SYM1-1-100_2021-03-02    SYM1  SYM1-1-100 2021-03-02       1       0.0   \n",
       "1  SYM1-1-100_2021-03-03    SYM1  SYM1-1-100 2021-03-03       1       0.0   \n",
       "2  SYM1-1-100_2021-03-19    SYM1  SYM1-1-100 2021-03-19       1       0.0   \n",
       "3  SYM1-1-100_2021-08-12    SYM1  SYM1-1-100 2021-08-12       1       0.0   \n",
       "4         SYM1-1-102_NaT    SYM1  SYM1-1-102        NaT       0       0.0   \n",
       "\n",
       "   job  smkHx  dtype_n  drinkHx  ...  SLT6  total_sleep  smoking  \\\n",
       "0  0.0    0.0        2      0.0  ...   NaN          NaN      NaN   \n",
       "1  0.0    0.0        2      0.0  ...   NaN          NaN      NaN   \n",
       "2  0.0    0.0        2      0.0  ...  0.00         8.00      NaN   \n",
       "3  0.0    0.0        2      0.0  ...  1.52         7.33      NaN   \n",
       "4  0.0    0.0        0      0.0  ...   NaN          NaN      NaN   \n",
       "\n",
       "   menstruation  dbp  demography_data  dailylog_data  lifelog_data  \\\n",
       "0           NaN  0.0                1              1             0   \n",
       "1           NaN  0.0                1              1             0   \n",
       "2           NaN  NaN                1              0             1   \n",
       "3           NaN  NaN                1              0             1   \n",
       "4           NaN  NaN                1              0             0   \n",
       "\n",
       "   questionnaire_data  panic_label  \n",
       "0                   1            1  \n",
       "1                   1            1  \n",
       "2                   1            0  \n",
       "3                   1            0  \n",
       "4                   0            0  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata_init = create_empty_df()\n",
    "metadata_init = data_pre_init.copy()\n",
    "\n",
    "metadata_init['demography_data'] = metadata_init[features_dict['demography']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['dailylog_data'] = metadata_init[features_dict['dailylog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['lifelog_data'] = metadata_init[features_dict['lifelog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['questionnaire_data'] = metadata_init[features_dict['questionnaire']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['dtype_n'] = metadata_init['dailylog_data'] + metadata_init['lifelog_data'] + metadata_init['questionnaire_data']\n",
    "move_column(metadata_init, 'dtype_n', 8)\n",
    "metadata_init['panic_label'] = metadata_init['dbp'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "add_list = ['dailylog_data', 'lifelog_data', 'questionnaire_data', 'dtype_n']\n",
    "for item in add_list:\n",
    "\tif item not in features_dict['metadata']:\n",
    "\t\tfeatures_dict['metadata'].append(item)\n",
    "del add_list\n",
    "if 'panic_label' not in features_dict['label']:\n",
    "\tfeatures_dict['label'].append('panic_label')\n",
    "\n",
    "check_metadata = False\n",
    "if check_metadata:\n",
    "    check_type = 'dailylog' # demography, dailylog, lifelog, questionnaire\n",
    "    check_for = 1\n",
    "    test = metadata_init[metadata_init[check_type+'_data'] == check_for].copy()\n",
    "    test = test[features_dict['id']+features_dict['metadata']+features_dict[check_type]]\n",
    "    print(f\"--------- TEST {test.shape[0]} ENTRIES WITH {check_type} = {check_for} ---------\")\n",
    "    display(test.head(10))\n",
    "    save_as_csv(test, TMP_PATH, f\"metadata_{check_type}_{check_for}\")\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    del test, check_type, check_for\n",
    "\n",
    "#metadata_init = metadata_init[features_dict['id'] + features_dict['metadata'] + features_dict['demography'] + features_dict['label']]\n",
    "display(metadata_init.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccf3dc",
   "metadata": {},
   "source": [
    "## Extract Demography Data\n",
    "\n",
    "- All patients within the scraped data were confirmed to have demographic data (`demography_data` = `True`)\n",
    "- as such demography_data will not be included in the `metadata`\n",
    "- Demography data was extracted and saved as `demography.csv` to the `output` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc9a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All demographic columns are unique for each ID in demo_data.\n",
      "Number of rows in demo_data: 429\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>marriage</th>\n",
       "      <th>job</th>\n",
       "      <th>smkHx</th>\n",
       "      <th>drinkHx</th>\n",
       "      <th>suicideHx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PXPN_10006</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PXPN_10007</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXPN_10008</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PXPN_10009</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PXPN_10010</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  gender   age  marriage  job  smkHx  drinkHx  suicideHx\n",
       "0  PXPN_10006       0  32.0       0.0  1.0    1.0      1.0        0.0\n",
       "1  PXPN_10007       1  38.0       1.0  1.0    0.0      0.0        0.0\n",
       "2  PXPN_10008       0  38.0       1.0  0.0    0.0      1.0        0.0\n",
       "3  PXPN_10009       1  28.0       0.0  0.0    1.0      0.0        1.0\n",
       "4  PXPN_10010       1  21.0       0.0  0.0    1.0      1.0        0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - (text_utils.py) save_as_csv: Saved demography_(final_result_20250612).csv to C:\\Users\\cyshi\\OneDrive\\Documents\\GitHub\\Panic-Project-CYS\\cys\\_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cyshi/OneDrive/Documents/GitHub/Panic-Project-CYS/cys/_output/demography_(final_result_20250612).csv')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_matrix = [\n",
    "\t('gender_n', 'gender', 'nunique'),\n",
    "\t('age_n', 'age', 'nunique'),\n",
    "\t('marriage_n', 'marriage', 'nunique'),\n",
    "\t('job_n', 'job', 'nunique'),\n",
    "\t('smkHx_n', 'smkHx', 'nunique'),\n",
    "\t('drinkHx_n', 'drinkHx', 'nunique'),\n",
    "\t('suicideHx_n', 'suicideHx', 'nunique'),\n",
    "    ('gender', 'gender', 'first'),\n",
    "\t('age', 'age', 'first'),\n",
    "\t('marriage', 'marriage', 'first'),\n",
    "\t('job', 'job', 'first'),\n",
    "\t('smkHx', 'smkHx', 'first'),\n",
    "\t('drinkHx', 'drinkHx', 'first'),\n",
    "\t('suicideHx', 'suicideHx', 'first')\n",
    "]\n",
    "demo_data = create_empty_df()\n",
    "demo_data = aggregate_by_column(metadata_init, 'ID', agg_matrix)\n",
    "# check if the length of each unique value is 1\n",
    "non_unique_cols = []\n",
    "for col in features_dict['demography']:\n",
    "\tif demo_data[col+'_n'].apply(lambda x: x > 1).any():\n",
    "\t\tnon_unique_cols.append(col)\n",
    "if non_unique_cols:\n",
    "\traise ValueError(f\"Demographic columns {non_unique_cols} are not unique for each ID in demo_data.\")\n",
    "else:\n",
    "\tprint(\"All demographic columns are unique for each ID in demo_data.\")\n",
    "\n",
    "for col in features_dict['demography']:\n",
    "\tremove_columns(demo_data, [col+'_n'])\n",
    "print(f\"Number of rows in demo_data: {demo_data.shape[0]}\")\n",
    "display(demo_data.head(5))\n",
    "\n",
    "save_as_csv(demo_data, OUTPUT_PATH, f\"demography_({metadata_filename})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52f179",
   "metadata": {},
   "source": [
    "## Construct Intermediate Metadata\n",
    "- the current `metadata` (`metadata_init`) was filtered to include only columns for identification, added columns for metadata, and labels\n",
    "- the `metadata` was also filtered to get rid of all entries that only have demography data (`dtype_n` = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b509aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dailylog_data</th>\n",
       "      <th>lifelog_data</th>\n",
       "      <th>questionnaire_data</th>\n",
       "      <th>dtype_n</th>\n",
       "      <th>dbp</th>\n",
       "      <th>panic_label</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23531</th>\n",
       "      <td>PXPN_10006_2024-11-04</td>\n",
       "      <td>PXPN_10006</td>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>PXPN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23532</th>\n",
       "      <td>PXPN_10006_2024-11-05</td>\n",
       "      <td>PXPN_10006</td>\n",
       "      <td>2024-11-05</td>\n",
       "      <td>PXPN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23533</th>\n",
       "      <td>PXPN_10006_2024-11-06</td>\n",
       "      <td>PXPN_10006</td>\n",
       "      <td>2024-11-06</td>\n",
       "      <td>PXPN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23534</th>\n",
       "      <td>PXPN_10006_2024-11-07</td>\n",
       "      <td>PXPN_10006</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>PXPN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23535</th>\n",
       "      <td>PXPN_10006_2024-11-08</td>\n",
       "      <td>PXPN_10006</td>\n",
       "      <td>2024-11-08</td>\n",
       "      <td>PXPN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    entry_id          ID       date dataset  dailylog_data  \\\n",
       "23531  PXPN_10006_2024-11-04  PXPN_10006 2024-11-04    PXPN              1   \n",
       "23532  PXPN_10006_2024-11-05  PXPN_10006 2024-11-05    PXPN              1   \n",
       "23533  PXPN_10006_2024-11-06  PXPN_10006 2024-11-06    PXPN              1   \n",
       "23534  PXPN_10006_2024-11-07  PXPN_10006 2024-11-07    PXPN              1   \n",
       "23535  PXPN_10006_2024-11-08  PXPN_10006 2024-11-08    PXPN              0   \n",
       "\n",
       "       lifelog_data  questionnaire_data  dtype_n  dbp  panic_label  severity  \n",
       "23531             1                   1        3  NaN            0       NaN  \n",
       "23532             1                   1        3  NaN            0       NaN  \n",
       "23533             1                   1        3  1.0            0       NaN  \n",
       "23534             1                   1        3  0.0            1       1.0  \n",
       "23535             1                   1        2  NaN            0       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata_int = create_empty_df()\n",
    "metadata_int = metadata_init.copy()\n",
    "\n",
    "metadata_int = metadata_int[features_dict['id'] + features_dict['metadata'] + features_dict['label']]\n",
    "move_column(metadata_int, 'severity', -1)\n",
    "metadata_int = metadata_int[metadata_int['dtype_n'] > 0]\n",
    "metadata_int.sort_values(by=['ID', 'date'], inplace=True)\n",
    "display(metadata_int.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c967c9",
   "metadata": {},
   "source": [
    "## Filter Preprocessed Data\n",
    "\n",
    "- demographic features were removed from preprocessed data (`data_pre`)\n",
    "- the data was filtered to remove entries with only demgraphic data\n",
    "- the removed IDs were checked to see if no relevant entries were discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e29ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = create_empty_df()\n",
    "data_pre = data_pre_init.copy()\n",
    "# Remove demographic features from data_proc\n",
    "remove_columns(data_pre, features_dict['demography'])\n",
    "# Filter data_proc to keep only rows with entry IDs present in metadata_int\n",
    "metadata_int_unique_ids = metadata_int['entry_id'].unique()\n",
    "data_pre = data_pre[data_pre['entry_id'].isin(metadata_int_unique_ids)]\n",
    "\n",
    "# remove rows with null dates\n",
    "data_pre = data_pre[data_pre['date'].notnull()]\n",
    "\n",
    "# Find IDs present in unfiltered_data but missing in filtered_data (i.e., lost after filtering)\n",
    "check_missing_ids = False\n",
    "if check_missing_ids:\n",
    "\tmissing_ids = np.setdiff1d(data_pre_init['ID'].unique(), data_pre['ID'].unique())\n",
    "\tmissing_data = data_pre_init[data_pre_init['ID'].isin(missing_ids)]\n",
    "\tprint(f\"Number of IDs lost after filtering: {len(missing_ids)}\")\n",
    "\t_ = save_as_csv(missing_data, TMP_PATH, f\"missing_{metadata_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc84e1",
   "metadata": {},
   "source": [
    "## 💾 | Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2daa5322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - (text_utils.py) save_as_csv: Saved panic_pre_(final_result_20250612).csv to C:\\Users\\cyshi\\OneDrive\\Documents\\GitHub\\Panic-Project-CYS\\cys\\_output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>suicide_need</th>\n",
       "      <th>medication_in_month</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>bandpower(0.001-0.0005Hz)</th>\n",
       "      <th>bandpower(0.0005-0.0001Hz)</th>\n",
       "      <th>bandpower(0.0001-0.00005Hz)</th>\n",
       "      <th>...</th>\n",
       "      <th>SLT1</th>\n",
       "      <th>SLT2</th>\n",
       "      <th>SLT3</th>\n",
       "      <th>SLT4</th>\n",
       "      <th>SLT5</th>\n",
       "      <th>SLT6</th>\n",
       "      <th>total_sleep</th>\n",
       "      <th>smoking</th>\n",
       "      <th>menstruation</th>\n",
       "      <th>dbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYM1-1-100_2021-03-02</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SYM1-1-100_2021-03-03</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SYM1-1-100_2021-03-19</td>\n",
       "      <td>SYM1</td>\n",
       "      <td>SYM1-1-100</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                entry_id dataset          ID       date  suicide_need  \\\n",
       "0  SYM1-1-100_2021-03-02    SYM1  SYM1-1-100 2021-03-02           0.0   \n",
       "1  SYM1-1-100_2021-03-03    SYM1  SYM1-1-100 2021-03-03           0.0   \n",
       "2  SYM1-1-100_2021-03-19    SYM1  SYM1-1-100 2021-03-19           0.0   \n",
       "\n",
       "   medication_in_month  alcohol  bandpower(0.001-0.0005Hz)  \\\n",
       "0                  0.0      3.0                        NaN   \n",
       "1                  0.0      3.0                        NaN   \n",
       "2                  0.0      NaN                        NaN   \n",
       "\n",
       "   bandpower(0.0005-0.0001Hz)  bandpower(0.0001-0.00005Hz)  ...  SLT1  SLT2  \\\n",
       "0                         NaN                          NaN  ...   NaN   NaN   \n",
       "1                         NaN                          NaN  ...   NaN   NaN   \n",
       "2                         NaN                          NaN  ...   0.0   8.0   \n",
       "\n",
       "   SLT3  SLT4  SLT5  SLT6  total_sleep  smoking  menstruation  dbp  \n",
       "0   NaN   NaN   NaN   NaN          NaN      NaN           NaN  0.0  \n",
       "1   NaN   NaN   NaN   NaN          NaN      NaN           NaN  0.0  \n",
       "2   0.0   0.0   0.0   0.0          8.0      NaN           NaN  NaN  \n",
       "\n",
       "[3 rows x 80 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Total entries in original:  24370\n",
      "    SYM entries: 23531\n",
      "    PXPN entries: 839\n",
      "Number of unique IDs in original: 429\n",
      "    SYM IDs:  400\n",
      "    PXPN IDs:  29\n",
      "Number of panic events (dbp=0): 802\n",
      "--------------------------------------------------------\n",
      "Total entries in filtered:  ['SYM1' 'SYM2' 'PXPN']\n",
      "    SYM entries: 23282\n",
      "    PXPN entries: 814\n",
      "Number of unique IDs in filtered: 274\n",
      "    SYM IDs:  245\n",
      "    PXPN IDs:  29\n",
      "Number of panic events (dbp=0): 802\n"
     ]
    }
   ],
   "source": [
    "# save data_pre to CSV\n",
    "save_as_csv(data_pre, OUTPUT_PATH, f\"panic_pre_({metadata_filename})\")\n",
    "\n",
    "display(data_pre.head(3))\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Total entries in original: \", data_pre_init.shape[0])\n",
    "sym1_n = data_pre_init[data_pre_init['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre_init[data_pre_init['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre_init[data_pre_init['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in original:\", len(data_pre_init['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre_init[data_pre_init['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre_init[data_pre_init['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre_init[data_pre_init['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data_pre_init[data_pre_init['dbp'] == 0].shape[0])\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Total entries in filtered: \", data_pre['dataset'].unique())\n",
    "sym1_n = data_pre[data_pre['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre[data_pre['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre[data_pre['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in filtered:\", len(data_pre['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre[data_pre['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre[data_pre['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre[data_pre['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data_pre[data_pre['dbp'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e6270",
   "metadata": {},
   "source": [
    "# 📖 | Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e7f97",
   "metadata": {},
   "source": [
    "### Calculate Days Before Panic Features for Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f133fed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "numpy.datetime64('2024-12-02T00:00:00.000000000')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m             metadata_calc\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_prior_data\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m j\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mcalculate_days_before_panic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# patient_ids = metadata_calc['ID'].unique()\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# results = Parallel(n_jobs=-1, backend='loky')(\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#     delayed(calculate_days_before_panic)(pid)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#         for idx, value in updates_dict.items():\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m#             metadata_calc.at[idx, col] = value  # idx is already global\u001b[39;00m\n\u001b[0;32m     65\u001b[0m display(metadata_calc\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m50\u001b[39m))\n",
      "Cell \u001b[1;32mIn[12], line 38\u001b[0m, in \u001b[0;36mcalculate_days_before_panic\u001b[1;34m(patient_id)\u001b[0m\n\u001b[0;32m     36\u001b[0m date_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(entry_dates)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry_date \u001b[38;5;129;01min\u001b[39;00m entry_dates:\n\u001b[1;32m---> 38\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43midx_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mentry_date\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# GLOBAL index\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, lookback_limit \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m lookback_limit:\n",
      "\u001b[1;31mKeyError\u001b[0m: numpy.datetime64('2024-12-02T00:00:00.000000000')"
     ]
    }
   ],
   "source": [
    "# TODO: Not working\n",
    "\n",
    "metadata_calc = create_empty_df()\n",
    "metadata_calc = metadata_int.copy()\n",
    "\n",
    "# Test\n",
    "p_id = 'PXPN_10006'\n",
    "metadata_calc = metadata_calc[metadata_calc['ID'] == p_id]\n",
    "\n",
    "metadata_calc['n_prior_data']    = None\n",
    "metadata_calc['ref_event_id']    = None\n",
    "move_column(metadata_calc, 'panic_label', -1)\n",
    "move_column(metadata_calc, 'severity', -1)\n",
    "metadata_calc.sort_values(by=['ID', 'date'], ascending=False, inplace=True)\n",
    "\n",
    "delta_days = 3\n",
    "lookback_limit = 7\n",
    "\n",
    "def calculate_days_before_panic(patient_id):\n",
    "    patient_data = metadata_calc[metadata_calc['ID'] == patient_id].sort_values('date', ascending=False)\n",
    "    idx_map = dict(zip(patient_data['date'], patient_data.index))  # <-- global indices\n",
    "\n",
    "    entry_dates = patient_data['date'].values\n",
    "    panic_dates = patient_data.loc[patient_data['dbp'] == 0, 'date'].values\n",
    "\n",
    "    for panic_date in panic_dates:\n",
    "        for j in range(1, delta_days + 1):\n",
    "            prior_date = panic_date - pd.Timedelta(days=j)\n",
    "            if prior_date in idx_map:\n",
    "                idx = idx_map[prior_date]  # GLOBAL index\n",
    "                metadata_calc.at[idx, 'dbp'] = j\n",
    "                panic_row = patient_data[patient_data['date'] == panic_date]\n",
    "                if not panic_row.empty:\n",
    "                    metadata_calc.at[idx, 'ref_event_id'] = panic_row.iloc[0]['entry_id']\n",
    "\n",
    "    date_set = set(entry_dates)\n",
    "    for entry_date in entry_dates:\n",
    "        idx = idx_map[entry_date]  # GLOBAL index\n",
    "        for j in range(1, lookback_limit + 1):\n",
    "            if j == lookback_limit:\n",
    "                metadata_calc.at[idx, 'n_prior_data'] = j\n",
    "                break\n",
    "            prior_date = entry_date - pd.Timedelta(days=j)\n",
    "            if prior_date not in date_set:\n",
    "                break\n",
    "            prior_idx = idx_map[prior_date]\n",
    "            if patient_data.loc[prior_idx, 'panic_label'] == 1:\n",
    "                break\n",
    "            metadata_calc.at[idx, 'n_prior_data'] = j\n",
    "\n",
    "# Test\n",
    "calculate_days_before_panic(p_id)\n",
    "\n",
    "# patient_ids = metadata_calc['ID'].unique()\n",
    "# results = Parallel(n_jobs=-1, backend='loky')(\n",
    "#     delayed(calculate_days_before_panic)(pid)\n",
    "#     for pid in tqdm(patient_ids, desc=\"Calculating Days Before Panic\")\n",
    "# )\n",
    "\n",
    "# for updates in results:\n",
    "#     for col, updates_dict in updates.items():\n",
    "#         for idx, value in updates_dict.items():\n",
    "#             metadata_calc.at[idx, col] = value  # idx is already global\n",
    "\n",
    "display(metadata_calc.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_calc.sort_values(by=['ID', 'date'], inplace=True)\n",
    "display(metadata_calc.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da162d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99402307",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'entry_id', 'count'),\n",
    "\t('n_panic_2', 'panic', lambda x: (x == 2).sum())\n",
    "]\n",
    "\n",
    "metadata_ljy_agg = aggregate_by_column(metadata, 'ID', agg_matrix)\n",
    "display(metadata_ljy_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68554311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all IDs that ever had panic==2\n",
    "panic_ids = metadata_ljy_agg.loc[\n",
    "    metadata_ljy_agg['n_panic_2'] > 0, 'ID'\n",
    "].unique()\n",
    "print(\"Unique IDs with panic events (panic=2):\", len(panic_ids))\n",
    "print(f\"Number of panic events (panic=2): {n_panic_2}\")\n",
    "print(\"--------------------------------------\")\n",
    "plot_histogram_of_counts(metadata_ljy_agg['n_panic_2'], title=\"Histogram of Panic Events per ID\", xlabel=\"Number of Panic Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "metadata_agg = aggregate_by_column(metadata, 'ref_event_id', agg_matrix)\n",
    "#display(metadata_agg.head(5))\n",
    "\n",
    "check = metadata_agg[metadata_agg['n_entries'] != metadata_agg['n_dates']]\n",
    "#print(\"Entries where n_entries != n_dates:\")\n",
    "#display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only those rows we marked as panic events\n",
    "panic_data = metadata[['ID', 'date', 'event_id', 'last_panic_days', 'n_prior_data', 'severity']].copy()\n",
    "panic_data = panic_data[panic_data['event_id'].notnull()]\n",
    "\n",
    "panic_data = panic_data[\n",
    "    (panic_data['last_panic_days'] > delta_days) &\n",
    "    (panic_data['n_prior_data'] >= delta_days) &\n",
    "    (panic_data['severity'].notnull())\n",
    "]\n",
    "print(f\"-------- last_panic_days > {delta_days} & n_prior_data ≥ {delta_days} --------\")\n",
    "print(f\"Number of qualifying panic events: {panic_data.shape[0]} out of {n_panic_2} ({panic_data.shape[0] / n_panic_2:.2%})\")\n",
    "print(f\"Unique IDs with panic events: {len(panic_data['ID'].unique())} out of {len(panic_ids)} ({len(panic_data['ID'].unique()) / len(panic_ids):.2%})\")\n",
    "\n",
    "display(panic_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = metadata[(metadata['ref_event_id'].notnull()) | (metadata['event_id'].notnull())].copy()\n",
    "qulifying_event_ids = panic_data['event_id'].unique()\n",
    "qualifying_metadata = filtered_metadata[filtered_metadata['ref_event_id'].isin(qulifying_event_ids)]\n",
    "display(qualifying_metadata.head(10))\n",
    "print(f\"Expected number of columns: {len(all_cols) * 3}\")\n",
    "\n",
    "disp_data = filtered_metadata[['ID', 'date', 'event_id', 'ref_event_id', 'last_panic_days', 'n_prior_data']].copy()\n",
    "display(disp_data.head(50))\n",
    "del disp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "qualifying_metadata_agg = aggregate_by_column(qualifying_metadata, 'ref_event_id', agg_matrix)\n",
    "#display(filtered_metadata_agg.head(5))\n",
    "plot_histogram_of_counts(qualifying_metadata_agg['n_entries'], title=\"Histogram of Entries per Ref Event ID\", xlabel=\"Number of Entries\", bins_step=1)\n",
    "check = qualifying_metadata_agg[qualifying_metadata_agg['n_entries'] != qualifying_metadata_agg['n_dates']]\n",
    "print(\"Entries where n_entries != n_dates:\")\n",
    "display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_panic = metadata['severity'].unique()\n",
    "print(f\"\\nUnique values in 'panic': {unique_panic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhlab-panic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
