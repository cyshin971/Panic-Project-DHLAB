{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f4fd25",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42654e1",
   "metadata": {},
   "source": [
    "# ðŸ“š | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "from library.pandas_utils import move_column, remove_columns, aggregate_by_column, create_empty_df\n",
    "from library.text_utils import save_as_csv\n",
    "# from library.path_utils import get_file_path\n",
    "from library.matplotlib_utils import plot_histogram_of_counts\n",
    "from library.json_utils import save_dict_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa403ca",
   "metadata": {},
   "source": [
    "# ðŸ“ | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../_data\"\n",
    "TMP_PATH = \"./cys/tmp\"\n",
    "OUTPUT_PATH = \"./cys/output\"\n",
    "\n",
    "metadata_filename = \"final_result_20250609_02\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbe0e9",
   "metadata": {},
   "source": [
    "# â›ï¸ | Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f0bc8",
   "metadata": {},
   "source": [
    "Scraped data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {\n",
    "\t\"demography\": [\n",
    "\t\t'gender', 'age', 'marriage', 'job', 'smkHx', 'drinkHx', 'suicideHx'\n",
    "\t],\n",
    "\t\"dailylog\": [\n",
    "\t\t'panic', 'severity', 'exercise', 'alcohol', 'coffee', 'menstruation',\n",
    "\t\t'smoking', 'positive_feeling', 'negative_feeling', 'positive_E', 'negative_E',\n",
    "\t\t'anxiety', 'annoying'\n",
    "\t],\n",
    "\t\"lifelog\": [\n",
    "\t\t'HR_var', 'HR_max', 'HR_mean', 'HR_hvar_mean', 'HR_acrophase', 'HR_amplitude',\n",
    "\t\t'HR_mesor','HR_acrophase_difference', 'HR_acrophase_difference_2d', 'HR_amplitude_difference',\n",
    "\t\t'HR_amplitude_difference_2d', 'HR_mesor_difference', 'HR_mesor_difference_2d',\n",
    "\t\t'bandpower(0.001-0.0005Hz)', 'bandpower(0.0005-0.0001Hz)', 'bandpower(0.0001-0.00005Hz)', 'bandpower(0.00005-0.00001Hz)',\n",
    "\t\t'steps', 'SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep'\n",
    "\t],\n",
    "\t\"questionnaire\": [\n",
    "\t\t'PHQ_9', 'STAI_X2', 'CSM', 'CTQ_1', 'CTQ_2', 'CTQ_3', 'CTQ_4', 'CTQ_5', 'KRQ', 'MDQ',\n",
    "\t\t'ACQ', 'APPQ_1', 'APPQ_2', 'APPQ_3', 'BSQ', 'GAD_7', 'BRIAN'\n",
    "\t],\n",
    "    \"id\": [\n",
    "        'ID', 'date'\n",
    "    ],\n",
    "    \"label\": [\n",
    "        'severity'\n",
    "    ],\n",
    "    \"metadata\": []\n",
    "}\n",
    "\n",
    "demo_vars = features_dict[\"demography\"]\n",
    "dailylog_vars = features_dict[\"dailylog\"]\n",
    "lifelog_vars = features_dict[\"lifelog\"]\n",
    "questionnaire_vars = features_dict[\"questionnaire\"]\n",
    "\n",
    "state_vars = demo_vars\n",
    "trait_vars = dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "all_vars = state_vars + dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "\n",
    "print(f'Number of variables: {len(all_vars)}')\n",
    "print(f'   Demographic variables: {len(state_vars)}')\n",
    "print(f'   Daily log variables: {len(dailylog_vars)}')\n",
    "print(f'   Life log variables: {len(lifelog_vars)}')\n",
    "print(f'   Questionnaire variables: {len(questionnaire_vars)}')\n",
    "\n",
    "save_dict_to_file(features_dict, OUTPUT_PATH+'/scraped', \"scraped_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81218b4a",
   "metadata": {},
   "source": [
    "## Load Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70f947",
   "metadata": {},
   "source": [
    "load preprocessed data (`junyeol_lee`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = pd.read_csv(os.path.join(DATA_PATH, f\"{metadata_filename}.csv\"))\n",
    "\n",
    "# check if all columns are present\n",
    "missing_cols = [col for col in all_vars if col not in scraped_data.columns]\n",
    "if missing_cols:\n",
    "    logging.warning(f\"Missing columns in scraped_data: {missing_cols}\")\n",
    "else:\n",
    "\tlogging.info(\"All expected columns are present in scraped_data.\")\n",
    "# convert date column to datetime format\n",
    "scraped_data['date'] = pd.to_datetime(scraped_data['date'], format='%Y-%m-%d')\n",
    "remove_columns(scraped_data, ['Unnamed: 0'])\n",
    "\n",
    "print(f\"Number of rows: {scraped_data.shape[0]}\")\n",
    "print(f\"Number of columns: {scraped_data.shape[1]}\")\n",
    "display(scraped_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbccebb",
   "metadata": {},
   "source": [
    "Add more descriptors to metadata\n",
    "- `entry_id`\n",
    "- `dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024277b",
   "metadata": {},
   "source": [
    "# ðŸ“– | Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1f06",
   "metadata": {},
   "source": [
    "## Initialize Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scraped_data.copy()\n",
    "\n",
    "# Add 'entry_id' column: unique identifier for each row\n",
    "data['entry_id'] = data['ID'] + '_' + data['date'].astype(str)\n",
    "instance_id_unique = data['entry_id'].unique()\n",
    "move_column(data, 'entry_id', 0)\n",
    "print(\"Number of unique entry IDs:\", len(instance_id_unique))\n",
    "# Check if 'entry_id' is unique\n",
    "if data['entry_id'].duplicated().any():\n",
    "\t# return the rows with duplicate 'entry_id'\n",
    "\tduplicates = data[data['entry_id'].duplicated(keep=False)]\n",
    "\tprint(f\"Duplicate entry_id found [{len(duplicates)}]:\")\n",
    "\tdisplay(duplicates.head(5))\n",
    "\tsave_as_csv(duplicates, TMP_PATH, f\"duplicates_{metadata_filename}\")\n",
    "\n",
    "# Add 'dataset' column: source of data\n",
    "data['dataset'] = data['ID'].str.split('_').str[0]\n",
    "data['dataset'] = data['dataset'].str.split('-').str[0]\n",
    "move_column(data, 'dataset', 1)\n",
    "\n",
    "# Convert 'panic' column to Days Before Panic (dbp)\n",
    "data['dbp'] = data.apply(\n",
    "\tlambda row: np.nan if row['panic'] == 0\n",
    " \t\t\t\telse 0 if row['panic'] == 2 else row['panic'],\n",
    "\taxis=1\n",
    ")\n",
    "remove_columns(data, ['panic'])\n",
    "\n",
    "# Convert 'daily_log' variables = 0 to NaN\n",
    "data['exercise'] = data['exercise'].replace(0, np.nan)\n",
    "data['alcohol'] = data['alcohol'].replace(0, np.nan)\n",
    "data['coffee'] = data['coffee'].replace(0, np.nan)\n",
    "data['menstruation'] = data['menstruation'].replace(0, np.nan)\n",
    "data['smoking'] = data['smoking'].replace(0, np.nan)\n",
    "\n",
    "# Update the features_dict\n",
    "features_dict['id'] = ['entry_id'] + features_dict['id'] + ['dataset']\n",
    "features_dict['label'] = ['dbp'] + features_dict['label']\n",
    "features_dict['dailylog'].remove('panic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.head(5))\n",
    "print(\"Unique sources in metadata_ljy: \", data['dataset'].unique())\n",
    "sym1_n = data[data['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data[data['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data[data['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in metadata_ljy:\", len(data['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data[data['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data[data['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data[data['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data[data['dbp'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cac54",
   "metadata": {},
   "source": [
    "## Initialize Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed181f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_init = create_empty_df()\n",
    "metadata_init = data.copy()\n",
    "\n",
    "metadata_init['demography_data'] = metadata_init[features_dict['demography']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['dailylog_data'] = metadata_init[features_dict['dailylog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['lifelog_data'] = metadata_init[features_dict['lifelog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['questionnaire_data'] = metadata_init[features_dict['questionnaire']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['dtype_n'] = metadata_init['dailylog_data'] + metadata_init['lifelog_data'] + metadata_init['questionnaire_data']\n",
    "move_column(metadata_init, 'dtype_n', 8)\n",
    "metadata_init['panic_label'] = metadata_init['dbp'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "add_list = ['dailylog_data', 'lifelog_data', 'questionnaire_data', 'dtype_n']\n",
    "for item in add_list:\n",
    "\tif item not in features_dict['metadata']:\n",
    "\t\tfeatures_dict['metadata'].append(item)\n",
    "del add_list\n",
    "if 'panic_label' not in features_dict['label']:\n",
    "\tfeatures_dict['label'].append('panic_label')\n",
    "\n",
    "check_metadata = False\n",
    "if check_metadata:\n",
    "    check_type = 'dailylog' # demography, dailylog, lifelog, questionnaire\n",
    "    check_for = 1\n",
    "    test = metadata_init[metadata_init[check_type+'_data'] == check_for].copy()\n",
    "    test = test[features_dict['id']+features_dict['metadata']+features_dict[check_type]]\n",
    "    print(f\"--------- TEST {test.shape[0]} ENTRIES WITH {check_type} = {check_for} ---------\")\n",
    "    display(test.head(10))\n",
    "    save_as_csv(test, TMP_PATH, f\"metadata_{check_type}_{check_for}\")\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    del test, check_type, check_for\n",
    "\n",
    "#metadata_init = metadata_init[features_dict['id'] + features_dict['metadata'] + features_dict['demography'] + features_dict['label']]\n",
    "display(metadata_init.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccf3dc",
   "metadata": {},
   "source": [
    "## Extract Demography Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('gender_n', 'gender', 'nunique'),\n",
    "\t('age_n', 'age', 'nunique'),\n",
    "\t('marriage_n', 'marriage', 'nunique'),\n",
    "\t('job_n', 'job', 'nunique'),\n",
    "\t('smkHx_n', 'smkHx', 'nunique'),\n",
    "\t('drinkHx_n', 'drinkHx', 'nunique'),\n",
    "\t('suicideHx_n', 'suicideHx', 'nunique'),\n",
    "    ('gender', 'gender', 'first'),\n",
    "\t('age', 'age', 'first'),\n",
    "\t('marriage', 'marriage', 'first'),\n",
    "\t('job', 'job', 'first'),\n",
    "\t('smkHx', 'smkHx', 'first'),\n",
    "\t('drinkHx', 'drinkHx', 'first'),\n",
    "\t('suicideHx', 'suicideHx', 'first')\n",
    "]\n",
    "demo_data = create_empty_df()\n",
    "demo_data = aggregate_by_column(metadata_init, 'ID', agg_matrix)\n",
    "# check if the length of each unique value is 1\n",
    "non_unique_cols = []\n",
    "for col in features_dict['demography']:\n",
    "\tif demo_data[col+'_n'].apply(lambda x: x > 1).any():\n",
    "\t\tnon_unique_cols.append(col)\n",
    "if non_unique_cols:\n",
    "\traise ValueError(f\"Demographic columns {non_unique_cols} are not unique for each ID in demo_data.\")\n",
    "else:\n",
    "\tprint(\"All demographic columns are unique for each ID in demo_data.\")\n",
    "\n",
    "for col in features_dict['demography']:\n",
    "\tremove_columns(demo_data, [col+'_n'])\n",
    "print(f\"Number of rows in demo_data: {demo_data.shape[0]}\")\n",
    "display(demo_data.head(5))\n",
    "\n",
    "save_as_csv(demo_data, OUTPUT_PATH+'/analysis', f\"demography\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52f179",
   "metadata": {},
   "source": [
    "## Construct Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b509aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = create_empty_df()\n",
    "metadata = metadata_init.copy()\n",
    "\n",
    "metadata = metadata[features_dict['id'] + features_dict['metadata'] + features_dict['label']]\n",
    "move_column(metadata, 'severity', -1)\n",
    "metadata = metadata[metadata['dtype_n'] > 0]\n",
    "display(metadata.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c967c9",
   "metadata": {},
   "source": [
    "## Filter Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_data = data.copy()\n",
    "print(\"Total entries in original: \", unfiltered_data.shape[0])\n",
    "sym1_n = unfiltered_data[unfiltered_data['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = unfiltered_data[unfiltered_data['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", unfiltered_data[unfiltered_data['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in original:\", len(unfiltered_data['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = unfiltered_data[unfiltered_data['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = unfiltered_data[unfiltered_data['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(unfiltered_data[unfiltered_data['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", unfiltered_data[unfiltered_data['dbp'] == 0].shape[0])\n",
    "\n",
    "# remove rows with null dates\n",
    "data = data[data['date'].notnull()]\n",
    "# display(data.head(5))\n",
    "\n",
    "print(\"Total entries in filtered: \", data['dataset'].unique())\n",
    "sym1_n = data[data['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data[data['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data[data['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in filtered:\", len(data['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data[data['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data[data['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data[data['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (dbp=0):\", data[data['dbp'] == 0].shape[0])\n",
    "\n",
    "# Find IDs present in unfiltered_data but missing in filtered_data (i.e., lost after filtering)\n",
    "missing_ids = np.setdiff1d(unfiltered_data['ID'].unique(), data['ID'].unique())\n",
    "missing_data = unfiltered_data[unfiltered_data['ID'].isin(missing_ids)]\n",
    "print(f\"Number of IDs lost after filtering: {len(missing_ids)}\")\n",
    "_ = save_as_csv(missing_data, TMP_PATH, f\"missing_{metadata_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e7f97",
   "metadata": {},
   "source": [
    "### Calculate Days Before Panic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_days = 3\n",
    "\n",
    "# calc_metadata = metadata.copy()\n",
    "# calc_metadata.sort_values(by=['ID', 'date'], ascending=False, inplace=True)\n",
    "\n",
    "# calc_metadata['n_prior_data']    = None\n",
    "# calc_metadata['ref_event_id']    = None\n",
    "# move_column(calc_metadata, 'panic_label', -1)\n",
    "# move_column(calc_metadata, 'severity', -1)\n",
    "\n",
    "# display(calc_metadata.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58023dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_days = 3\n",
    "\n",
    "# calc_metadata = metadata.copy()\n",
    "\n",
    "# calc_metadata['last_panic_days'] = None\n",
    "# calc_metadata['n_prior_data']    = None\n",
    "# calc_metadata['ref_event_id']    = None\n",
    "# move_column(calc_metadata, 'panic_label', -1)\n",
    "# move_column(calc_metadata, 'severity', -1)\n",
    "\n",
    "# for id in calc_metadata['ID'].unique():\n",
    "#     id_data = calc_metadata[calc_metadata['ID'] == id].sort_values(by='date')\n",
    "#     # Build a set of all observed dates for quick membership tests\n",
    "#     date_set = set(id_data['date'])\n",
    "    \n",
    "#     last_panic_date = None\n",
    "    \n",
    "#     for idx, row in id_data.iterrows():\n",
    "#         if row['panic_label'] == 1:\n",
    "#             calc_metadata.loc[idx, 'ref_event_id'] = row['entry_id']\n",
    "#             if last_panic_date is None:\n",
    "#                 # This is the very first panic for this subject\n",
    "#                 calc_metadata.loc[idx, 'last_panic_days'] = -1\n",
    "#             else:\n",
    "#                 days_diff = (row['date'] - last_panic_date).days\n",
    "#                 calc_metadata.loc[idx, 'last_panic_days'] = days_diff\n",
    "#             last_panic_date = row['date']\n",
    "        \n",
    "#         # Count how many consecutive prior days (up to 100) exist before this panic\n",
    "#         # We look backward one day at a time. If we hit another panic, or a missing date, we break.\n",
    "#         current_date = row['date']\n",
    "#         found_n_prior = False\n",
    "\n",
    "#         for j in range(1, 101):  # j = 1, 2, â€¦, 100\n",
    "#             look_date = current_date - pd.Timedelta(days=j)\n",
    "\n",
    "#             if look_date not in date_set:\n",
    "#                 # We encountered a missing day. That means only (j-1) consecutive prior days exist.\n",
    "#                 calc_metadata.loc[idx, 'n_prior_data'] = j - 1\n",
    "#                 found_n_prior = True\n",
    "#                 break\n",
    "\n",
    "#             # There *is* at least one row on look_dateâ€”grab it (or them)\n",
    "#             rows_on_that_date = id_data[id_data['date'] == look_date]\n",
    "#             if len(rows_on_that_date) > 1:\n",
    "#                 raise ValueError(f\"Warning: More than one row for date {look_date} for ID {row['ID']}\")\n",
    "#             # If ANY of those rows had panic==2, we stop and record (j-1):\n",
    "#             if (rows_on_that_date['panic_label'] == 1).any():\n",
    "#                 calc_metadata.loc[idx, 'n_prior_data'] = j - 1\n",
    "#                 found_n_prior = True\n",
    "#                 break\n",
    "\n",
    "#             if j in range(1, delta_days + 1):\n",
    "#                 # Set the 'ref_event_id' to event_id for all the rows for rows_on_that_date if we are within the delta_days\n",
    "#                 calc_metadata.loc[rows_on_that_date.index, 'ref_event_id'] = row['entry_id']\n",
    "\n",
    "#         if not found_n_prior:\n",
    "#             # We never hit a prior panic or missing day within 100 days â†’ cap at 100\n",
    "#             calc_metadata.loc[idx, 'n_prior_data'] = 100\n",
    "\n",
    "# display(calc_metadata.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# panic_ids = metadata[metadata['dbp'] == 0]['ID'].unique()\n",
    "\n",
    "# for panic_id in panic_ids:\n",
    "#     # Extract only rows for this subject, sorted by date ascending\n",
    "#     df = metadata.loc[metadata['ID'] == panic_id].sort_values('date', ascending=True)\n",
    "\n",
    "#     # Build a set of all observed dates for quick membership tests\n",
    "#     date_set = set(df['date'])\n",
    "\n",
    "#     last_panic_date = None\n",
    "\n",
    "#     for idx, row in df.iterrows():\n",
    "\n",
    "#         # Compute last_panic_days\n",
    "#         if last_panic_date is None:\n",
    "#             # This is the very first panic for this subject\n",
    "#             metadata.loc[idx, 'last_panic_days'] = 100\n",
    "#         else:\n",
    "#             days_diff = (row['date'] - last_panic_date).days\n",
    "#             metadata.loc[idx, 'last_panic_days'] = days_diff\n",
    "\n",
    "#         # Count how many consecutive prior days (up to 100) exist before this panic\n",
    "#         # We look backward one day at a time. If we hit another panic, or a missing date, we break.\n",
    "#         current_date = row['date']\n",
    "#         found_n_prior = False\n",
    "\n",
    "#         for j in range(1, 101):  # j = 1, 2, â€¦, 100\n",
    "#             look_date = current_date - pd.Timedelta(days=j)\n",
    "\n",
    "#             if look_date not in date_set:\n",
    "#                 # We encountered a missing day. That means only (j-1) consecutive prior days exist.\n",
    "#                 metadata.loc[idx, 'n_prior_data'] = j - 1\n",
    "#                 found_n_prior = True\n",
    "#                 break\n",
    "\n",
    "#             # There *is* at least one row on look_dateâ€”grab it (or them)\n",
    "#             rows_on_that_date = df[df['date'] == look_date]\n",
    "#             if len(rows_on_that_date) > 1:\n",
    "#                 print(f\"Warning: More than one row for date {look_date} for ID {panic_id}\")\n",
    "#             # If ANY of those rows had panic==2, we stop and record (j-1):\n",
    "#             if (rows_on_that_date['panic'] == 2).any():\n",
    "#                 metadata.loc[idx, 'n_prior_data'] = j - 1\n",
    "#                 found_n_prior = True\n",
    "#                 break\n",
    "\n",
    "#             if j in range(1, delta_days + 1):\n",
    "#                 # Set the 'ref_event_id' to event_id for all the rows for rows_on_that_date if we are within the delta_days\n",
    "#                 metadata.loc[rows_on_that_date.index, 'ref_event_id'] = event_id\n",
    "  \n",
    "#         if not found_n_prior:\n",
    "#             # We never hit a prior panic or missing day within 100 days â†’ cap at 100\n",
    "#             metadata.loc[idx, 'n_prior_data'] = 100\n",
    "\n",
    "#         # update last_panic_date so that the next panic (if any) calculates correctly\n",
    "#         last_panic_date = row['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da162d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99402307",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'entry_id', 'count'),\n",
    "\t('n_panic_2', 'panic', lambda x: (x == 2).sum())\n",
    "]\n",
    "\n",
    "metadata_ljy_agg = aggregate_by_column(metadata, 'ID', agg_matrix)\n",
    "display(metadata_ljy_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68554311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all IDs that ever had panic==2\n",
    "panic_ids = metadata_ljy_agg.loc[\n",
    "    metadata_ljy_agg['n_panic_2'] > 0, 'ID'\n",
    "].unique()\n",
    "print(\"Unique IDs with panic events (panic=2):\", len(panic_ids))\n",
    "print(f\"Number of panic events (panic=2): {n_panic_2}\")\n",
    "print(\"--------------------------------------\")\n",
    "plot_histogram_of_counts(metadata_ljy_agg['n_panic_2'], title=\"Histogram of Panic Events per ID\", xlabel=\"Number of Panic Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "metadata_agg = aggregate_by_column(metadata, 'ref_event_id', agg_matrix)\n",
    "#display(metadata_agg.head(5))\n",
    "\n",
    "check = metadata_agg[metadata_agg['n_entries'] != metadata_agg['n_dates']]\n",
    "#print(\"Entries where n_entries != n_dates:\")\n",
    "#display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only those rows we marked as panic events\n",
    "panic_data = metadata[['ID', 'date', 'event_id', 'last_panic_days', 'n_prior_data', 'severity']].copy()\n",
    "panic_data = panic_data[panic_data['event_id'].notnull()]\n",
    "\n",
    "panic_data = panic_data[\n",
    "    (panic_data['last_panic_days'] > delta_days) &\n",
    "    (panic_data['n_prior_data'] >= delta_days) &\n",
    "    (panic_data['severity'].notnull())\n",
    "]\n",
    "print(f\"-------- last_panic_days > {delta_days} & n_prior_data â‰¥ {delta_days} --------\")\n",
    "print(f\"Number of qualifying panic events: {panic_data.shape[0]} out of {n_panic_2} ({panic_data.shape[0] / n_panic_2:.2%})\")\n",
    "print(f\"Unique IDs with panic events: {len(panic_data['ID'].unique())} out of {len(panic_ids)} ({len(panic_data['ID'].unique()) / len(panic_ids):.2%})\")\n",
    "\n",
    "display(panic_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = metadata[(metadata['ref_event_id'].notnull()) | (metadata['event_id'].notnull())].copy()\n",
    "qulifying_event_ids = panic_data['event_id'].unique()\n",
    "qualifying_metadata = filtered_metadata[filtered_metadata['ref_event_id'].isin(qulifying_event_ids)]\n",
    "display(qualifying_metadata.head(10))\n",
    "print(f\"Expected number of columns: {len(all_cols) * 3}\")\n",
    "\n",
    "disp_data = filtered_metadata[['ID', 'date', 'event_id', 'ref_event_id', 'last_panic_days', 'n_prior_data']].copy()\n",
    "display(disp_data.head(50))\n",
    "del disp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'ref_event_id', 'count'),\n",
    "\t('n_dates', 'date', 'nunique')\n",
    "]\n",
    "\n",
    "qualifying_metadata_agg = aggregate_by_column(qualifying_metadata, 'ref_event_id', agg_matrix)\n",
    "#display(filtered_metadata_agg.head(5))\n",
    "plot_histogram_of_counts(qualifying_metadata_agg['n_entries'], title=\"Histogram of Entries per Ref Event ID\", xlabel=\"Number of Entries\", bins_step=1)\n",
    "check = qualifying_metadata_agg[qualifying_metadata_agg['n_entries'] != qualifying_metadata_agg['n_dates']]\n",
    "print(\"Entries where n_entries != n_dates:\")\n",
    "display(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_panic = metadata['severity'].unique()\n",
    "print(f\"\\nUnique values in 'panic': {unique_panic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhlab-panic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
