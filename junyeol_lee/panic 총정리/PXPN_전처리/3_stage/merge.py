import pandas as pd
import zipfile
from io import BytesIO


import pandas as pd
from functools import reduce

# (1) CSV íŒŒì¼ ë¡œë“œ & 'Unnamed' ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì œê±° í•¨ìˆ˜
def load_and_clean(path):
    df = pd.read_csv(path)
    # Strip whitespace from column names
    df.columns = df.columns.str.strip()
    df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
    return df

# (2) ëª¨ë“  CSV ë¶ˆëŸ¬ì˜¤ê¸°
preprocessed = load_and_clean("/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/1_stage/ì „ì²˜ë¦¬ ê²°ê³¼/processed.csv")
band_power            = load_and_clean("/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/2_stage/processed/bandpower.csv")
circadian_delta       = load_and_clean("/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/2_stage/processed/circadian_delta.csv")
step_delta = load_and_clean("/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/2_stage/processed/step_delta.csv")
HR_date               = load_and_clean("/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/2_stage/processed/HR_date.csv")
sleep                 = load_and_clean("/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/2_stage/processed/sleep_type.csv")

# (3) ë‚ ì§œ ê¸°ë°˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
date_dfs = [
    preprocessed,
    band_power,
    circadian_delta,
    step_delta,
    HR_date,
    sleep,
]

# (3.5) ëª¨ë“  date ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
for df in date_dfs:
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')

# (5) ëª¨ë“  (ID, date) ì¡°í•©ì„ ë§ˆìŠ¤í„° í‚¤ë¡œ ìƒì„±
all_keys = pd.concat([df[['ID', 'date']] for df in date_dfs])
all_keys = all_keys.drop_duplicates().dropna()

# (6) ê° dfë¥¼ ë§ˆìŠ¤í„° í‚¤ ê¸°ì¤€ìœ¼ë¡œ align
def align_to_master(df):
    return pd.merge(all_keys, df, how='left', on=['ID', 'date'])

aligned_dfs = [align_to_master(df) for df in date_dfs]

# (7) ëª¨ë“  ë‚ ì§œë³„ í…Œì´ë¸”ì„ outer join ìœ¼ë¡œ ìˆœì°¨ ë³‘í•© (ìˆœì„œ ì˜í–¥ ì—†ìŒ)
merged_all = reduce(lambda left, right: pd.merge(left, right, how='outer', on=['ID', 'date']), aligned_dfs)


# (9) panic ê°’ì´ ì˜ ì‚´ì•„ ìˆëŠ”ì§€ í™•ì¸
print("ğŸ“Œ panic value counts (before any drop/fill):")
print(merged_all['panic'].value_counts(dropna=False))

# (10) ffill ëŒ€ìƒ ì»¬ëŸ¼ ì •ì˜
survey_cols = [
    'BRIAN','CSM','CTQ_1','CTQ_2','CTQ_3','CTQ_4','CTQ_5',
    'KRQ','MDQ','SPAQ_1','SPAQ_2','STAI_X2','ACQ',
    'APPQ_1','APPQ_2','APPQ_3','BSQ','BFNE','CES_D',
    'GAD_7','KOSSSF','PHQ_9','SADS','STAI_X1'
]
band_cols   = [c for c in merged_all.columns if c.startswith('bandpower')]
ffill_cols  = [c for c in survey_cols + band_cols if c in merged_all.columns]

# panic ì»¬ëŸ¼ì€ ffill ëŒ€ìƒì—ì„œ ì œì™¸
merged_full = merged_all.sort_values(['ID','date'])
merged_full[ffill_cols] = merged_full.groupby('ID')[ffill_cols].ffill()

# (11) ì»¬ëŸ¼ ì •ë¦¬ ë° ê²°ì¸¡ ì²˜ë¦¬
merged_full.rename(columns={
    'amp': 'HR_amplitude',           'mesor': 'HR_mesor',         'acr': 'HR_acrophase',
    'amp_delta': 'HR_amplitude_difference',  'mesor_delta': 'HR_mesor_difference',   'acr_delta': 'HR_acrophase_difference',
    'amp_delta2': 'HR_amplitude_difference_2d',  'mesor_delta2': 'HR_mesor_difference_2d',  'acr_delta2': 'HR_acrophase_difference_2d',
    'negative': 'negative_feeling',
    'step_max': 'steps_maximum',    'step_var': 'steps_variance',
    'step_mean': 'steps_mean', 
    'bandpower_a': 'bandpower(0.001-0.0005Hz)', 
    'bandpower_b': 'bandpower(0.0005-0.0001Hz)',
    'bandpower_c': 'bandpower(0.0001-0.00005Hz)', 
    'bandpower_d': 'bandpower(0.00005-0.00001Hz)',
    'suicide_need_in_month': 'suicide_need'
}, inplace=True)


merged_full['alcohol'] = merged_full['alcohol'].fillna(0)
merged_full['exercise'] = merged_full['exercise'].fillna(0)
merged_full['coffee'] = merged_full['coffee'].fillna(0)
merged_full['smoking'] = merged_full['smoking'].fillna(0)
merged_full['menstruation'] = merged_full['menstruation'].fillna(0)

# (12) date ì»¬ëŸ¼ì´ datetime íƒ€ì…ì¸ ê²½ìš°, ë¬¸ìì—´ YYYY-MM-DDë¡œ ë³€í™˜
if 'date' in merged_full.columns:
    merged_full['date'] = merged_full['date'].dt.strftime('%Y-%m-%d')

# (13) ì»¬ëŸ¼ ìˆœì„œ: ID, date, panic â†’ ë‚˜ë¨¸ì§€
cols = merged_full.columns.tolist()
ordered_cols = ['ID', 'date', 'panic'] + [c for c in cols if c not in ['ID', 'date', 'panic']]
merged_full = merged_full[ordered_cols]

merged_full.to_csv("/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/result/result_before_severity.csv")


# 1. Load all_data and prepare the 'severity' column
all_data_path = "/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/result/result_before_severity.csv"
all_data = pd.read_csv(all_data_path, dtype={"ID": str})

# Ensure 'date' is in datetime.date format for matching
all_data["date"] = pd.to_datetime(all_data["date"]).dt.date

# Initialize a new column 'severity' with NaN
all_data["severity"] = pd.NA


# 2. Fill 'severity' for PXPN-group patients by reading each patient's panic CSV inside the nested ZIP
zip_path = "/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/1_stage/ì´ˆê¸°_íŒŒì¼_í´ë”/PXPN/PXPN_dropbox/pixelpanic_raw_data.zip"
with zipfile.ZipFile(zip_path, "r") as outer_zip:
    # patient indices run from 6 to 40 (inclusive)
    for i in range(6, 41):
        formatted_index = f"{i:02d}"
        patient_code = f"PXPN_100{formatted_index}"
        inner_zip_name = f"ActiveData/{patient_code}_ActiveData.zip"

        # Skip if the inner zip for this patient isn't present
        if inner_zip_name not in outer_zip.namelist():
            continue

        # Read the inner ZIP from the outer ZIP into memory
        inner_zip_bytes = BytesIO(outer_zip.read(inner_zip_name))
        with zipfile.ZipFile(inner_zip_bytes, "r") as inner_zip:
            inner_file_name = f"{patient_code}_Panic.csv"
            if inner_file_name not in inner_zip.namelist():
                continue

            # Open the patient's panic CSV
            with inner_zip.open(inner_file_name) as f:
                df_panic = pd.read_csv(f, dtype={"ê°•ë„": float})

            # Convert the ì‘ì„±ì¼ column to datetime.date
            df_panic["ì‘ì„±ì¼"] = pd.to_datetime(df_panic["ì‘ì„±ì¼"]).dt.date

            # For each row in df_panic, match ID & date, then assign 'ê°•ë„' to all_data['severity']
            for _, row in df_panic.iterrows():
                panic_date = row["ì‘ì„±ì¼"]
                severity_val = row["ê°•ë„"]

                mask = (all_data["ID"] == patient_code) & (all_data["date"] == panic_date)
                if mask.any():
                    all_data.loc[mask, "severity"] = severity_val


# 5. (Optional) Check how many rows still have NaN in severity
num_missing = all_data["severity"].isna().sum()
print(f"Number of rows with missing severity: {num_missing}")

# 6. Save the updated DataFrame
output_path = "/Users/lee-junyeol/Downloads/panic ì´ì •ë¦¬/PXPN_ì „ì²˜ë¦¬/result/result.csv"
all_data.to_csv(output_path, index=False)