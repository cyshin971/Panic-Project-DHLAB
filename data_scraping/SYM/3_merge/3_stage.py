import pandas as pd
from functools import reduce
import os
output_folder = ("/Panic-Project-DHLAB/tmp/SYM")

# (1) CSV íŒŒì¼ ë¡œë“œ & 'Unnamed' ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì œê±° í•¨ìˆ˜
def load_and_clean(path):
    df = pd.read_csv(path)
    df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
    return df

# (2) ëª¨ë“  CSV ë¶ˆëŸ¬ì˜¤ê¸°
output_path = os.path.join(output_folder, "start_date.csv")
start_date = load_and_clean(output_path)
start_date['start_date'] = pd.to_datetime(start_date['start_date'], errors='coerce')
output_path = os.path.join(output_folder, "alcohol_per_date.csv")
Alcohol_per_date      = load_and_clean(output_path)
output_path = os.path.join(output_folder, "bandpower_fixed_720.csv")
band_power            = load_and_clean(output_path)
output_path = os.path.join(output_folder, "circadian_delta_720.csv")
circadian_delta       = load_and_clean(output_path)
output_path = os.path.join(output_folder, "coffee_per_date.csv")
coffee_date           = load_and_clean(output_path)
output_path = os.path.join(output_folder, "emotion_diary.csv")
emotion_diary         = load_and_clean(output_path)
output_path = os.path.join(output_folder, "exercise_per_date.csv")
exercise_date         = load_and_clean(output_path)
output_path = os.path.join(output_folder, "step_delta.csv")
step_delta = load_and_clean(output_path)
output_path = os.path.join(output_folder, "HR_date_fixed.csv")
HR_date               = load_and_clean(output_path)
output_path = os.path.join(output_folder, "panic_by_date.csv")
panic                 = load_and_clean(output_path).drop(columns=['time','datetime'], errors='ignore')
output_path = os.path.join(output_folder, "questionnaire.csv")
questionnaire_bydate  = load_and_clean(output_path)
output_path = os.path.join(output_folder, "sleep_summary.csv")
sleep                 = load_and_clean(output_path)
output_path = os.path.join(output_folder, "smoking_diet_mens.csv")
smoking_diet_mens     = load_and_clean(output_path)
output_path = os.path.join(output_folder, "demographic_data.csv")
demographic_data      = load_and_clean(output_path)
output_path = os.path.join(output_folder, "diary.csv")
diary = load_and_clean(output_path)
# (3) ë‚ ì§œ ê¸°ë°˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
date_dfs = [
    Alcohol_per_date,
    band_power,
    circadian_delta,
    coffee_date,
    emotion_diary,
    exercise_date,
    step_delta,
    HR_date,
    panic,
    questionnaire_bydate,
    sleep,
    smoking_diet_mens,
    diary
]


# (3.5) ëª¨ë“  date ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜
for df in date_dfs:
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')

# (3.6) Check for records with a date earlier than the observed min_date for each ID
# Using min_date computed below in section (5.1)
# First, ensure min_date is computed before this block (move section (5.1) above section (3.6)), or compute here:
min_date = (
    pd.concat([df[['ID','date']] for df in date_dfs if 'date' in df.columns])
    .dropna()
    .assign(date=lambda x: pd.to_datetime(x['date'], errors='coerce'))
    .groupby('ID')['date']
    .min()
    .reset_index()
    .rename(columns={'date':'min_date'})
)
for df, name in zip(
    date_dfs,
    [
        'Alcohol_per_date', 'band_power', 'circadian_delta',
        'coffee_date', 'emotion_diary', 'exercise_date',
        'step_delta', 'HR_date', 'panic', 'questionnaire_bydate',
        'sleep', 'smoking_diet_mens', 'diary'
    ]
):
    # Merge each dataframe's dates with min_date by ID
    merged_dates = pd.merge(
        df[['ID', 'date']],
        min_date,
        on='ID',
        how='left'
    )
    # Find records where date < min_date
    earlier = merged_dates[merged_dates['date'] < merged_dates['min_date']]
    if not earlier.empty:
        print(f"âš ï¸ DataFrame {name} has {len(earlier)} records with date earlier than observed min_date")
        print(earlier.to_string(index=False))


# (5) ëª¨ë“  (ID, date) ì¡°í•©ì„ ë§ˆìŠ¤í„° í‚¤ë¡œ ìƒì„±
all_dates = pd.concat([df[['ID', 'date']] for df in date_dfs if 'date' in df.columns]).dropna()
all_dates['date'] = pd.to_datetime(all_dates['date'])

# (5.2) IDë³„ ì‹¤ì œ ë°ì´í„°ì—ì„œ ê°€ì¥ ëŠ¦ì€ dateë¥¼ êµ¬í•´ end_date ìƒì„±
end_date = (
    all_dates
    .groupby('ID')['date']
    .max()
    .reset_index()
    .rename(columns={'date':'end_date'})
)

# (5.3) min_dateì™€ end_date ë³‘í•©
id_date_range = pd.merge(min_date, end_date, on='ID', how='inner')

# 5.3) ê° IDë³„ë¡œ date range ìƒì„±
expanded_rows = []
for _, row in id_date_range.iterrows():
    id_ = row['ID']
    start = row['min_date']
    end = row['end_date']
    date_range = pd.date_range(start, end, freq='D')
    expanded_rows.extend([{'ID': id_, 'date': d} for d in date_range])

all_keys = pd.DataFrame(expanded_rows)

# (5.5) diaryë¡œ ì¸í•´ ì¶”ê°€ëœ (ID, date) ì¡°í•© í™•ì¸
date_dfs_wo_diary = [df for df in date_dfs if not df.equals(diary)]
all_keys_wo_diary = pd.concat([df[['ID', 'date']] for df in date_dfs_wo_diary]).drop_duplicates().dropna()

new_keys_from_diary = pd.merge(
    all_keys,
    all_keys_wo_diary,
    how='outer',
    indicator=True
).query("_merge == 'left_only'")[['ID', 'date']]

print("ğŸ“Œ diaryë¡œ ì¸í•´ ì¶”ê°€ëœ ìƒˆë¡œìš´ (ID, date) ì¡°í•© ìˆ˜:", len(new_keys_from_diary))
print(new_keys_from_diary.head())

# (6) all_keysì— demographic ì •ë³´ ë¶™ì—¬ master_key ìƒì„±
master_key = pd.merge(
    all_keys,
    demographic_data,
    how='left',  # all_keysì˜ ID-date ì¡°í•©ë§Œ ë³µì œ
    on='ID'
)

# === Debug: demographic_data age í™•ì¸ ===
if 'age' in demographic_data.columns:
    print("ğŸ›  demographic_data: total rows:", len(demographic_data))
    print("ğŸ›  demographic_data: age NaN count:", demographic_data['age'].isna().sum())
    print("ğŸ›  demographic_data: sample IDs with missing age:", demographic_data.loc[demographic_data['age'].isna(), 'ID'].unique()[:10])
else:
    print("ğŸ›  demographic_dataì— 'age' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
# all_keys ê¸°ì¤€ìœ¼ë¡œ merge ì „í›„ age ëˆ„ë½ í™•ì¸
merged_demo = pd.merge(all_keys[['ID']].drop_duplicates(), demographic_data[['ID','age']], how='left', on='ID')
print("ğŸ›  all_keysì™€ demographic_data merge í›„: total rows:", len(merged_demo))
print("ğŸ›  all_keys merge í›„: age NaN count:", merged_demo['age'].isna().sum())
print("ğŸ›  all_keys merge í›„: sample IDs with missing age:", merged_demo.loc[merged_demo['age'].isna(), 'ID'].unique()[:10])

# (7) date ì •ë³´ê°€ ì—†ëŠ” ID ì²˜ë¦¬: demographic-only IDsë¥¼ í•œ í–‰ìœ¼ë¡œ ì¶”ê°€
ids_with_dates = all_keys['ID'].unique()
demog_only = demographic_data[~demographic_data['ID'].isin(ids_with_dates)].copy()
demog_only['date'] = pd.NaT
# demographic-only í–‰ ì¶”ê°€
master_key = pd.concat([master_key, demog_only], ignore_index=True)

# (8) master_key ìœ„ì— ë‚ ì§œ ê¸°ë°˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ left join
merged_full = master_key
for df in date_dfs:
    merged_full = pd.merge(
        merged_full,
        df,
        how='left',
        on=['ID', 'date']
    )

# === Debug: merged_full age ìƒíƒœ í™•ì¸ ===
if 'age' in merged_full.columns:
    print("ğŸ›  merged_full: total rows:", len(merged_full))
    print("ğŸ›  merged_full: age NaN count:", merged_full['age'].isna().sum())
    print("ğŸ›  merged_full: sample IDs with missing age:", merged_full.loc[merged_full['age'].isna(), 'ID'].unique()[:10])
    # ë‚ ì§œë³„ë¡œ age ê°’ì´ ì¼ê´€ë˜ê²Œ ë“¤ì–´ê°€ëŠ”ì§€ ëª‡ ê°œ ì˜ˆì‹œ ë³´ê¸°
    sample_ids = merged_full['ID'].unique()[:5]
    for sid in sample_ids:
        sub = merged_full[merged_full['ID']==sid]
        print(f"ğŸ›  ID={sid}: unique age values:", sub['age'].dropna().unique()[:5], "NaN rows count:", sub['age'].isna().sum())
else:
    print("ğŸ›  merged_fullì— 'age' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# (9) panic ê°’ì´ ì˜ ì‚´ì•„ ìˆëŠ”ì§€ í™•ì¸
print("ğŸ“Œ panic value counts (before any drop/fill):")
print(merged_full['panic'].value_counts(dropna=False))



# panic ì»¬ëŸ¼ì€ ffill ëŒ€ìƒì—ì„œ ì œì™¸
merged_full = merged_full.sort_values(['ID','date']).reset_index(drop=True)

# (11) ì»¬ëŸ¼ ì •ë¦¬ ë° ê²°ì¸¡ ì²˜ë¦¬
merged_full.rename(columns={
    'amp': 'HR_amplitude',           'mesor': 'HR_mesor',         'acr': 'HR_acrophase',
    'amp_delta': 'HR_amplitude_difference',  'mesor_delta': 'HR_mesor_difference',   'acr_delta': 'HR_acrophase_difference',
    'amp_delta2': 'HR_amplitude_difference_2d',  'mesor_delta2': 'HR_mesor_difference_2d',  'acr_delta2': 'HR_acrophase_difference_2d',
    'positive': 'positive_feeling', 
    'negative': 'negative_feeling',
    'step_max': 'steps_maximum',    'step_var': 'steps_variance',
    'step_mean': 'steps_mean', 
    'bandpower_a': 'bandpower(0.001-0.0005Hz)', 
    'bandpower_b': 'bandpower(0.0005-0.0001Hz)',
    'bandpower_c': 'bandpower(0.0001-0.00005Hz)', 
    'bandpower_d': 'bandpower(0.00005-0.00001Hz)',
    'suicide_need_in_month': 'suicide_need'
}, inplace=True)

merged_full.drop(['ht','wt','late_night_snack'], axis=1, inplace=True, errors='ignore')

# (12) date ì»¬ëŸ¼ì´ datetime íƒ€ì…ì¸ ê²½ìš°, ë¬¸ìì—´ YYYY-MM-DDë¡œ ë³€í™˜
if 'date' in merged_full.columns:
    merged_full['date'] = merged_full['date'].dt.strftime('%Y-%m-%d')

# (13) ì»¬ëŸ¼ ìˆœì„œ: ID, date, panic â†’ ë‚˜ë¨¸ì§€
cols = merged_full.columns.tolist()
ordered_cols = ['ID', 'date', 'panic'] + [c for c in cols if c not in ['ID', 'date', 'panic']]
merged_full = merged_full[ordered_cols]

# (13.5) panic == 2ì¸ ë‚ ì˜ ì´ì „ ë‚ ì— panic == 1 ì±„ìš°ê¸°
merged_full = merged_full.sort_values(['ID', 'date']).reset_index(drop=True)
for i in range(1, len(merged_full)):
    if merged_full.loc[i, 'panic'] == 2:
        j = i - 1
        while j >= 0 and merged_full.loc[j, 'ID'] == merged_full.loc[i, 'ID']:
            if pd.isna(merged_full.loc[j, 'panic']):
                merged_full.loc[j, 'panic'] = 1
                break
            elif merged_full.loc[j, 'panic'] == 2:
                j -= 1
            else:
                break

# (14) panicì˜ ë‚¨ì€ NaNì€ 0ìœ¼ë¡œ ì²˜ë¦¬
merged_full['panic'] = merged_full['panic'].fillna(0)

print("ğŸ“Œ panic value counts after concat:")
print(merged_full['panic'].value_counts(dropna=False))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (16) panic ë¼ë²¨ ê°ì†Œ ì›ì¸ë³„ ê°œìˆ˜ í™•ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1) IDÂ·date ìˆœìœ¼ë¡œ ì •ë ¬ëœ DataFrameì´ í•„ìš”í•˜ë¯€ë¡œ ë‹¤ì‹œ ì •ë ¬
merged_full = merged_full.sort_values(['ID', 'date']).reset_index(drop=True)

# 2) ì „ì²´ panic==2, panic==1 ê°œìˆ˜ êµ¬í•˜ê¸°
total_2 = (merged_full['panic'] == 2).sum()
total_1 = (merged_full['panic'] == 1).sum()

# 3) ì¹´ìš´í„° ì´ˆê¸°í™”
skipped_consecutive_2 = 0    # ì—°ì†ëœ 2ë¡œ ì¸í•´ 1ë¡œ ì±„ìš°ê¸° ëŒ€ìƒì—ì„œ ì œì™¸ëœ ê²½ìš°
skipped_first_date = 0       # í•´ë‹¹ IDì—ì„œ ì²«ë‚ ì´ 2ë¼ì„œ ì•ì— ì“¸ ìˆ˜ ì—†ì–´ì„œ ì œì™¸ëœ ê²½ìš°
skipped_non_nan_prev = 0     # ì „ë‚  ê°’ì´ ì´ë¯¸ NaNì´ ì•„ë‹Œ(0 í˜¹ì€ 1)ì´ì–´ì„œ 1ë¡œ ë®ì–´ì“°ì§€ ì•Šì€ ê²½ìš°

# 4) IDë³„ë¡œ ìˆœíšŒí•˜ë©° ìŠ¤í‚µ ì¼€ì´ìŠ¤ ê³„ì‚°
for id_, group in merged_full.groupby('ID'):
    group = group.reset_index(drop=True)
    for i in range(len(group)):
        if group.loc[i, 'panic'] == 2:
            if i == 0:
                # (2) IDë³„ ì²«ë‚ ì´ panic==2ì¸ ê²½ìš°
                skipped_first_date += 1
            else:
                prev_val = group.loc[i-1, 'panic']
                if prev_val == 2:
                    # (1) ì—°ì†ëœ 2ì¸ ê²½ìš°: ì²« ë²ˆì§¸ 2ì˜ ì „ë‚ ë§Œ 1ë¡œ ì±„ìš°ê³  ì´í›„ ì—°ì†ëœ 2ë“¤ì€ ì „ë‚ ì´ 2ë¼ì„œ skip
                    skipped_consecutive_2 += 1
                elif pd.notna(prev_val):
                    # (3) ì „ë‚  ê°’ì´ NaNì´ ì•„ë‹Œ(ì¦‰ ì´ë¯¸ 0 í˜¹ì€ 1ë¡œ ì±„ì›Œì§) ê²½ìš° â†’ ë®ì–´ì“°ì§€ ì•Šê³  skip
                    skipped_non_nan_prev += 1
                # ë§Œì•½ prev_valì´ NaNì´ë©´ ì •ìƒì ìœ¼ë¡œ 1ë¡œ ë¼ë²¨ë§ëìœ¼ë¯€ë¡œ ì œì™¸

# 5) ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ë‚˜ë¨¸ì§€ ì°¨ì´ ê³„ì‚°
explained = skipped_consecutive_2 + skipped_first_date + skipped_non_nan_prev
unexplained = total_2 - explained

# 6) ê²°ê³¼ ì¶œë ¥
print("\nâ”€â”€â”€â”€â”€â”€â”€â”€ Panic Label Analysis â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"Total panic==2 count:                       {total_2}")
print(f"Total panic==1 count:                       {total_1}")
print(f"Skipped due to consecutive 2:               {skipped_consecutive_2}")
print(f"Skipped due to first date being 2:          {skipped_first_date}")
print(f"Skipped due to previous not NaN (already labeled): {skipped_non_nan_prev}")
print(f"Unexplained difference:                     {unexplained}")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")


## (17) SYM1-1-343, 2021-05-22 ì¤‘ë³µ ì²˜ë¦¬: ì²« ë²ˆì§¸ í–‰ ë“œë¡­ (ë§ˆì§€ë§‰ í–‰ ìœ ì§€)
mask = (merged_full['ID'] == 'SYM1-1-343') & (merged_full['date'] == '2021-05-22')
dup_idxs = merged_full[mask].index
if len(dup_idxs) > 1:
    merged_full = merged_full.drop(dup_idxs[0])
merged_full = merged_full[~merged_full['ID'].isin(['SYM2-1-412', 'SYM2-1-425'])]
merged_full = merged_full.drop_duplicates(subset=['ID','date'])
print(len((merged_full['ID'].unique())))

output_path = "/Users/lee-junyeol/Documents/GitHub/Panic-Project-DHLAB/data/SYM_720.csv"
merged_full.to_csv(output_path, index=False)