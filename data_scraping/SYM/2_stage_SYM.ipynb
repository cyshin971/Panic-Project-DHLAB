{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c636e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from library.path_utils import get_file_path, to_absolute_path\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bfd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_SYM_DIR = \"./raw_data/SYM\"\n",
    "# 엑셀 파일 경로 (실제 경로로 수정)\n",
    "output_folder_name = \"./_tmp/SYM\"\n",
    "SYM1_file_name = \"backup_SYM1.xlsx\"\n",
    "SYM2_file_name = \"backup_SYM2.xlsx\"\n",
    "\n",
    "SYM_raw_paths = [\n",
    "    get_file_path(RAW_SYM_DIR, f\"{SYM1_file_name}\"),\n",
    "    get_file_path(RAW_SYM_DIR, f\"{SYM2_file_name}\"),\n",
    "]\n",
    "SYM_raw_paths = [Path(p) for p in SYM_raw_paths]\n",
    "SYM_raw_paths = [str(p) for p in SYM_raw_paths]\n",
    "output_folder = to_absolute_path(output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "input_path = os.path.join(output_folder, \"foot.csv\")\n",
    "step= pd.read_csv(input_path)\n",
    "step.rename(columns={'foot': 'step'}, inplace=True)\n",
    "\n",
    "#data preprocessing\n",
    "step['step'] = pd.to_numeric(step['step'])\n",
    "step_nonzero = step[step.step != 0].copy()\n",
    "\n",
    "#statistical analysis\n",
    "step_mean = step_nonzero.groupby(['ID','date'])['step'].mean().reset_index().rename(columns={'step':'step_mean'})\n",
    "step_var = step_nonzero.groupby(['ID','date'])['step'].var().reset_index().rename(columns={'step':'step_var'})\n",
    "step_max = step_nonzero.groupby(['ID','date'])['step'].max().reset_index().rename(columns={'step':'step_max'})\n",
    "\n",
    "#calculation of step_hvar_mean\n",
    "step_nonzero['hour'] = pd.to_datetime(step_nonzero['time']).dt.hour \n",
    "step_hvar = step_nonzero.groupby(['ID','date','hour'])['step'].var().reset_index()\n",
    "\n",
    "step_hvar_mean = step_hvar.groupby(['ID','date'])['step'].mean().reset_index().rename(columns={'step':'step_hvar_mean'})\n",
    "\n",
    "# create total daily steps\n",
    "daily_steps = step.groupby(['ID','date'])['step'].sum().reset_index().rename(columns={'step':'steps'})\n",
    "\n",
    "#data merge\n",
    "step_statistics_merged= pd.merge(left=step, right=step_var, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_max, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_mean, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_hvar_mean, how=\"outer\", on =['date','ID'])\n",
    "\n",
    "#data preprocessing\n",
    "step_statistics_merged['datetime'] = step_statistics_merged['date'] + ' ' + step_statistics_merged['time']\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_stactistics.csv\")\n",
    "step_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "step_date= pd.merge(left=daily_steps, right=step_var, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_max, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_mean, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_hvar_mean, how=\"left\", on =['date','ID'])\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_date.csv\")\n",
    "step_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0245cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(output_folder, \"step_date.csv\")\n",
    "step = pd.read_csv(input_path)\n",
    "step['date'] = pd.to_datetime(step['date'])\n",
    "id_list = step['ID'].unique()\n",
    "\n",
    "step_delta = pd.DataFrame(columns=['ID', 'date', 'steps', 'step_max', 'step_mean', 'step_hvar_mean', 'step_delta', 'step_max_delta',\n",
    "                                   'step_mean_delta', 'step_hvar_mean_delta', 'step_delta2', 'step_max_delta2',\n",
    "                                   'step_mean_delta2', 'step_hvar_mean_delta2'])\n",
    "for id in id_list:\n",
    "    step_id = step.loc[(step.ID == id)]\n",
    "    time_per_day = pd.date_range(step_id.date.min(), step_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    step_id = pd.merge(step_id, temp, how='right', on='date')\n",
    "    step_id.ID = id\n",
    "    step_id['step_delta'] = step_id['steps'].diff()\n",
    "    step_id['step_delta2'] = step_id['steps'].diff(periods=2)\n",
    "    step_id['step_max_delta'] = step_id['step_max'].diff()\n",
    "    step_id['step_max_delta2'] = step_id['step_max'].diff(periods=2)\n",
    "    step_id['step_mean_delta'] = step_id['step_mean'].diff()\n",
    "    step_id['step_mean_delta2'] = step_id['step_mean'].diff(periods=2)\n",
    "    step_id['step_hvar_mean_delta'] = step_id['step_hvar_mean'].diff()\n",
    "    step_id['step_hvar_mean_delta2'] = step_id['step_hvar_mean'].diff(periods=2)\n",
    "    step_delta = pd.concat([step_delta, step_id], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "step_delta['date'] = step_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "step_delta.reset_index(drop=True, inplace=True)\n",
    "# Drop rows where steps, step_delta, and step_delta2 are all zero\n",
    "step_delta = step_delta[~((step_delta['steps'] == 0) & (step_delta['step_delta'] == 0) & (step_delta['step_delta2'] == 0))]\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_delta.csv\")\n",
    "step_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c54e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "input_path = os.path.join(output_folder, \"HR.csv\")\n",
    "HR= pd.read_csv(input_path)\n",
    "\n",
    "#data preprocessing\n",
    "HR['HR'] = pd.to_numeric(HR['HR'])\n",
    "HR_nonzero = HR[HR.HR != 0]\n",
    "\n",
    "#statistical analysis\n",
    "HR_mean = HR_nonzero.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "HR_var = HR_nonzero.groupby(['ID','date'])['HR'].var().reset_index()\n",
    "# 최소 2개 이상의 데이터가 있는 날짜만 분산 계산\n",
    "HR_count = HR_nonzero.groupby(['ID','date']).size().reset_index(name='count')\n",
    "HR_var = HR_var.merge(HR_count, on=['ID','date'])\n",
    "HR_var = HR_var[HR_var['count'] >= 2].drop('count', axis=1)\n",
    "HR_min = HR_nonzero.groupby(['ID','date'])['HR'].min().reset_index()\n",
    "HR_max = HR_nonzero.groupby(['ID','date'])['HR'].max().reset_index()\n",
    "\n",
    "#calculation of HR_hvar_mean\n",
    "HR['hour'] = pd.to_datetime(HR['time']).dt.hour \n",
    "HR_hvar = HR.groupby(['ID','date','hour'])['HR'].var().reset_index()\n",
    "HR_hvar_mean = HR_hvar.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "# hvar_mean도 동일한 조건 적용: 최소 2개 이상의 nonzero HR이 있는 날짜만\n",
    "HR_hvar_mean = HR_hvar_mean.merge(HR_count, on=['ID','date'], how='left')\n",
    "HR_hvar_mean = HR_hvar_mean[HR_hvar_mean['count'] >= 2].drop('count', axis=1)\n",
    "\n",
    "#data merge\n",
    "HR_statistics_merged = pd.merge(left=HR, right=HR_var, how=\"outer\", on =['date','ID'], suffixes=['', '_var'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_min, how=\"outer\", on =['date','ID'], suffixes=['', '_min'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_max, how=\"outer\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_hvar_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "\n",
    "#data preprocessing\n",
    "HR_statistics_merged['datetime'] = HR_statistics_merged['date'] + ' ' + HR_statistics_merged['time']\n",
    "HR_statistics_merged.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"hr_stactistics_fixed.csv\")\n",
    "HR_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "HR_date = pd.merge(left=HR_var, right=HR_max, how=\"left\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_mean, how=\"left\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_hvar_mean, how=\"left\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "HR_date.rename(columns = {'HR':'HR_var'}, inplace=True)\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"HR_date_fixed.csv\")\n",
    "HR_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(output_folder, \"HR.csv\")\n",
    "HR = pd.read_csv(input_path)\n",
    "HR['HR'] = pd.to_numeric(HR['HR'])\n",
    "id_list = HR['ID'].unique()\n",
    "    \n",
    "HR_interpolated = pd.DataFrame(columns=['index', 'ID', 'date', 'time', 'HR'])\n",
    "for id in tqdm(id_list):\n",
    "    temp_id = HR.loc[(HR['ID'] == id)].copy()\n",
    "    temp_id.reset_index(inplace=True)\n",
    "    temp_id.drop('index', axis=1, inplace=True)\n",
    "    date_list =temp_id['date'].unique()\n",
    "    for date in date_list:\n",
    "        temp_date = temp_id.loc[(temp_id['date'] == date)].copy()\n",
    "        temp_date.reset_index(inplace=True)\n",
    "        temp_date.drop('index', axis=1, inplace=True)\n",
    "        temp_date.reset_index(inplace=True)  \n",
    "        temp_date = temp_date.replace(0, np.nan)\n",
    "        if temp_date.HR.count() > 720:\n",
    "            temp_date = temp_date.interpolate(method='values', limit_direction = 'both')\n",
    "            HR_interpolated = pd.concat([HR_interpolated, temp_date], axis=0)\n",
    "            file_name =  id + ' ' + date\n",
    "            # plot_df(temp_date['index'], temp_date['HR'], file_name)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "HR_interpolated.reset_index(drop=True, inplace=True)\n",
    "output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "HR_interpolated.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_analysis import mesor, amplitude, acrophase\n",
    "input_path = os.path.join(output_folder, \"HR.csv\")\n",
    "HR_interpolated = pd.read_csv(input_path)\n",
    "HR_interpolated['HR'] = pd.to_numeric(HR_interpolated['HR'])\n",
    "\n",
    "id_list = HR_interpolated['ID'].unique()\n",
    "circadian_data = pd.DataFrame(columns=['ID','date','acr','amp','mesor'])\n",
    "                  \n",
    "for id in id_list:\n",
    "    temp_id = HR_interpolated.loc[(HR_interpolated['ID'] == id)]\n",
    "    temp_id.reset_index(inplace=True)\n",
    "    temp_id = temp_id.drop('index', axis=1)\n",
    "    date_list =temp_id['date'].unique()\n",
    "    for date in date_list:\n",
    "        temp_date = temp_id.loc[(temp_id['date'] == date)]\n",
    "        temp_date.reset_index(inplace=True)\n",
    "        temp_date = temp_date.drop('index', axis=1)\n",
    "        temp_date.reset_index(inplace=True)  \n",
    "        if temp_date.HR.count() > 720:\n",
    "            acr = acrophase(temp_date['index'], temp_date['HR'])\n",
    "            amp = amplitude(temp_date['index'], temp_date['HR'])\n",
    "            mes = mesor(temp_date['index'], temp_date['HR'])\n",
    "            new_row = pd.DataFrame([[id, date, acr, amp, mes]], columns=['ID','date','acr','amp','mesor'])\n",
    "            circadian_data = pd.concat([circadian_data, new_row], ignore_index=True)\n",
    "            print(id, date, acr, amp, mes)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian = pd.read_csv(input_path)\n",
    "circadian['date'] = pd.to_datetime(circadian['date'])\n",
    "id_list = circadian['ID'].unique()\n",
    "\n",
    "circadian_delta = pd.DataFrame(columns=['ID', 'date', 'acr', 'amp', 'mesor','acr_delta', 'acr_delta2', 'amp_delta', 'amp_delta2', 'mesor_delta', 'mesor_delta2'])\n",
    "for id in id_list:\n",
    "    circadian_id = circadian.loc[(circadian.ID == id)]\n",
    "    time_per_day = pd.date_range(circadian_id.date.min(), circadian_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    circadian_id = pd.merge(circadian_id, temp, how='right', on='date')\n",
    "    circadian_id.ID = id\n",
    "    circadian_id['acr_delta'] = circadian_id['acr'].diff()\n",
    "    circadian_id['acr_delta2'] = circadian_id['acr'].diff(periods=2)\n",
    "    circadian_id['amp_delta'] = circadian_id['amp'].diff()\n",
    "    circadian_id['amp_delta2'] = circadian_id['amp'].diff(periods=2)\n",
    "    circadian_id['mesor_delta'] = circadian_id['mesor'].diff()\n",
    "    circadian_id['mesor_delta2'] = circadian_id['mesor'].diff(periods=2)\n",
    "    circadian_delta = pd.concat([circadian_delta, circadian_id], axis=0)\n",
    "\n",
    "circadian_delta['date'] = circadian_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "circadian_delta.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_delta_720.csv\")\n",
    "circadian_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_analysis import (\n",
    "    check_bandpower_value_a,\n",
    "    check_bandpower_value_b,\n",
    "    check_bandpower_value_c,\n",
    "    check_bandpower_value_d,\n",
    ")\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "input_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "# 1) 파일 로드 & 타입 변환\n",
    "HR = pd.read_csv(\n",
    "    input_path,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "HR[\"HR\"] = pd.to_numeric(HR[\"HR\"], errors=\"coerce\")\n",
    "\n",
    "# ——— build per-minute DataFrame ———\n",
    "df_per_min = pd.DataFrame(columns=['ID','HR','date'])\n",
    "for id in HR['ID'].unique():\n",
    "    df_id = HR[HR['ID'] == id]\n",
    "    time_per_min = pd.date_range(df_id['date'].min(), df_id['date'].max(), freq='min')\n",
    "    temp = pd.DataFrame({'date': time_per_min})\n",
    "    df_id = pd.merge(df_id, temp, how='right', on='date')\n",
    "    df_id['ID'] = id\n",
    "    df_per_min = pd.concat([df_per_min, df_id], axis=0)\n",
    "\n",
    "df_per_min[\"day\"] = df_per_min[\"date\"].dt.date\n",
    "\n",
    "# 4) 하루 그룹 하나당 밴드파워 계산 함수\n",
    "def compute_bandpower_for_group(group):\n",
    "    (id_, day), sub = group\n",
    "    valid_count = sub['HR'].notna().sum()\n",
    "    if valid_count <= 720:\n",
    "        return None\n",
    "    idx = np.arange(len(sub))\n",
    "    hr  = sub[\"HR\"].to_numpy()\n",
    "    return {\n",
    "        \"ID\":           id_,\n",
    "        \"date\":         pd.Timestamp(day),\n",
    "        \"bandpower_a\":  check_bandpower_value_a(idx, hr),\n",
    "        \"bandpower_b\":  check_bandpower_value_b(idx, hr),\n",
    "        \"bandpower_c\":  check_bandpower_value_c(idx, hr),\n",
    "        \"bandpower_d\":  check_bandpower_value_d(idx, hr),\n",
    "    }\n",
    "\n",
    "groups = df_per_min.groupby([\"ID\",\"day\"], sort=False)\n",
    "\n",
    "# 5) tqdm_joblib 로 진행률 표시하며 병렬 처리\n",
    "with tqdm_joblib(tqdm(total=df_per_min[\"ID\"].nunique(), desc=\"IDs\")):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda g: compute_bandpower_for_group(g))(grp)\n",
    "        for grp in groups\n",
    "    )\n",
    "\n",
    "# 6) None 삭제 & DataFrame 생성\n",
    "records = [r for r in results if r is not None]\n",
    "bandpower_df = pd.DataFrame(records)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"bandpower_fixed_720.csv\")\n",
    "bandpower_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panic_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
