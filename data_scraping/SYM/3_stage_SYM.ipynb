{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37589401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from library.path_utils import get_file_path, to_absolute_path\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import reduce\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_SYM_DIR = \"./raw_data/SYM\"\n",
    "# ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •)\n",
    "\n",
    "SYM1_file_name = \"backup_SYM1.xlsx\"\n",
    "SYM2_file_name = \"backup_SYM2.xlsx\"\n",
    "\n",
    "SYM_raw_paths = [\n",
    "    get_file_path(RAW_SYM_DIR, f\"{SYM1_file_name}\"),\n",
    "    get_file_path(RAW_SYM_DIR, f\"{SYM2_file_name}\"),\n",
    "]\n",
    "SYM_raw_paths = [Path(p) for p in SYM_raw_paths]\n",
    "SYM_raw_paths = [str(p) for p in SYM_raw_paths]\n",
    "\n",
    "output_folder_name = \"./_tmp/SYM\"\n",
    "result_folder_name = \"./data\"\n",
    "\n",
    "\n",
    "output_folder = to_absolute_path(output_folder_name)\n",
    "result_folder = to_absolute_path(result_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) CSV íŒŒì¼ ë¡œë“œ & 'Unnamed' ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì œê±° í•¨ìˆ˜\n",
    "def load_and_clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "    return df\n",
    "\n",
    "# (2) ëª¨ë“  CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "output_path = os.path.join(output_folder, \"start_date.csv\")\n",
    "start_date = load_and_clean(output_path)\n",
    "start_date['start_date'] = pd.to_datetime(start_date['start_date'], errors='coerce')\n",
    "output_path = os.path.join(output_folder, \"alcohol_per_date.csv\")\n",
    "Alcohol_per_date      = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"bandpower_fixed_720.csv\")\n",
    "band_power            = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"circadian_delta_720.csv\")\n",
    "circadian_delta       = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"coffee_per_date.csv\")\n",
    "coffee_date           = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"emotion_diary.csv\")\n",
    "emotion_diary         = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"exercise_per_date.csv\")\n",
    "exercise_date         = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"step_delta.csv\")\n",
    "step_delta = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"HR_date_fixed.csv\")\n",
    "HR_date               = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"panic_by_date.csv\")\n",
    "panic                 = load_and_clean(output_path).drop(columns=['time','datetime'], errors='ignore')\n",
    "output_path = os.path.join(output_folder, \"questionnaire.csv\")\n",
    "questionnaire_bydate  = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"sleep_summary.csv\")\n",
    "sleep                 = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"smoking_diet_mens.csv\")\n",
    "smoking_diet_mens     = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"demographic_data.csv\")\n",
    "demographic_data      = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"diary.csv\")\n",
    "diary = load_and_clean(output_path)\n",
    "# (3) ë‚ ì§œ ê¸°ë°˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "date_dfs = [\n",
    "    Alcohol_per_date,\n",
    "    band_power,\n",
    "    circadian_delta,\n",
    "    coffee_date,\n",
    "    emotion_diary,\n",
    "    exercise_date,\n",
    "    step_delta,\n",
    "    HR_date,\n",
    "    panic,\n",
    "    questionnaire_bydate,\n",
    "    sleep,\n",
    "    smoking_diet_mens,\n",
    "    diary\n",
    "]\n",
    "\n",
    "\n",
    "# (3.5) ëª¨ë“  date ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
    "for df in date_dfs:\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# (3.6) Check for records with a date earlier than the observed min_date for each ID\n",
    "# Using min_date computed below in section (5.1)\n",
    "# First, ensure min_date is computed before this block (move section (5.1) above section (3.6)), or compute here:\n",
    "min_date = (\n",
    "    pd.concat([df[['ID','date']] for df in date_dfs if 'date' in df.columns])\n",
    "    .dropna()\n",
    "    .assign(date=lambda x: pd.to_datetime(x['date'], errors='coerce'))\n",
    "    .groupby('ID')['date']\n",
    "    .min()\n",
    "    .reset_index()\n",
    "    .rename(columns={'date':'min_date'})\n",
    ")\n",
    "for df, name in zip(\n",
    "    date_dfs,\n",
    "    [\n",
    "        'Alcohol_per_date', 'band_power', 'circadian_delta',\n",
    "        'coffee_date', 'emotion_diary', 'exercise_date',\n",
    "        'step_delta', 'HR_date', 'panic', 'questionnaire_bydate',\n",
    "        'sleep', 'smoking_diet_mens', 'diary'\n",
    "    ]\n",
    "):\n",
    "    # Merge each dataframe's dates with min_date by ID\n",
    "    merged_dates = pd.merge(\n",
    "        df[['ID', 'date']],\n",
    "        min_date,\n",
    "        on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "    # Find records where date < min_date\n",
    "    earlier = merged_dates[merged_dates['date'] < merged_dates['min_date']]\n",
    "    if not earlier.empty:\n",
    "        print(f\"âš ï¸ DataFrame {name} has {len(earlier)} records with date earlier than observed min_date\")\n",
    "        print(earlier.to_string(index=False))\n",
    "\n",
    "\n",
    "# (5) ëª¨ë“  (ID, date) ì¡°í•©ì„ ë§ˆìŠ¤í„° í‚¤ë¡œ ìƒì„±\n",
    "all_dates = pd.concat([df[['ID', 'date']] for df in date_dfs if 'date' in df.columns]).dropna()\n",
    "all_dates['date'] = pd.to_datetime(all_dates['date'])\n",
    "\n",
    "# (5.2) IDë³„ ì‹¤ì œ ë°ì´í„°ì—ì„œ ê°€ì¥ ëŠ¦ì€ dateë¥¼ êµ¬í•´ end_date ìƒì„±\n",
    "end_date = (\n",
    "    all_dates\n",
    "    .groupby('ID')['date']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'date':'end_date'})\n",
    ")\n",
    "\n",
    "# (5.3) min_dateì™€ end_date ë³‘í•©\n",
    "id_date_range = pd.merge(min_date, end_date, on='ID', how='inner')\n",
    "\n",
    "# 5.3) ê° IDë³„ë¡œ date range ìƒì„±\n",
    "expanded_rows = []\n",
    "for _, row in id_date_range.iterrows():\n",
    "    id_ = row['ID']\n",
    "    start = row['min_date']\n",
    "    end = row['end_date']\n",
    "    date_range = pd.date_range(start, end, freq='D')\n",
    "    expanded_rows.extend([{'ID': id_, 'date': d} for d in date_range])\n",
    "\n",
    "all_keys = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# (5.5) diaryë¡œ ì¸í•´ ì¶”ê°€ëœ (ID, date) ì¡°í•© í™•ì¸\n",
    "date_dfs_wo_diary = [df for df in date_dfs if not df.equals(diary)]\n",
    "all_keys_wo_diary = pd.concat([df[['ID', 'date']] for df in date_dfs_wo_diary]).drop_duplicates().dropna()\n",
    "\n",
    "new_keys_from_diary = pd.merge(\n",
    "    all_keys,\n",
    "    all_keys_wo_diary,\n",
    "    how='outer',\n",
    "    indicator=True\n",
    ").query(\"_merge == 'left_only'\")[['ID', 'date']]\n",
    "\n",
    "print(\"ğŸ“Œ diaryë¡œ ì¸í•´ ì¶”ê°€ëœ ìƒˆë¡œìš´ (ID, date) ì¡°í•© ìˆ˜:\", len(new_keys_from_diary))\n",
    "print(new_keys_from_diary.head())\n",
    "\n",
    "# (6) all_keysì— demographic ì •ë³´ ë¶™ì—¬ master_key ìƒì„±\n",
    "master_key = pd.merge(\n",
    "    all_keys,\n",
    "    demographic_data,\n",
    "    how='left',  # all_keysì˜ ID-date ì¡°í•©ë§Œ ë³µì œ\n",
    "    on='ID'\n",
    ")\n",
    "\n",
    "# === Debug: demographic_data age í™•ì¸ ===\n",
    "if 'age' in demographic_data.columns:\n",
    "    print(\"ğŸ›  demographic_data: total rows:\", len(demographic_data))\n",
    "    print(\"ğŸ›  demographic_data: age NaN count:\", demographic_data['age'].isna().sum())\n",
    "    print(\"ğŸ›  demographic_data: sample IDs with missing age:\", demographic_data.loc[demographic_data['age'].isna(), 'ID'].unique()[:10])\n",
    "else:\n",
    "    print(\"ğŸ›  demographic_dataì— 'age' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "# all_keys ê¸°ì¤€ìœ¼ë¡œ merge ì „í›„ age ëˆ„ë½ í™•ì¸\n",
    "merged_demo = pd.merge(all_keys[['ID']].drop_duplicates(), demographic_data[['ID','age']], how='left', on='ID')\n",
    "print(\"ğŸ›  all_keysì™€ demographic_data merge í›„: total rows:\", len(merged_demo))\n",
    "print(\"ğŸ›  all_keys merge í›„: age NaN count:\", merged_demo['age'].isna().sum())\n",
    "print(\"ğŸ›  all_keys merge í›„: sample IDs with missing age:\", merged_demo.loc[merged_demo['age'].isna(), 'ID'].unique()[:10])\n",
    "\n",
    "# (7) date ì •ë³´ê°€ ì—†ëŠ” ID ì²˜ë¦¬: demographic-only IDsë¥¼ í•œ í–‰ìœ¼ë¡œ ì¶”ê°€\n",
    "ids_with_dates = all_keys['ID'].unique()\n",
    "demog_only = demographic_data[~demographic_data['ID'].isin(ids_with_dates)].copy()\n",
    "demog_only['date'] = pd.NaT\n",
    "# demographic-only í–‰ ì¶”ê°€\n",
    "master_key = pd.concat([master_key, demog_only], ignore_index=True)\n",
    "\n",
    "# (8) master_key ìœ„ì— ë‚ ì§œ ê¸°ë°˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ left join\n",
    "merged_full = master_key\n",
    "for df in date_dfs:\n",
    "    merged_full = pd.merge(\n",
    "        merged_full,\n",
    "        df,\n",
    "        how='left',\n",
    "        on=['ID', 'date']\n",
    "    )\n",
    "\n",
    "# === Debug: merged_full age ìƒíƒœ í™•ì¸ ===\n",
    "if 'age' in merged_full.columns:\n",
    "    print(\"ğŸ›  merged_full: total rows:\", len(merged_full))\n",
    "    print(\"ğŸ›  merged_full: age NaN count:\", merged_full['age'].isna().sum())\n",
    "    print(\"ğŸ›  merged_full: sample IDs with missing age:\", merged_full.loc[merged_full['age'].isna(), 'ID'].unique()[:10])\n",
    "    # ë‚ ì§œë³„ë¡œ age ê°’ì´ ì¼ê´€ë˜ê²Œ ë“¤ì–´ê°€ëŠ”ì§€ ëª‡ ê°œ ì˜ˆì‹œ ë³´ê¸°\n",
    "    sample_ids = merged_full['ID'].unique()[:5]\n",
    "    for sid in sample_ids:\n",
    "        sub = merged_full[merged_full['ID']==sid]\n",
    "        print(f\"ğŸ›  ID={sid}: unique age values:\", sub['age'].dropna().unique()[:5], \"NaN rows count:\", sub['age'].isna().sum())\n",
    "else:\n",
    "    print(\"ğŸ›  merged_fullì— 'age' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# (9) panic ê°’ì´ ì˜ ì‚´ì•„ ìˆëŠ”ì§€ í™•ì¸\n",
    "print(\"ğŸ“Œ panic value counts (before any drop/fill):\")\n",
    "print(merged_full['panic'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "\n",
    "# panic ì»¬ëŸ¼ì€ ffill ëŒ€ìƒì—ì„œ ì œì™¸\n",
    "merged_full = merged_full.sort_values(['ID','date']).reset_index(drop=True)\n",
    "\n",
    "# (11) ì»¬ëŸ¼ ì •ë¦¬ ë° ê²°ì¸¡ ì²˜ë¦¬\n",
    "merged_full.rename(columns={\n",
    "    'amp': 'HR_amplitude',           'mesor': 'HR_mesor',         'acr': 'HR_acrophase',\n",
    "    'amp_delta': 'HR_amplitude_difference',  'mesor_delta': 'HR_mesor_difference',   'acr_delta': 'HR_acrophase_difference',\n",
    "    'amp_delta2': 'HR_amplitude_difference_2d',  'mesor_delta2': 'HR_mesor_difference_2d',  'acr_delta2': 'HR_acrophase_difference_2d',\n",
    "    'positive': 'positive_feeling', \n",
    "    'negative': 'negative_feeling',\n",
    "    'step_max': 'steps_maximum',    'step_var': 'steps_variance',\n",
    "    'step_mean': 'steps_mean', \n",
    "    'bandpower_a': 'bandpower(0.001-0.0005Hz)', \n",
    "    'bandpower_b': 'bandpower(0.0005-0.0001Hz)',\n",
    "    'bandpower_c': 'bandpower(0.0001-0.00005Hz)', \n",
    "    'bandpower_d': 'bandpower(0.00005-0.00001Hz)',\n",
    "    'suicide_need_in_month': 'suicide_need'\n",
    "}, inplace=True)\n",
    "\n",
    "merged_full.drop(['ht','wt','late_night_snack'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# (12) date ì»¬ëŸ¼ì´ datetime íƒ€ì…ì¸ ê²½ìš°, ë¬¸ìì—´ YYYY-MM-DDë¡œ ë³€í™˜\n",
    "if 'date' in merged_full.columns:\n",
    "    merged_full['date'] = merged_full['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# (13) ì»¬ëŸ¼ ìˆœì„œ: ID, date, panic â†’ ë‚˜ë¨¸ì§€\n",
    "cols = merged_full.columns.tolist()\n",
    "ordered_cols = ['ID', 'date', 'panic'] + [c for c in cols if c not in ['ID', 'date', 'panic']]\n",
    "merged_full = merged_full[ordered_cols]\n",
    "\n",
    "# (13.5) panic == 2ì¸ ë‚ ì˜ ì´ì „ ë‚ ì— panic == 1 ì±„ìš°ê¸°\n",
    "merged_full = merged_full.sort_values(['ID', 'date']).reset_index(drop=True)\n",
    "for i in range(1, len(merged_full)):\n",
    "    if merged_full.loc[i, 'panic'] == 2:\n",
    "        j = i - 1\n",
    "        while j >= 0 and merged_full.loc[j, 'ID'] == merged_full.loc[i, 'ID']:\n",
    "            if pd.isna(merged_full.loc[j, 'panic']):\n",
    "                merged_full.loc[j, 'panic'] = 1\n",
    "                break\n",
    "            elif merged_full.loc[j, 'panic'] == 2:\n",
    "                j -= 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "# (14) panicì˜ ë‚¨ì€ NaNì€ 0ìœ¼ë¡œ ì²˜ë¦¬\n",
    "merged_full['panic'] = merged_full['panic'].fillna(0)\n",
    "\n",
    "print(\"ğŸ“Œ panic value counts after concat:\")\n",
    "print(merged_full['panic'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# (16) panic ë¼ë²¨ ê°ì†Œ ì›ì¸ë³„ ê°œìˆ˜ í™•ì¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1) IDÂ·date ìˆœìœ¼ë¡œ ì •ë ¬ëœ DataFrameì´ í•„ìš”í•˜ë¯€ë¡œ ë‹¤ì‹œ ì •ë ¬\n",
    "merged_full = merged_full.sort_values(['ID', 'date']).reset_index(drop=True)\n",
    "\n",
    "# 2) ì „ì²´ panic==2, panic==1 ê°œìˆ˜ êµ¬í•˜ê¸°\n",
    "total_2 = (merged_full['panic'] == 2).sum()\n",
    "total_1 = (merged_full['panic'] == 1).sum()\n",
    "\n",
    "# 3) ì¹´ìš´í„° ì´ˆê¸°í™”\n",
    "skipped_consecutive_2 = 0    # ì—°ì†ëœ 2ë¡œ ì¸í•´ 1ë¡œ ì±„ìš°ê¸° ëŒ€ìƒì—ì„œ ì œì™¸ëœ ê²½ìš°\n",
    "skipped_first_date = 0       # í•´ë‹¹ IDì—ì„œ ì²«ë‚ ì´ 2ë¼ì„œ ì•ì— ì“¸ ìˆ˜ ì—†ì–´ì„œ ì œì™¸ëœ ê²½ìš°\n",
    "skipped_non_nan_prev = 0     # ì „ë‚  ê°’ì´ ì´ë¯¸ NaNì´ ì•„ë‹Œ(0 í˜¹ì€ 1)ì´ì–´ì„œ 1ë¡œ ë®ì–´ì“°ì§€ ì•Šì€ ê²½ìš°\n",
    "\n",
    "# 4) IDë³„ë¡œ ìˆœíšŒí•˜ë©° ìŠ¤í‚µ ì¼€ì´ìŠ¤ ê³„ì‚°\n",
    "for id_, group in merged_full.groupby('ID'):\n",
    "    group = group.reset_index(drop=True)\n",
    "    for i in range(len(group)):\n",
    "        if group.loc[i, 'panic'] == 2:\n",
    "            if i == 0:\n",
    "                # (2) IDë³„ ì²«ë‚ ì´ panic==2ì¸ ê²½ìš°\n",
    "                skipped_first_date += 1\n",
    "            else:\n",
    "                prev_val = group.loc[i-1, 'panic']\n",
    "                if prev_val == 2:\n",
    "                    # (1) ì—°ì†ëœ 2ì¸ ê²½ìš°: ì²« ë²ˆì§¸ 2ì˜ ì „ë‚ ë§Œ 1ë¡œ ì±„ìš°ê³  ì´í›„ ì—°ì†ëœ 2ë“¤ì€ ì „ë‚ ì´ 2ë¼ì„œ skip\n",
    "                    skipped_consecutive_2 += 1\n",
    "                elif pd.notna(prev_val):\n",
    "                    # (3) ì „ë‚  ê°’ì´ NaNì´ ì•„ë‹Œ(ì¦‰ ì´ë¯¸ 0 í˜¹ì€ 1ë¡œ ì±„ì›Œì§) ê²½ìš° â†’ ë®ì–´ì“°ì§€ ì•Šê³  skip\n",
    "                    skipped_non_nan_prev += 1\n",
    "                # ë§Œì•½ prev_valì´ NaNì´ë©´ ì •ìƒì ìœ¼ë¡œ 1ë¡œ ë¼ë²¨ë§ëìœ¼ë¯€ë¡œ ì œì™¸\n",
    "\n",
    "# 5) ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ë‚˜ë¨¸ì§€ ì°¨ì´ ê³„ì‚°\n",
    "explained = skipped_consecutive_2 + skipped_first_date + skipped_non_nan_prev\n",
    "unexplained = total_2 - explained\n",
    "\n",
    "# 6) ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nâ”€â”€â”€â”€â”€â”€â”€â”€ Panic Label Analysis â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"Total panic==2 count:                       {total_2}\")\n",
    "print(f\"Total panic==1 count:                       {total_1}\")\n",
    "print(f\"Skipped due to consecutive 2:               {skipped_consecutive_2}\")\n",
    "print(f\"Skipped due to first date being 2:          {skipped_first_date}\")\n",
    "print(f\"Skipped due to previous not NaN (already labeled): {skipped_non_nan_prev}\")\n",
    "print(f\"Unexplained difference:                     {unexplained}\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\")\n",
    "\n",
    "\n",
    "## (17) SYM1-1-343, 2021-05-22 ì¤‘ë³µ ì²˜ë¦¬: ì²« ë²ˆì§¸ í–‰ ë“œë¡­ (ë§ˆì§€ë§‰ í–‰ ìœ ì§€)\n",
    "mask = (merged_full['ID'] == 'SYM1-1-343') & (merged_full['date'] == '2021-05-22')\n",
    "dup_idxs = merged_full[mask].index\n",
    "if len(dup_idxs) > 1:\n",
    "    merged_full = merged_full.drop(dup_idxs[0])\n",
    "merged_full = merged_full[~merged_full['ID'].isin(['SYM2-1-412', 'SYM2-1-425'])]\n",
    "merged_full = merged_full.drop_duplicates(subset=['ID','date'])\n",
    "print(len((merged_full['ID'].unique())))\n",
    "\n",
    "\n",
    "result_path = os.path.join(result_folder, \"SYM_720.csv\")\n",
    "merged_full.to_csv(result_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
