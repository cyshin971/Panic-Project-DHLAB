{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 초기_파일_폴더/PXPN/ActiveData 에 ActiveData.zip 들을 넣어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = (\"/Panic-Project-DHLAB/raw_data/PXPN/pixelpanic_raw_data.zip\")\n",
    "output_folder = (\"/Panic-Project-DHLAB/tmp/PXPN\")\n",
    "enroll_path = (\"/Panic-Project-DHLAB/raw_data/PXPN/1. 픽셀패닉 enroll 정보_250516.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n",
      "/var/folders/sd/3jx_llfd0y9g7jndhzx5j_ph0000gn/T/ipykernel_43914/945697273.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result.loc[\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# 설문 리스트\n",
    "top_5 = [\n",
    "    '특성 불안 설문', '한국형 회복탄력성 지수', '한국어판 아침형-저녁형 설문지',\n",
    "    '한글판 생물학적 리듬 평가 설문지', '유년기 외상 척도', '한국형 기분장애 설문지',\n",
    "    '광장공포 인지 설문지', '알바니 공황-공포 질문지', '신체감각 설문지',\n",
    "    '한글판 범불안 장애', '한국어판 우울증 선별도구'\n",
    "]\n",
    "\n",
    "# 결과 DataFrame 초기화: patient_code, date 컬럼 확보\n",
    "result = pd.DataFrame(columns=['patient_code', '날짜'])\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    for i in range(6, 41):\n",
    "        formatted_index = f'{i:02d}'\n",
    "        patient_code = f'PXPN_100{formatted_index}'\n",
    "\n",
    "        # 내부 zip 파일 경로\n",
    "        inner_zip_name = f'ActiveData/{patient_code}_ActiveData.zip'\n",
    "        if inner_zip_name not in outer_zip.namelist():\n",
    "            continue\n",
    "\n",
    "        inner_zip_bytes = BytesIO(outer_zip.read(inner_zip_name))\n",
    "        with zipfile.ZipFile(inner_zip_bytes, 'r') as inner_zip:\n",
    "            inner_file_name = f'{patient_code}_SurveyResponse.csv'\n",
    "            if inner_file_name not in inner_zip.namelist():\n",
    "                continue\n",
    "\n",
    "            with inner_zip.open(inner_file_name) as f:\n",
    "                df = pd.read_csv(f)\n",
    "\n",
    "                # 작성일 컬럼에서 날짜만 추출\n",
    "                date_value = pd.to_datetime(df['작성일'].iloc[0]).date()\n",
    "\n",
    "                # 새로운 환자-작성일 행 추가\n",
    "                if not ((result['patient_code'] == patient_code) & (result['날짜'] == date_value)).any():\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'patient_code': [patient_code],\n",
    "                        '날짜': [date_value]\n",
    "                    })\n",
    "                    result = pd.concat([result, new_row], ignore_index=True)\n",
    "\n",
    "                # 점수 처리\n",
    "                for j in top_5:\n",
    "                    sub_df = df[df['설문명'] == j].reset_index(drop=True)\n",
    "                    if sub_df.empty:\n",
    "                        continue\n",
    "\n",
    "                    # 역채점 점수가 있으면 사용\n",
    "                    scores = []\n",
    "                    for idx, row in sub_df.iterrows():\n",
    "                        if row['역채점인 경우 역채점 점수'] != '-':\n",
    "                            scores.append(float(row['역채점인 경우 역채점 점수']))\n",
    "                        else:\n",
    "                            val = row['점수']\n",
    "                            scores.append('***' if pd.isna(val) else float(val))\n",
    "\n",
    "                    # 컬럼명 생성 및 값 삽입\n",
    "                    if j == '특성 불안 설문':\n",
    "                        prefix = 'STAI_X2'\n",
    "                    elif j == '한국형 회복탄력성 지수':\n",
    "                        prefix = 'KRQ'\n",
    "                    elif j == '한국어판 아침형-저녁형 설문지':\n",
    "                        prefix = 'CSM'\n",
    "                    elif j == '한글판 생물학적 리듬 평가 설문지':\n",
    "                        prefix = 'BRIAN'\n",
    "                    elif j == '한국형 기분장애 설문지':\n",
    "                        prefix = 'MDQ'\n",
    "                    elif j == '광장공포 인지 설문지':\n",
    "                        prefix = 'ACQ'\n",
    "                    elif j == '신체감각 설문지':\n",
    "                        prefix = 'BSQ'\n",
    "                    elif j == '한글판 범불안 장애':\n",
    "                        prefix = 'GAD'\n",
    "                    elif j == '한국어판 우울증 선별도구':\n",
    "                        prefix = 'PHQ'\n",
    "\n",
    "                    # 주제별 분리 처리 필요 설문\n",
    "                    if j in ['유년기 외상 척도', '알바니 공황-공포 질문지']:\n",
    "                        grouped = sub_df.copy()\n",
    "                        grouped['real_score'] = scores\n",
    "                        topic_order = {t: i+1 for i, t in enumerate(sorted(grouped['주제'].unique()))}\n",
    "                        for topic, order in topic_order.items():\n",
    "                            topic_df = grouped[grouped['주제'] == topic].reset_index(drop=True)\n",
    "                            for qnum, sc in enumerate(topic_df['real_score'], start=1):\n",
    "                                col_name = f\"{('CTQ' if j=='유년기 외상 척도' else 'APPQ')}-{order}-{qnum}\"\n",
    "                                result.loc[\n",
    "                                    (result['patient_code'] == patient_code) &\n",
    "                                    (result['날짜'] == date_value),\n",
    "                                    col_name\n",
    "                                ] = sc\n",
    "                    else:\n",
    "                        for idx, sc in enumerate(scores, start=1):\n",
    "                            col_name = f\"{prefix}-{idx}\"\n",
    "                            result.loc[\n",
    "                                (result['patient_code'] == patient_code) &\n",
    "                                (result['날짜'] == date_value),\n",
    "                                col_name\n",
    "                            ] = sc\n",
    "\n",
    "# 컬럼 순서 재배열: patient_code, date, 나머지\n",
    "cols = ['patient_code', '날짜'] + [c for c in result.columns if c not in ['patient_code', '날짜']]\n",
    "result = result[cols]\n",
    "\n",
    "# 불필요 컬럼 삭제\n",
    "result = result.drop(columns=['MDQ-14', 'MDQ-15', 'PHQ-10'], errors='ignore')\n",
    "\n",
    "# 저장\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire.csv\")\n",
    "result.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient_code          날짜  STAI_X2-1  STAI_X2-2  STAI_X2-3  STAI_X2-4  \\\n",
      "0    PXPN_10006  2024-11-04        2.0        2.0        1.0        1.0   \n",
      "1    PXPN_10007  2024-11-13        3.0        4.0        3.0        1.0   \n",
      "2    PXPN_10008  2024-11-04        3.0        3.0        2.0        3.0   \n",
      "3    PXPN_10009  2024-11-04        4.0        4.0        2.0        1.0   \n",
      "4    PXPN_10010  2024-11-06        3.0        4.0        3.0        4.0   \n",
      "5    PXPN_10011  2024-11-09        3.0        3.0        3.0        2.0   \n",
      "6    PXPN_10012  2024-11-11        3.0        4.0        4.0        4.0   \n",
      "7    PXPN_10013  2024-11-08        4.0        4.0        3.0        2.0   \n",
      "8    PXPN_10014  2024-11-18        4.0        3.0        3.0        3.0   \n",
      "9    PXPN_10015  2024-11-25        3.0        3.0        3.0        3.0   \n",
      "10   PXPN_10018  2024-11-20        3.0        3.0        3.0        4.0   \n",
      "11   PXPN_10019  2024-11-22        3.0        3.0        2.0        3.0   \n",
      "12   PXPN_10020  2024-12-11        4.0        4.0        4.0        4.0   \n",
      "13   PXPN_10021  2024-12-24        4.0        4.0        3.0        4.0   \n",
      "14   PXPN_10022  2024-12-11        2.0        4.0        1.0        4.0   \n",
      "15   PXPN_10023  2025-01-03        4.0        3.0        1.0        3.0   \n",
      "16   PXPN_10024  2025-01-23        2.0        3.0        1.0        3.0   \n",
      "17   PXPN_10025  2025-01-23        3.0        4.0        2.0        2.0   \n",
      "18   PXPN_10028  2025-02-26        4.0        4.0        4.0        4.0   \n",
      "19   PXPN_10029  2025-02-28        3.0        4.0        2.0        4.0   \n",
      "20   PXPN_10030  2025-02-12        4.0        4.0        1.0        3.0   \n",
      "21   PXPN_10032  2025-03-06        4.0        4.0        3.0        4.0   \n",
      "22   PXPN_10034  2025-02-14        3.0        3.0        2.0        3.0   \n",
      "23   PXPN_10036  2025-05-13        3.0        3.0        2.0        2.0   \n",
      "24   PXPN_10037  2025-03-31        4.0        3.0        3.0        2.0   \n",
      "25   PXPN_10038  2025-03-19        2.0        2.0        2.0        3.0   \n",
      "26   PXPN_10039  2025-04-10        4.0        3.0        3.0        3.0   \n",
      "27   PXPN_10040  2025-05-05        2.0        4.0        2.0        1.0   \n",
      "\n",
      "    STAI_X2-5  STAI_X2-6  STAI_X2-7  STAI_X2-8  ...  GAD-7  PHQ-1  PHQ-2  \\\n",
      "0         1.0        2.0        2.0        1.0  ...    1.0    0.0    0.0   \n",
      "1         2.0        4.0        4.0        4.0  ...    2.0    2.0    2.0   \n",
      "2         3.0        4.0        2.0        2.0  ...    1.0    0.0    0.0   \n",
      "3         2.0        3.0        4.0        4.0  ...    3.0    2.0    3.0   \n",
      "4         3.0        4.0        4.0        2.0  ...    1.0    1.0    2.0   \n",
      "5         1.0        3.0        3.0        2.0  ...    2.0    2.0    1.0   \n",
      "6         1.0        3.0        3.0        2.0  ...    0.0    NaN    NaN   \n",
      "7         1.0        4.0        3.0        4.0  ...    2.0    3.0    3.0   \n",
      "8         3.0        4.0        3.0        3.0  ...    1.0    2.0    1.0   \n",
      "9         3.0        4.0        2.0        3.0  ...    1.0    1.0    2.0   \n",
      "10        1.0        3.0        3.0        3.0  ...    1.0    0.0    1.0   \n",
      "11        2.0        3.0        2.0        2.0  ...    1.0    1.0    1.0   \n",
      "12        4.0        4.0        4.0        4.0  ...    2.0    0.0    3.0   \n",
      "13        3.0        4.0        4.0        4.0  ...    0.0    3.0    2.0   \n",
      "14        4.0        3.0        4.0        1.0  ...    0.0    0.0    1.0   \n",
      "15        2.0        3.0        3.0        3.0  ...    0.0    1.0    1.0   \n",
      "16        1.0        2.0        2.0        1.0  ...    0.0    1.0    1.0   \n",
      "17        1.0        3.0        2.0        4.0  ...    0.0    1.0    1.0   \n",
      "18        1.0        4.0        4.0        2.0  ...    1.0    3.0    3.0   \n",
      "19        3.0        4.0        3.0        3.0  ...    1.0    3.0    2.0   \n",
      "20        2.0        4.0        1.0        2.0  ...    2.0    3.0    2.0   \n",
      "21        3.0        4.0        4.0        4.0  ...    3.0    3.0    3.0   \n",
      "22        2.0        4.0        3.0        3.0  ...    0.0    1.0    2.0   \n",
      "23        1.0        3.0        3.0        2.0  ...    0.0    0.0    0.0   \n",
      "24        2.0        4.0        4.0        2.0  ...    0.0    2.0    1.0   \n",
      "25        2.0        3.0        1.0        2.0  ...    0.0    1.0    1.0   \n",
      "26        1.0        4.0        2.0        1.0  ...    2.0    1.0    1.0   \n",
      "27        1.0        4.0        3.0        1.0  ...    2.0    1.0    0.0   \n",
      "\n",
      "    PHQ-3  PHQ-4  PHQ-5  PHQ-6  PHQ-7  PHQ-8  PHQ-9  \n",
      "0     0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "1     2.0    2.0    0.0    3.0    2.0    1.0    0.0  \n",
      "2     0.0    1.0    1.0    0.0    0.0    0.0    0.0  \n",
      "3     3.0    3.0    2.0    1.0    0.0    1.0    3.0  \n",
      "4     3.0    3.0    2.0    1.0    1.0    2.0    0.0  \n",
      "5     3.0    2.0    2.0    1.0    1.0    1.0    1.0  \n",
      "6     NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
      "7     2.0    3.0    3.0    2.0    3.0    0.0    2.0  \n",
      "8     3.0    3.0    1.0    1.0    1.0    0.0    1.0  \n",
      "9     3.0    2.0    1.0    2.0    2.0    1.0    0.0  \n",
      "10    0.0    2.0    0.0    0.0    3.0    0.0    0.0  \n",
      "11    0.0    1.0    1.0    1.0    1.0    1.0    1.0  \n",
      "12    3.0    3.0    3.0    3.0    0.0    3.0    3.0  \n",
      "13    3.0    3.0    3.0    1.0    3.0    1.0    3.0  \n",
      "14    3.0    3.0    3.0    0.0    0.0    0.0    0.0  \n",
      "15    2.0    2.0    2.0    0.0    1.0    2.0    0.0  \n",
      "16    1.0    1.0    1.0    0.0    0.0    0.0    0.0  \n",
      "17    1.0    0.0    1.0    0.0    1.0    1.0    0.0  \n",
      "18    3.0    3.0    3.0    3.0    0.0    0.0    0.0  \n",
      "19    3.0    2.0    3.0    1.0    0.0    1.0    0.0  \n",
      "20    3.0    2.0    1.0    2.0    3.0    0.0    0.0  \n",
      "21    3.0    3.0    3.0    3.0    3.0    3.0    3.0  \n",
      "22    2.0    2.0    2.0    1.0    1.0    3.0    0.0  \n",
      "23    3.0    3.0    0.0    0.0    0.0    0.0    0.0  \n",
      "24    3.0    3.0    1.0    2.0    1.0    1.0    1.0  \n",
      "25    0.0    1.0    1.0    1.0    0.0    1.0    0.0  \n",
      "26    2.0    2.0    2.0    1.0    2.0    2.0    0.0  \n",
      "27    3.0    2.0    3.0    0.0    1.0    0.0    0.0  \n",
      "\n",
      "[28 rows x 224 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_code</th>\n",
       "      <th>날짜</th>\n",
       "      <th>PHQ</th>\n",
       "      <th>STAI_X2</th>\n",
       "      <th>CSM</th>\n",
       "      <th>CTQ-1</th>\n",
       "      <th>CTQ-2</th>\n",
       "      <th>CTQ-3</th>\n",
       "      <th>CTQ-4</th>\n",
       "      <th>CTQ-5</th>\n",
       "      <th>KRQ</th>\n",
       "      <th>MDQ</th>\n",
       "      <th>ACQ</th>\n",
       "      <th>APPQ-1</th>\n",
       "      <th>APPQ-2</th>\n",
       "      <th>APPQ-3</th>\n",
       "      <th>BSQ</th>\n",
       "      <th>GAD</th>\n",
       "      <th>BRIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PXPN_10006</td>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PXPN_10007</td>\n",
       "      <td>2024-11-13</td>\n",
       "      <td>14.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXPN_10008</td>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PXPN_10009</td>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>18.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PXPN_10010</td>\n",
       "      <td>2024-11-06</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PXPN_10011</td>\n",
       "      <td>2024-11-09</td>\n",
       "      <td>14.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PXPN_10012</td>\n",
       "      <td>2024-11-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PXPN_10013</td>\n",
       "      <td>2024-11-08</td>\n",
       "      <td>21.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PXPN_10014</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PXPN_10015</td>\n",
       "      <td>2024-11-25</td>\n",
       "      <td>14.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PXPN_10018</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PXPN_10019</td>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PXPN_10020</td>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PXPN_10021</td>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>22.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PXPN_10022</td>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PXPN_10023</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>11.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PXPN_10024</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PXPN_10025</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>6.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PXPN_10028</td>\n",
       "      <td>2025-02-26</td>\n",
       "      <td>18.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PXPN_10029</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PXPN_10030</td>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PXPN_10032</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PXPN_10034</td>\n",
       "      <td>2025-02-14</td>\n",
       "      <td>14.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PXPN_10036</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PXPN_10037</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>15.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PXPN_10038</td>\n",
       "      <td>2025-03-19</td>\n",
       "      <td>6.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>PXPN_10039</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PXPN_10040</td>\n",
       "      <td>2025-05-05</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_code          날짜   PHQ  STAI_X2   CSM  CTQ-1  CTQ-2  CTQ-3  CTQ-4  \\\n",
       "0    PXPN_10006  2024-11-04   0.0     32.0  31.0   11.0   13.0   17.0   28.0   \n",
       "1    PXPN_10007  2024-11-13  14.0     71.0  20.0    5.0    9.0   24.0   40.0   \n",
       "2    PXPN_10008  2024-11-04   2.0     54.0  24.0    5.0    9.0    7.0   28.0   \n",
       "3    PXPN_10009  2024-11-04  18.0     70.0  26.0    9.0   17.0   17.0   38.0   \n",
       "4    PXPN_10010  2024-11-06  15.0     67.0  20.0    5.0    6.0    5.0   15.0   \n",
       "5    PXPN_10011  2024-11-09  14.0     49.0  20.0    5.0    5.0    7.0   11.0   \n",
       "6    PXPN_10012  2024-11-11   0.0     60.0  20.0    5.0    6.0    5.0   26.0   \n",
       "7    PXPN_10013  2024-11-08  21.0     67.0  28.0    7.0   12.0   20.0   40.0   \n",
       "8    PXPN_10014  2024-11-18  13.0     71.0  14.0   15.0    5.0   16.0   28.0   \n",
       "9    PXPN_10015  2024-11-25  14.0     56.0  26.0   15.0   10.0   19.0   26.0   \n",
       "10   PXPN_10018  2024-11-20   6.0     56.0  31.0    5.0    7.0   17.0   31.0   \n",
       "11   PXPN_10019  2024-11-22   8.0     51.0  26.0    5.0    7.0    7.0   10.0   \n",
       "12   PXPN_10020  2024-12-11  21.0     77.0  36.0    5.0    5.0    5.0    8.0   \n",
       "13   PXPN_10021  2024-12-24  22.0     73.0  14.0    5.0    6.0   11.0   34.0   \n",
       "14   PXPN_10022  2024-12-11  10.0     46.0  31.0    5.0    5.0   14.0    8.0   \n",
       "15   PXPN_10023  2025-01-03  11.0     57.0  19.0    5.0    8.0    7.0   17.0   \n",
       "16   PXPN_10024  2025-01-23   5.0     35.0  36.0    5.0    5.0    5.0    9.0   \n",
       "17   PXPN_10025  2025-01-23   6.0     51.0  29.0    5.0    9.0    7.0   10.0   \n",
       "18   PXPN_10028  2025-02-26  18.0     64.0  32.0    5.0    5.0    6.0   19.0   \n",
       "19   PXPN_10029  2025-02-28  15.0     60.0  33.0    5.0    7.0    9.0   22.0   \n",
       "20   PXPN_10030  2025-02-12  16.0     60.0  17.0    5.0    6.0   12.0   23.0   \n",
       "21   PXPN_10032  2025-03-06  27.0     74.0  13.0   13.0   16.0   24.0   31.0   \n",
       "22   PXPN_10034  2025-02-14  14.0     54.0  39.0    5.0   14.0   21.0   31.0   \n",
       "23   PXPN_10036  2025-05-13   6.0     44.0  29.0   14.0   16.0   10.0   27.0   \n",
       "24   PXPN_10037  2025-03-31  15.0     58.0  16.0    6.0    5.0    6.0   16.0   \n",
       "25   PXPN_10038  2025-03-19   6.0     47.0  44.0    5.0    5.0    7.0   14.0   \n",
       "26   PXPN_10039  2025-04-10  13.0     50.0  33.0    5.0    6.0   12.0   20.0   \n",
       "27   PXPN_10040  2025-05-05  10.0     45.0  29.0    5.0    5.0   14.0   14.0   \n",
       "\n",
       "    CTQ-5    KRQ   MDQ   ACQ  APPQ-1  APPQ-2  APPQ-3   BSQ   GAD  BRIAN  \n",
       "0    12.0  219.0   1.0  21.0     0.0    10.0     2.0  25.0   2.0   25.0  \n",
       "1    20.0  131.0   4.0  33.0    24.0    12.0    16.0  29.0  18.0   71.0  \n",
       "2     5.0  165.0   4.0  35.0    29.0    28.0    31.0  49.0   7.0   46.0  \n",
       "3    23.0  117.0  11.0  60.0    56.0    19.0    64.0  43.0  16.0   64.0  \n",
       "4     7.0  163.0   4.0  43.0    32.0    32.0    58.0  57.0  13.0   64.0  \n",
       "5     5.0  167.0  13.0  44.0    38.0    20.0    54.0  50.0  12.0   62.0  \n",
       "6    14.0  153.0   0.0  37.0    26.0     7.0    23.0  42.0   9.0    0.0  \n",
       "7    20.0  110.0  12.0  61.0    42.0    58.0    65.0  65.0  16.0   58.0  \n",
       "8     9.0  123.0   2.0  33.0    31.0    13.0    30.0  46.0  18.0   70.0  \n",
       "9    14.0  159.0   2.0  27.0    34.0    38.0    49.0  64.0   7.0   51.0  \n",
       "10   13.0  163.0  12.0  42.0     8.0    11.0    15.0  49.0  10.0   45.0  \n",
       "11    5.0  187.0   2.0  25.0     0.0     6.0    14.0  38.0   6.0   44.0  \n",
       "12    5.0  152.0   4.0  43.0     4.0    34.0    32.0  83.0  12.0   59.0  \n",
       "13   14.0  115.0   9.0  47.0    40.0    34.0    46.0  81.0  10.0   70.0  \n",
       "14   16.0  220.0   7.0  27.0    14.0    30.0    11.0  56.0   9.0   60.0  \n",
       "15    5.0  161.0   7.0  28.0     6.0    10.0    50.0  43.0   5.0   60.0  \n",
       "16    5.0  214.0   5.0  25.0     8.0    26.0    20.0  42.0   5.0   45.0  \n",
       "17    6.0  218.0   7.0  31.0    32.0    36.0    33.0  48.0  10.0   51.0  \n",
       "18    8.0  215.0   3.0  14.0    12.0    12.0     2.0  46.0  14.0   47.0  \n",
       "19    9.0  170.0  10.0  35.0    42.0    32.0    51.0  63.0  10.0   44.0  \n",
       "20   11.0  161.0  12.0  50.0    14.0    26.0    51.0  70.0  11.0   57.0  \n",
       "21   23.0   99.0  13.0  63.0    71.0    39.0    71.0  74.0  21.0   78.0  \n",
       "22   20.0  173.0  10.0  19.0     8.0     5.0    21.0  50.0  13.0   43.0  \n",
       "23   18.0  170.0   8.0  25.0     6.0    13.0     8.0  36.0   1.0   36.0  \n",
       "24    5.0  171.0   5.0  26.0    25.0     8.0    13.0  63.0   4.0   69.0  \n",
       "25   11.0  212.0   4.0  50.0    43.0    16.0    25.0  58.0   5.0   38.0  \n",
       "26    9.0  163.0   0.0  31.0     6.0     4.0     5.0  49.0   8.0   38.0  \n",
       "27    6.0  224.0  10.0  45.0    52.0    60.0    12.0  79.0  17.0   33.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 사용할 접두어 목록\n",
    "prefixes = [\"PHQ\", \"STAI_X2\", \"CSM\", \"CTQ-1\", \"CTQ-2\", \"CTQ-3\", \"CTQ-4\", \"CTQ-5\", \"KRQ\", \"MDQ\", \"ACQ\", \"APPQ-1\", \"APPQ-2\", \"APPQ-3\", \"BSQ\", \"GAD\", \"BRIAN\"]\n",
    "# 결과를 저장할 데이터프레임\n",
    "aggregated_df = result[['patient_code', '날짜']].copy()\n",
    "\n",
    "for prefix in prefixes:\n",
    "    # 해당 접두어로 시작하는 컬럼 찾기\n",
    "    matched_cols = [col for col in result.columns if col.startswith(prefix)]\n",
    "    \n",
    "    # 값 합산해서 새로운 컬럼으로 추가\n",
    "    aggregated_df[f'{prefix.rstrip(\"-\")}'] = result[matched_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1)\n",
    "\n",
    "display(aggregated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    patient_code         날짜 gender\n",
      "0     PXPN_10008 2024-11-04      0\n",
      "1     PXPN_10008 2024-11-05      0\n",
      "2     PXPN_10008 2024-11-06      0\n",
      "3     PXPN_10008 2024-11-07      0\n",
      "4     PXPN_10008 2024-11-08      0\n",
      "..           ...        ...    ...\n",
      "834   PXPN_10036 2025-05-16      0\n",
      "835   PXPN_10036 2025-05-17      0\n",
      "836   PXPN_10036 2025-05-18      0\n",
      "837   PXPN_10036 2025-05-19      0\n",
      "838   PXPN_10036 2025-05-20      0\n",
      "\n",
      "[839 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "file_path = enroll_path\n",
    "\n",
    "# 엑셀 시트 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 컬럼 이름 변경\n",
    "df = df.rename(columns={'회원코드': 'patient_code', '2. 성별': 'gender'})\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "df = df[['patient_code', '연구시작일', '연구종료일', 'gender']]\n",
    "\n",
    "# 날짜 형식으로 변환\n",
    "df['연구시작일'] = pd.to_datetime(df['연구시작일'], errors='coerce')\n",
    "df['연구종료일'] = pd.to_datetime(df['연구종료일'], errors='coerce')\n",
    "\n",
    "# 각 환자에 대해 날짜 생성\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notnull(row['연구시작일']) and pd.notnull(row['연구종료일']):\n",
    "        date_range = pd.date_range(start=row['연구시작일'], end=row['연구종료일'])\n",
    "        for date in date_range:\n",
    "            expanded_rows.append({'patient_code': row['patient_code'], '날짜': date, 'gender': row['gender']})\n",
    "\n",
    "# 결과 데이터프레임 생성\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df = expanded_df.replace({'남': '0', '여': '1'})\n",
    "print(expanded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    patient_code         날짜 gender\n",
      "0     PXPN_10008 2024-11-04      0\n",
      "1     PXPN_10008 2024-11-05      0\n",
      "2     PXPN_10008 2024-11-06      0\n",
      "3     PXPN_10008 2024-11-07      0\n",
      "4     PXPN_10008 2024-11-08      0\n",
      "..           ...        ...    ...\n",
      "834   PXPN_10036 2025-05-16      0\n",
      "835   PXPN_10036 2025-05-17      0\n",
      "836   PXPN_10036 2025-05-18      0\n",
      "837   PXPN_10036 2025-05-19      0\n",
      "838   PXPN_10036 2025-05-20      0\n",
      "\n",
      "[839 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "file_path = enroll_path\n",
    "\n",
    "# CSV 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 컬럼 이름 변경 (생년월일 컬럼도 추가)\n",
    "df = df.rename(columns={\n",
    "    '회원코드': 'patient_code',\n",
    "    '2. 성별': 'gender'  # 생년월일 컬럼 이름이 실제 다르면 이 부분 수정 필요\n",
    "})\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "df = df[['patient_code', '연구시작일', '연구종료일', 'gender', '3. 생년월일']]\n",
    "\n",
    "# 날짜 형식으로 변환\n",
    "df['연구시작일'] = pd.to_datetime(df['연구시작일'], errors='coerce')\n",
    "df['연구종료일'] = pd.to_datetime(df['연구종료일'], errors='coerce')\n",
    "df['3. 생년월일'] = pd.to_datetime(df['3. 생년월일'], errors='coerce')\n",
    "\n",
    "\n",
    "# 각 환자에 대해 날짜 생성\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notnull(row['연구시작일']) and pd.notnull(row['연구종료일']):\n",
    "        date_range = pd.date_range(start=row['연구시작일'], end=row['연구종료일'])\n",
    "        for date in date_range:\n",
    "            expanded_rows.append({\n",
    "                'patient_code': row['patient_code'],\n",
    "                '날짜': date,\n",
    "                'gender': '0' if row['gender'] == '남' else '1'\n",
    "            })\n",
    "\n",
    "# 결과 데이터프레임 생성\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# 출력\n",
    "print(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['날짜']    = pd.to_datetime(expanded_df['날짜'])\n",
    "aggregated_df['날짜'] = pd.to_datetime(aggregated_df['날짜'])\n",
    "expanded_answer = pd.merge(expanded_df, aggregated_df, on=['patient_code', '날짜'], how='outer')\n",
    "expanded_answer = expanded_answer.rename(columns={'patient_code': 'ID', '날짜': 'date', 'GAD': 'GAD_7', 'CTQ-1': 'CTQ_1', 'CTQ-2': 'CTQ_2', 'CTQ-3': 'CTQ_3', 'CTQ-4': 'CTQ_4', 'CTQ-5': 'CTQ_5', 'APPQ-1': 'APPQ_1', 'APPQ-2': 'APPQ_2', 'APPQ-3': 'APPQ_3', 'PHQ': 'PHQ_9'})\n",
    "output_path = os.path.join(output_folder, \"questionnaire_test.csv\")\n",
    "expanded_answer.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Panic.csv 없음: ActiveData/PXPN_10010_ActiveData.zip\n",
      "⚠️ Panic.csv 없음: ActiveData/PXPN_10012_ActiveData.zip\n",
      "⚠️ Panic.csv 없음: ActiveData/PXPN_10023_ActiveData.zip\n",
      "⚠️ Panic.csv 없음: ActiveData/PXPN_10034_ActiveData.zip\n",
      "⚠️ Panic.csv 없음: ActiveData/PXPN_10039_ActiveData.zip\n",
      "⚠️ Panic.csv 없음: ActiveData/PXPN_10040_ActiveData.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# 0. 날짜 형식 통일\n",
    "expanded_answer['date'] = pd.to_datetime(expanded_answer['date'])\n",
    "\n",
    "# 1. PXPN별 Panic 날짜 수집\n",
    "PXPN_panic_dates = pd.DataFrame(columns=['ID', 'date'])\n",
    "main_zip_path = zip_path  # 예: '/Users/.../ActiveData.zip'\n",
    "\n",
    "with zipfile.ZipFile(main_zip_path, 'r') as outer_zip:\n",
    "    for inner_name in outer_zip.namelist():\n",
    "        if inner_name.startswith(\"ActiveData/\") and inner_name.endswith('_ActiveData.zip'):\n",
    "            pid = os.path.basename(inner_name).replace('_ActiveData.zip', '')\n",
    "\n",
    "            with outer_zip.open(inner_name) as inner_file:\n",
    "                data = inner_file.read()\n",
    "                inner_bytes = BytesIO(data)\n",
    "\n",
    "                if not zipfile.is_zipfile(inner_bytes):\n",
    "                    print(f\"❌ 내부 zip 아님 (무시됨): {inner_name}\")\n",
    "                    continue\n",
    "\n",
    "                with zipfile.ZipFile(inner_bytes, 'r') as active_zip:\n",
    "                    panic_csvs = [f for f in active_zip.namelist() if f.endswith('Panic.csv')]\n",
    "                    if not panic_csvs:\n",
    "                        print(f\"⚠️ Panic.csv 없음: {inner_name}\")\n",
    "                        continue\n",
    "\n",
    "                    with active_zip.open(panic_csvs[0]) as f:\n",
    "                        df_panic = pd.read_csv(f)\n",
    "                        if '작성일' not in df_panic.columns:\n",
    "                            print(f\"⚠️ '작성일' 없음: {inner_name}\")\n",
    "                            continue\n",
    "\n",
    "                        for 작성일 in df_panic['작성일']:\n",
    "                            PXPN_panic_dates = pd.concat([\n",
    "                                PXPN_panic_dates,\n",
    "                                pd.DataFrame({'ID': [pid], 'date': [작성일]})\n",
    "                            ], ignore_index=True)\n",
    "\n",
    "# 2. 날짜 및 panic 정리\n",
    "PXPN_panic_dates['panic'] = 2\n",
    "PXPN_panic_dates['date'] = pd.to_datetime(PXPN_panic_dates['date']).dt.strftime('%Y-%m-%d')\n",
    "expanded_answer['date'] = pd.to_datetime(expanded_answer['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3. outer merge\n",
    "merged = pd.merge(\n",
    "    PXPN_panic_dates,\n",
    "    expanded_answer,\n",
    "    on=['ID', 'date'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# 4. 우선순위 panic 값 유지\n",
    "merged = (\n",
    "    merged\n",
    "    .sort_values(['ID', 'date', 'panic'], ascending=[True, True, False])\n",
    "    .drop_duplicates(subset=['ID', 'date'], keep='first')\n",
    ")\n",
    "\n",
    "# 5. 전날 panic=1 적용\n",
    "df = merged.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "for _, row in df[df['panic'] == 2].iterrows():\n",
    "    pid = row['ID']\n",
    "    curr_date = row['date']\n",
    "    prev_date = curr_date - pd.Timedelta(days=1)\n",
    "\n",
    "    # 연속된 2 제거\n",
    "    while ((df['ID'] == pid) & (df['date'] == prev_date) & (df['panic'] == 2)).any():\n",
    "        prev_date -= pd.Timedelta(days=1)\n",
    "\n",
    "    mask = (df['ID'] == pid) & (df['date'] == prev_date)\n",
    "    if mask.any():\n",
    "        df.loc[mask & (df['panic'].fillna(0) < 1), 'panic'] = 1\n",
    "\n",
    "df = df.sort_values(['ID', 'date']).reset_index(drop=True)\n",
    "df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "df['panic'] = df['panic'].fillna(0)\n",
    "\n",
    "# 6. Enroll 병합 (age)\n",
    "enroll_df = pd.read_csv(enroll_path, encoding='utf-8')\n",
    "enroll_df = enroll_df.rename(columns={\n",
    "    '회원코드': 'patient_code',\n",
    "    '3. 생년월일': 'birthdate',\n",
    "    '연구종료일': 'end_date'\n",
    "})\n",
    "enroll_df['birthdate'] = pd.to_datetime(enroll_df['birthdate'], errors='coerce')\n",
    "enroll_df['end_date'] = pd.to_datetime(enroll_df['end_date'], errors='coerce')\n",
    "enroll_df['age'] = enroll_df['end_date'].dt.year - enroll_df['birthdate'].dt.year\n",
    "\n",
    "age_df = enroll_df[['patient_code', 'age']].drop_duplicates().rename(columns={'patient_code': 'ID'})\n",
    "final_df = df.merge(age_df, on='ID', how='left')\n",
    "\n",
    "# 7. 저장\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_date.csv\")\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# 파일 경로\n",
    "csv_path = output_path\n",
    "zip_path = zip_path\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df[\"ID\"].str.startswith(\"PXPN\")].copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# 컬럼 초기화\n",
    "for col in ['marriage', 'job', 'alcohol', 'coffee', 'smoking', 'menstruation', 'exercise',\n",
    "            'smkHx', 'drinkHx', 'suicideHx',\n",
    "            'suicide_need']: \n",
    "    df[col] = np.nan\n",
    "\n",
    "# ZIP 열기\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    for pid in df['ID'].unique():\n",
    "        inner_zip_name = f\"ActiveData/{pid}_ActiveData.zip\"\n",
    "        if inner_zip_name not in outer_zip.namelist():\n",
    "            continue\n",
    "\n",
    "        # 내부 ZIP 열기\n",
    "        with outer_zip.open(inner_zip_name) as inner_zip_file:\n",
    "            inner_zip_bytes = BytesIO(inner_zip_file.read())\n",
    "            with zipfile.ZipFile(inner_zip_bytes) as inner_zip:\n",
    "\n",
    "                # 1. Sociodemographic 처리\n",
    "                soc_path = f\"{pid}_Sociodemographic.csv\"\n",
    "                if soc_path in inner_zip.namelist():\n",
    "                    soc = pd.read_csv(inner_zip.open(soc_path), header=None, index_col=0).T\n",
    "                    if '결혼' in soc.columns:\n",
    "                        df.loc[df['ID'] == pid, 'marriage'] = 1 if soc['결혼'].values[0] == '기혼' else 0\n",
    "                    if '현재 직업 유무' in soc.columns:\n",
    "                        df.loc[df['ID'] == pid, 'job'] = 1 if soc['현재 직업 유무'].values[0] == 'Y' else 0\n",
    "                    if '과거 흡연 여부' in soc.columns:\n",
    "                        df.loc[df['ID'] == pid, 'smkHx'] = 1 if soc['과거 흡연 여부'].values[0] == 'Y' else 0\n",
    "                    if '지금까지 음주 여부' in soc.columns:\n",
    "                        df.loc[df['ID'] == pid, 'drinkHx'] = 1 if soc['지금까지 음주 여부'].values[0] == 'Y' else 0\n",
    "                    if '과거 자살 시도 여부' in soc.columns:\n",
    "                        df.loc[df['ID'] == pid, 'suicideHx'] = 1 if soc['과거 자살 시도 여부'].values[0] == 'Y' else 0\n",
    "                    if '지난 1달간 자살시도 여부' in soc.columns:\n",
    "                        df.loc[df['ID'] == pid, 'suicide_need'] = 1 if soc['지난 1달간 자살시도 여부'].values[0] == 'Y' else 0\n",
    "\n",
    "\n",
    "                # 2. Pattern 처리\n",
    "                pat_path = f\"{pid}_Pattern.csv\"\n",
    "                if pat_path in inner_zip.namelist():\n",
    "                    pat = pd.read_csv(inner_zip.open(pat_path))\n",
    "                    pat['작성일'] = pd.to_datetime(pat['작성일'], errors='coerce')\n",
    "\n",
    "                    for idx, row in df[df[\"ID\"] == pid].iterrows():\n",
    "                        d = row[\"date\"]\n",
    "                        today_rows = pat[pat[\"작성일\"] == d]\n",
    "                        for _, r in today_rows.iterrows():\n",
    "                            t = r.get('종류', '')\n",
    "                            st = r.get('세부종류', '')\n",
    "                            amount = r.get('양', None)  # '양' 컬럼 값\n",
    "                            # 운동\n",
    "                            if t == '운동':\n",
    "                                # 양 값이 있으면 그 값을, 없으면 1 로 디폴트\n",
    "                                df.at[idx, 'exercise'] = amount if pd.notna(amount) else 1\n",
    "                            # 카페인\n",
    "                            if t == '카페인':\n",
    "                                df.at[idx, 'coffee'] = amount if pd.notna(amount) else 1\n",
    "                            # 흡연\n",
    "                            if t == '흡연':\n",
    "                                df.at[idx, 'smoking'] = amount if pd.notna(amount) else 1\n",
    "                            # 음주(양이 아닌 단순 여부만 원하면 기존처럼 1로)\n",
    "                            if t == '음주':\n",
    "                                df.at[idx, 'alcohol'] = amount if pd.notna(amount) else 1\n",
    "                            # 생리\n",
    "                            if t == '생리' and st == '생리중':\n",
    "                                df.at[idx, 'menstruation'] = 1\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_dates_and_demo.csv\")\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 1. 경로 설정\n",
    "zip_path = zip_path\n",
    "processed = df  \n",
    "\n",
    "# 3. PXPN ID 목록 추출\n",
    "pxpn_ids = processed[processed['ID'].astype(str).str.startswith('PXPN')]['ID'].unique()\n",
    "\n",
    "# 4. 감정 관련 컬럼 초기화\n",
    "emotion_cols = ['positive_feeling', 'negative', 'positive_E', 'negative_E', 'anxiety', 'annoying']\n",
    "for col in emotion_cols:\n",
    "    if col not in processed.columns:\n",
    "        processed[col] = np.nan\n",
    "\n",
    "# 5. 디버그용 카운터 및 정보\n",
    "match_count = 0\n",
    "no_date_match = 0\n",
    "processed_ids = set()\n",
    "debug_info = []\n",
    "\n",
    "# 6. 외부 zip 열기\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "\n",
    "        # 7. 모든 PXPN ID에 대해 반복\n",
    "        for pid in pxpn_ids:\n",
    "            pid = str(pid).strip()\n",
    "            inner_zip_name = f\"ActiveData/{pid}_ActiveData.zip\"\n",
    "\n",
    "            if inner_zip_name not in outer_zip.namelist():\n",
    "                debug_info.append(f\"❌ ID {pid}: 내부 ZIP 없음 → {inner_zip_name}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with outer_zip.open(inner_zip_name) as inner_zip_file:\n",
    "                    inner_zip_bytes = BytesIO(inner_zip_file.read())\n",
    "\n",
    "                    with zipfile.ZipFile(inner_zip_bytes) as inner_zip:\n",
    "                        checkup_filename = f\"{pid}_Checkup.csv\"\n",
    "\n",
    "                        if checkup_filename not in inner_zip.namelist():\n",
    "                            debug_info.append(f\"⚠️ ID {pid}: Checkup 파일 없음\")\n",
    "                            continue\n",
    "\n",
    "                        # Checkup CSV 읽기\n",
    "                        checkup = pd.read_csv(inner_zip.open(checkup_filename))\n",
    "\n",
    "                        # 날짜 타입 변환\n",
    "                        processed_pid = processed[processed['ID'] == pid].copy()\n",
    "                        processed_pid['date'] = pd.to_datetime(processed_pid['date'], errors='coerce')\n",
    "                        checkup['작성일'] = pd.to_datetime(checkup['작성일'], errors='coerce')\n",
    "\n",
    "                        # 감정 카테고리별 처리\n",
    "                        for category in ['기분', '에너지', '불안', '짜증']:\n",
    "                            category_data = checkup[checkup['종류'] == category]\n",
    "\n",
    "                            for _, row in category_data.iterrows():\n",
    "                                checkup_date = row['작성일']\n",
    "                                score = row['척도']\n",
    "\n",
    "                                for idx, proc_row in processed_pid.iterrows():\n",
    "                                    proc_date = proc_row['date']\n",
    "                                    if (\n",
    "                                        proc_date.year == checkup_date.year and\n",
    "                                        proc_date.month == checkup_date.month and\n",
    "                                        proc_date.day == checkup_date.day\n",
    "                                    ):\n",
    "                                        if category == '기분':\n",
    "                                            if score > 0:\n",
    "                                                processed.at[idx, 'positive_feeling'] = score\n",
    "                                            elif score < 0:\n",
    "                                                processed.at[idx, 'negative'] = score\n",
    "                                        elif category == '에너지':\n",
    "                                            if score > 0:\n",
    "                                                processed.at[idx, 'positive_E'] = score\n",
    "                                            elif score < 0:\n",
    "                                                processed.at[idx, 'negative_E'] = score\n",
    "                                        elif category == '불안':\n",
    "                                            processed.at[idx, 'anxiety'] = score\n",
    "                                        elif category == '짜증':\n",
    "                                            processed.at[idx, 'annoying'] = score\n",
    "\n",
    "                                        match_count += 1\n",
    "                                        processed_ids.add(pid)\n",
    "            except Exception as e:\n",
    "                debug_info.append(f\"❗ ID {pid} 처리 중 오류: {str(e)}\")\n",
    "except Exception as e:\n",
    "    debug_info.append(f\"ZIP 파일 처리 전체 실패: {str(e)}\")\n",
    "\n",
    "\n",
    "# 8. 기분 및 에너지 충돌 조정 (절대값 기준)\n",
    "mask_mood = processed['positive_feeling'].notna() & processed['negative'].notna()\n",
    "for idx in processed[mask_mood].index:\n",
    "    pos = processed.at[idx, 'positive_feeling']\n",
    "    neg = processed.at[idx, 'negative']\n",
    "    if abs(pos) > abs(neg):\n",
    "        processed.at[idx, 'negative'] = 0\n",
    "    elif abs(pos) < abs(neg):\n",
    "        processed.at[idx, 'positive_feeling'] = 0\n",
    "    else:\n",
    "        processed.at[idx, 'positive_feeling'] = 0  # 동일하면 긍정 제거, 부정 유지\n",
    "\n",
    "mask_energy = processed['positive_E'].notna() & processed['negative_E'].notna()\n",
    "for idx in processed[mask_energy].index:\n",
    "    posE = processed.at[idx, 'positive_E']\n",
    "    negE = processed.at[idx, 'negative_E']\n",
    "    if abs(posE) > abs(negE):\n",
    "        processed.at[idx, 'negative_E'] = 0\n",
    "    elif abs(posE) < abs(negE):\n",
    "        processed.at[idx, 'positive_E'] = 0\n",
    "    else:\n",
    "        processed.at[idx, 'positive_E'] = 0\n",
    "\n",
    "# 9. 값이 한쪽만 있을 경우 다른 쪽을 0으로 설정\n",
    "processed.loc[\n",
    "    processed['positive_feeling'].notna() & processed['negative'].isna(),\n",
    "    'negative'\n",
    "] = 0\n",
    "processed.loc[\n",
    "    processed['negative'].notna() & processed['positive_feeling'].isna(),\n",
    "    'positive_feeling'\n",
    "] = 0\n",
    "\n",
    "processed.loc[\n",
    "    processed['positive_E'].notna() & processed['negative_E'].isna(),\n",
    "    'negative_E'\n",
    "] = 0\n",
    "processed.loc[\n",
    "    processed['negative_E'].notna() & processed['positive_E'].isna(),\n",
    "    'positive_E'\n",
    "] = 0\n",
    "\n",
    "# 10. 디버그 출력 (최대 20개)\n",
    "for info in debug_info[:20]:\n",
    "    print(info)\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_panic_demo_mood.csv\")\n",
    "processed.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 컬럼 형식 통일 완료 및 저장됨.\n"
     ]
    }
   ],
   "source": [
    "from pandas.api.types import is_numeric_dtype, is_string_dtype, is_datetime64_any_dtype\n",
    "\n",
    "# 전체 컬럼 순회하며 형식 통일\n",
    "for col in processed.columns:\n",
    "    if col == 'ID':\n",
    "        processed[col] = processed[col].astype(str).str.strip()\n",
    "    elif col == 'date':\n",
    "        processed[col] = pd.to_datetime(processed[col], errors='coerce')  # 문자열 포함 시 자동 처리\n",
    "    elif is_datetime64_any_dtype(processed[col]):\n",
    "        processed[col] = pd.to_datetime(processed[col], errors='coerce')  # datetime이면 그대로\n",
    "    elif is_string_dtype(processed[col]):\n",
    "        processed[col] = processed[col].astype(str).str.strip()  # 문자열이면 정리\n",
    "    else:\n",
    "        # 예외적인 경우도 문자열로 통일\n",
    "        processed[col] = processed[col].astype(str).str.strip()\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"processed.csv\")\n",
    "processed.to_csv(output_path, index=False)\n",
    "print(\"모든 컬럼 형식 통일 완료 및 저장됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'date', 'panic', 'gender', 'PHQ_9', 'STAI_X2', 'CSM', 'CTQ_1',\n",
      "       'CTQ_2', 'CTQ_3', 'CTQ_4', 'CTQ_5', 'KRQ', 'MDQ', 'ACQ', 'APPQ_1',\n",
      "       'APPQ_2', 'APPQ_3', 'BSQ', 'GAD_7', 'BRIAN', 'age', 'marriage', 'job',\n",
      "       'alcohol', 'coffee', 'smoking', 'menstruation', 'exercise', 'smkHx',\n",
      "       'drinkHx', 'suicideHx', 'suicide_need', 'positive_feeling', 'negative',\n",
      "       'positive_E', 'negative_E', 'anxiety', 'annoying'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(processed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
