{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PXPN_DIR = \"./raw_data/PXPN\"\n",
    "# 엑셀 파일 경로 (실제 경로로 수정)\n",
    "enroll_file_name = \"1. 픽셀패닉 enroll 정보_250516\"\n",
    "zip_file_name = \"pixelpanic_raw_data.zip\"\n",
    "output_folder_name = \"./tmp/PXPN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1aa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from path_utils import get_file_path\n",
    "\n",
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")\n",
    "zip_path = get_file_path(RAW_PXPN_DIR, f\"{zip_file_name}\")\n",
    "output_folder = get_file_path(output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c89079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import read_all_data\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "step = read_all_data('Step', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Convert obtained_at to datetime then split into date and time\n",
    "step['started_at'] = pd.to_datetime(step['started_at'])\n",
    "step['date'] = step['started_at'].dt.date.astype(str)\n",
    "step['time'] = step['started_at'].dt.time.astype(str)\n",
    "\n",
    "\n",
    "# Drop unneeded columns and reset index\n",
    "step = step.drop(columns=['started_at', 'ended_at', 'obtained_at']).reset_index(drop=True)\n",
    "output_path = os.path.join(output_folder, \"step.csv\")\n",
    "step.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529d2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data preprocessing\n",
    "step['steps'] = pd.to_numeric(step['steps'])\n",
    "step_nonzero = step[step.steps != 0]\n",
    "\n",
    "#statistical analysis\n",
    "step_mean = step_nonzero.groupby(['ID','date'])['steps'].mean().reset_index().rename(columns={'steps':'step_mean'})\n",
    "step_var = step_nonzero.groupby(['ID','date'])['steps'].var().reset_index().rename(columns={'steps':'step_var'})\n",
    "step_max = step_nonzero.groupby(['ID','date'])['steps'].max().reset_index().rename(columns={'steps':'step_max'})\n",
    "\n",
    "#calculation of step_hvar_mean\n",
    "step_nonzero['hour'] = pd.to_datetime(step_nonzero['time']).dt.hour \n",
    "step_hvar = step_nonzero.groupby(['ID','date','hour'])['steps'].var().reset_index()\n",
    "\n",
    "step_hvar_mean = step_hvar.groupby(['ID','date'])['steps'].mean().reset_index().rename(columns={'steps':'step_hvar_mean'})\n",
    "\n",
    "# create total daily steps\n",
    "daily_steps = step.groupby(['ID','date'])['steps'].sum().reset_index()\n",
    "\n",
    "#data merge\n",
    "step_statistics_merged= pd.merge(left=step, right=step_var, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_max, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_mean, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_hvar_mean, how=\"outer\", on =['date','ID'])\n",
    "\n",
    "#data preprocessing\n",
    "step_statistics_merged['datetime'] = step_statistics_merged['date'] + ' ' + step_statistics_merged['time']\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_stactistics.csv\")\n",
    "step_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "step_date= pd.merge(left=daily_steps, right=step_var, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_max, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_mean, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_hvar_mean, how=\"left\", on =['date','ID'])\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_date.csv\")\n",
    "step_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14fa5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step = step_date\n",
    "step['date'] = pd.to_datetime(step['date'])\n",
    "id_list = step['ID'].unique()\n",
    "\n",
    "step_delta = pd.DataFrame(columns=['ID', 'date', 'steps', 'step_max', 'step_mean', 'step_hvar_mean', 'step_delta', 'step_max_delta',\n",
    "                                   'step_mean_delta', 'step_hvar_mean_delta', 'step_delta2', 'step_max_delta2',\n",
    "                                   'step_mean_delta2', 'step_hvar_mean_delta2'])\n",
    "for id in id_list:\n",
    "    step_id = step.loc[(step.ID == id)]\n",
    "    time_per_day = pd.date_range(step_id.date.min(), step_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    step_id = pd.merge(step_id, temp, how='right', on='date')\n",
    "    step_id.ID = id\n",
    "    step_id['step_delta'] = step_id['steps'].diff()\n",
    "    step_id['step_delta2'] = step_id['steps'].diff(periods=2)\n",
    "    step_id['step_max_delta'] = step_id['step_max'].diff()\n",
    "    step_id['step_max_delta2'] = step_id['step_max'].diff(periods=2)\n",
    "    step_id['step_mean_delta'] = step_id['step_mean'].diff()\n",
    "    step_id['step_mean_delta2'] = step_id['step_mean'].diff(periods=2)\n",
    "    step_id['step_hvar_mean_delta'] = step_id['step_hvar_mean'].diff()\n",
    "    step_id['step_hvar_mean_delta2'] = step_id['step_hvar_mean'].diff(periods=2)\n",
    "    step_delta = pd.concat([step_delta, step_id], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "step_delta['date'] = step_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "step_delta.reset_index(drop=True, inplace=True)\n",
    "# Drop rows where steps, step_delta, and step_delta2 are all zero\n",
    "step_delta = step_delta[~((step_delta['steps'] == 0) & (step_delta['step_delta'] == 0) & (step_delta['step_delta2'] == 0))]\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_delta.csv\")\n",
    "step_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a1a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import read_all_data\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "sleep = read_all_data('Sleep', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Map various sleep type labels into standardized SLT codes\n",
    "sleep['type'] = sleep['type'].replace({\n",
    "    'SLT1': 'SLT3',\n",
    "    'SLT0': 'SLT2',\n",
    "    'asleepCore': 'SLT4',\n",
    "    'asleepDeep': 'SLT5',\n",
    "    'asleepREM': 'SLT6',\n",
    "    'asleepUnspecified': 'SLT2',\n",
    "    'awake': 'SLT1'\n",
    "})\n",
    "\n",
    "# Ensure datetime types for start and end\n",
    "sleep['started_at'] = pd.to_datetime(sleep['started_at'])\n",
    "sleep['ended_at'] = pd.to_datetime(sleep['ended_at'])\n",
    "\n",
    "# Calculate session duration\n",
    "sleep['duration'] = sleep['ended_at'] - sleep['started_at']\n",
    "\n",
    "# Extract date for grouping\n",
    "sleep['date'] = sleep['started_at'].dt.date\n",
    "\n",
    "# Pivot to sum durations per SLT type\n",
    "sleep_summary = sleep.pivot_table(\n",
    "    index=['ID', 'date'],\n",
    "    columns='type',\n",
    "    values='duration',\n",
    "    aggfunc='sum',\n",
    "    fill_value=pd.Timedelta(0)\n",
    ")\n",
    "\n",
    "# Ensure all SLT1–SLT6 columns exist\n",
    "for slt in ['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6']:\n",
    "    if slt not in sleep_summary.columns:\n",
    "        sleep_summary[slt] = pd.Timedelta(0)\n",
    "\n",
    "# Compute total sleep as sum of all SLT durations\n",
    "sleep_summary['total_sleep'] = sleep_summary[['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6']].sum(axis=1)\n",
    "\n",
    "# Convert SLT and total_sleep durations from Timedelta to hours (float)\n",
    "for col in ['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep']:\n",
    "    sleep_summary[col] = sleep_summary[col] / pd.Timedelta(hours=1)\n",
    "\n",
    "# Convert index back to columns\n",
    "sleep_summary = sleep_summary.reset_index()\n",
    "\n",
    "# Overwrite sleep with the summary table\n",
    "sleep = sleep_summary\n",
    "\n",
    "output_path = os.path.join(output_folder, \"sleep_type.csv\")\n",
    "sleep.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcadd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import read_all_data\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "sleep = read_all_data('Sleep', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "sleep = sleep.drop(columns='type')\n",
    "output_path = os.path.join(output_folder, \"sleep_log.csv\")\n",
    "sleep.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f44ec4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import read_all_data\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "heartrate = read_all_data('HeartRate', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Convert obtained_at to datetime then split into date and time\n",
    "heartrate['obtained_at'] = pd.to_datetime(heartrate['obtained_at'])\n",
    "heartrate['date'] = heartrate['obtained_at'].dt.date.astype(str)\n",
    "heartrate['time'] = heartrate['obtained_at'].dt.time.astype(str)\n",
    "\n",
    "# Drop unneeded columns and reset index\n",
    "heartrate = heartrate.drop(columns=['started_at', 'ended_at', 'obtained_at']).reset_index(drop=True)\n",
    "output_path = os.path.join(output_folder, \"HR.csv\")\n",
    "heartrate.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ccb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data\n",
    "HR= heartrate\n",
    "HR = HR.rename(columns={'heart_rate' : 'HR'})\n",
    "#data preprocessing\n",
    "HR['HR'] = pd.to_numeric(HR['HR'])\n",
    "HR_nonzero = HR[HR.HR != 0]\n",
    "\n",
    "#statistical analysis\n",
    "HR_mean = HR_nonzero.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "HR_var = HR_nonzero.groupby(['ID','date'])['HR'].var().reset_index()\n",
    "HR_min = HR_nonzero.groupby(['ID','date'])['HR'].min().reset_index()\n",
    "HR_max = HR_nonzero.groupby(['ID','date'])['HR'].max().reset_index()\n",
    "\n",
    "#calculation of HR_hvar_mean\n",
    "HR['hour'] = pd.to_datetime(HR['time']).dt.hour \n",
    "HR_hvar = HR.groupby(['ID','date','hour'])['HR'].var().reset_index()\n",
    "HR_hvar_mean = HR_hvar.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "\n",
    "#data merge\n",
    "HR_statistics_merged = pd.merge(left=HR, right=HR_var, how=\"outer\", on =['date','ID'], suffixes=['', '_var'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_min, how=\"outer\", on =['date','ID'], suffixes=['', '_min'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_max, how=\"outer\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_hvar_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "\n",
    "#data preprocessing\n",
    "HR_statistics_merged['datetime'] = HR_statistics_merged['date'] + ' ' + HR_statistics_merged['time']\n",
    "HR_statistics_merged.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"hr_stactistics_fixed.csv\")\n",
    "HR_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "HR_date = pd.merge(left=HR_var, right=HR_max, how=\"left\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_mean, how=\"left\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_hvar_mean, how=\"left\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "HR_date.rename(columns = {'HR':'HR_var'}, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"HR_date_fixed.csv\")\n",
    "HR_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c03bcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs: 100%|██████████| 18/18 [00:00<00:00, 19.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    output_path = os.path.join(output_folder, \"HR.csv\")\n",
    "    HR = pd.read_csv(output_path)\n",
    "    if 'heart_rate' in HR.columns:\n",
    "        HR.rename(columns={'heart_rate': 'HR'}, inplace=True)\n",
    "    HR['HR'] = pd.to_numeric(HR['HR'], errors='coerce')\n",
    "    HR['datetime'] = pd.to_datetime(HR['date'] + ' ' + HR['time'], errors='coerce')\n",
    "    \n",
    "    out = []\n",
    "    for pid, grp in tqdm(HR.groupby('ID'), desc='Processing IDs'):\n",
    "        grp = grp.dropna(subset=['datetime']).set_index('datetime').sort_index()\n",
    "        for day, day_grp in grp.groupby(grp.index.date):\n",
    "            orig_count = day_grp['HR'].dropna().shape[0]\n",
    "            base = pd.to_datetime(f\"{day}\") + pd.to_timedelta(np.arange(1440), unit='m')\n",
    "            df_full = pd.DataFrame(index=base)\n",
    "            if orig_count > 720:\n",
    "                tmp = day_grp[['HR']].resample('1min').mean()\n",
    "                tmp = tmp.reindex(df_full.index)\n",
    "                tmp['HR'] = tmp['HR'].interpolate(method='time', limit=30, limit_direction='both')\n",
    "                df_full['HR'] = tmp['HR']\n",
    "            else:\n",
    "                df_full['HR'] = np.nan\n",
    "            df_full['ID'] = pid\n",
    "            df_full['date'] = pd.to_datetime(day).date()\n",
    "            df_full['time'] = df_full.index.time.astype(str)\n",
    "            out.append(df_full[['HR', 'ID', 'date', 'time']])\n",
    "    \n",
    "    HR_interp = pd.concat(out, ignore_index=True)\n",
    "    output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "    HR_interp.to_csv(output_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cee9cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PXPN_10006 2024-11-25 19.265145588436745 15.424105846332768 69.03950135486359\n",
      "PXPN_10006 2024-11-26 17.994066748389073 11.548442258213504 70.02448313558153\n",
      "PXPN_10007 2024-11-30 21.576007247313072 1.0231328086876441 81.67332569912041\n",
      "PXPN_10008 2024-11-05 19.770565746121743 18.491975889228527 77.24021177435837\n",
      "PXPN_10008 2024-11-12 19.614707422070083 16.678075562293504 79.32278832555278\n",
      "PXPN_10008 2024-11-13 18.541739662094216 7.441858093046665 72.66930650655237\n",
      "PXPN_10008 2024-11-21 21.014255048695478 17.61298673939339 77.96165404811236\n",
      "PXPN_10008 2024-11-28 15.441927204364685 7.686289341123911 78.49288894876533\n",
      "PXPN_10011 2024-11-21 1.0102779284418977 16.54302164609696 104.18441510054075\n",
      "PXPN_10011 2024-11-25 1.6321218131114865 28.999639909969943 110.13771794332347\n",
      "PXPN_10011 2024-11-26 12.12516858682338 19.52795850434224 85.28402832094925\n",
      "PXPN_10011 2024-11-30 16.273908480290647 23.153342481335066 77.15986539844646\n",
      "PXPN_10018 2024-11-22 12.838550456043496 12.629442879917468 95.11612401391127\n",
      "PXPN_10018 2024-11-23 4.003702092919552 3.0919085018931907 101.73483724666293\n",
      "PXPN_10018 2024-11-24 15.717216565166806 12.445782503237542 85.53849623563164\n",
      "PXPN_10018 2024-12-11 18.31701870554639 7.52149250488739 82.91041441775818\n",
      "PXPN_10018 2024-12-14 17.05285543892691 14.875695551208603 84.23658125577371\n",
      "PXPN_10018 2024-12-15 16.668990067654978 10.094832279708188 79.74817708333336\n",
      "PXPN_10019 2024-11-29 3.249861804032933 16.965011841921857 97.4097492610372\n",
      "PXPN_10019 2024-12-01 9.723247302759939 13.93407133187909 94.95446363321123\n",
      "PXPN_10019 2024-12-02 1.3896353433904118 14.536844864817061 89.5164455374496\n",
      "PXPN_10019 2024-12-05 23.356591655393903 8.093225372206826 100.24790994443643\n",
      "PXPN_10019 2024-12-09 4.534198249228057 5.578819602018054 91.22544411501048\n",
      "PXPN_10019 2024-12-10 20.247840427780787 5.3644447077505 89.42592769759663\n",
      "PXPN_10019 2024-12-12 19.216227869652577 11.884380487408414 94.6807323416201\n",
      "PXPN_10019 2024-12-13 0.09545809020612023 8.67516950475084 96.18370066316555\n",
      "PXPN_10020 2024-12-01 13.598210072891963 4.641246848736432 71.928072433549\n",
      "PXPN_10020 2024-12-12 17.101773565646525 6.330497389984483 74.97331848336256\n",
      "PXPN_10020 2024-12-18 13.927363530972366 3.7338209907612483 95.0818034013695\n",
      "PXPN_10020 2024-12-19 19.506716527865986 4.40311201589614 83.1343386897696\n",
      "PXPN_10021 2024-12-03 22.060196517827862 9.664652607118528 85.81296797627525\n",
      "PXPN_10021 2024-12-08 17.366658263548537 5.558295071231929 82.32782911918329\n",
      "PXPN_10021 2024-12-15 20.832274285970676 6.497181080658251 95.60045388446433\n",
      "PXPN_10021 2024-12-16 20.757140681141244 23.36720809596385 94.90687157465666\n",
      "PXPN_10021 2024-12-17 21.09293980924047 15.645461102899539 88.60360801726073\n",
      "PXPN_10021 2024-12-20 0.6667520331368888 7.239094372184745 80.43704902447088\n",
      "PXPN_10021 2024-12-26 0.08557542275468581 20.780741156859825 89.29468823232845\n",
      "PXPN_10021 2024-12-29 1.7992143617539924 15.90822888103892 88.33551675386079\n",
      "PXPN_10023 2024-12-30 20.73655866904504 17.243737290990232 60.99428840671681\n",
      "PXPN_10023 2024-12-31 21.816257206776385 15.955368505632993 63.86434838854189\n",
      "PXPN_10023 2025-01-01 0.859281509896012 10.25196220462979 62.525507432382454\n",
      "PXPN_10023 2025-01-02 7.364156709516787 8.641112233936834 66.36348326761178\n",
      "PXPN_10023 2025-01-06 19.374890990884477 7.976252706233866 57.456900463541096\n",
      "PXPN_10023 2025-01-07 23.45918677497874 12.34566674775259 59.30987639964674\n",
      "PXPN_10023 2025-01-09 20.562621415023756 11.168626483814096 65.0375373512386\n",
      "PXPN_10023 2025-01-13 6.928041247036835 2.8358034062800326 67.06031461545302\n",
      "PXPN_10023 2025-01-15 20.86585694732411 9.11294117029639 63.214165761824106\n",
      "PXPN_10023 2025-01-17 14.90123680783134 13.494690802771903 64.16975827687146\n",
      "PXPN_10023 2025-01-18 13.667815237856821 16.349600254329793 62.861257939435184\n",
      "PXPN_10023 2025-01-20 18.85769252527684 1.7627516323320973 67.61284629739595\n",
      "PXPN_10025 2025-01-25 19.03311968859705 5.163572934201304 74.43813414594663\n",
      "PXPN_10025 2025-02-02 18.34720720735173 8.066243744632276 82.50694692460317\n",
      "PXPN_10025 2025-02-05 15.853316166915446 13.290294033874781 80.43897070311843\n",
      "PXPN_10025 2025-02-06 17.41396950017804 15.923772306616431 86.99616855913351\n",
      "PXPN_10025 2025-02-07 23.114291061966572 7.430475478344984 84.67768330627703\n",
      "PXPN_10029 2025-02-26 19.004897319676022 5.418036291402179 85.17138197462128\n",
      "PXPN_10029 2025-02-27 16.870708068638628 11.824764400548423 89.56223286874986\n",
      "PXPN_10029 2025-02-28 14.374066168503326 4.834525751617449 90.10256683557924\n",
      "PXPN_10029 2025-03-01 18.33363147690592 15.13568126262146 85.98692069821824\n",
      "PXPN_10029 2025-03-02 17.63619970599103 13.430788702441191 83.57749129901305\n",
      "PXPN_10029 2025-03-03 20.247033056655155 15.444699148878694 79.21047725927662\n",
      "PXPN_10029 2025-03-04 20.333731111761534 9.161354681689216 78.93979364962846\n",
      "PXPN_10029 2025-03-05 17.899076974598866 18.001019721123477 80.66873133533619\n",
      "PXPN_10029 2025-03-06 18.307582464456395 18.491634133738533 83.52456650064993\n",
      "PXPN_10029 2025-03-07 13.949784316699255 4.5014383172795105 92.4149702110593\n",
      "PXPN_10029 2025-03-08 18.370731155008905 14.804664097435284 79.45952257093498\n",
      "PXPN_10029 2025-03-09 19.955002621435025 15.248374849058917 82.21573239001246\n",
      "PXPN_10030 2025-02-28 12.68462055345593 5.302849533370088 95.58392209125404\n",
      "PXPN_10030 2025-03-02 12.039369330727565 7.392622387392926 93.34318673405065\n",
      "PXPN_10030 2025-03-03 15.435194517967785 7.103949272237959 94.60759974399423\n",
      "PXPN_10030 2025-03-05 8.766332457978208 6.437251089177146 88.05657988417862\n",
      "PXPN_10030 2025-03-06 4.981921414096977 1.6314579210394122 89.28969868015172\n",
      "PXPN_10030 2025-03-07 13.819280209145349 15.27700870924181 95.57664379042366\n",
      "PXPN_10030 2025-03-08 11.985274078309816 5.290023109210616 92.24649150980339\n",
      "PXPN_10030 2025-03-09 10.660604476523027 10.082218784957679 95.391762600439\n",
      "PXPN_10030 2025-03-10 17.082415598766232 10.308921602069054 93.89878153781329\n",
      "PXPN_10037 2025-03-14 15.66154032304465 13.335555819193543 71.02455932909935\n",
      "PXPN_10037 2025-03-16 20.281515426143763 18.773899347706944 73.82376590218551\n",
      "PXPN_10037 2025-03-18 19.007227264295103 12.128105283789166 67.76366591886352\n",
      "PXPN_10037 2025-03-19 16.032058647388027 18.701209000644898 73.27247163704787\n",
      "PXPN_10037 2025-03-20 21.77701436053607 23.864098879348134 77.63663567021\n",
      "PXPN_10037 2025-03-21 19.533088118351984 21.56709842899765 78.54734451905756\n",
      "PXPN_10037 2025-03-22 17.51376519959043 18.785068059308912 80.37914677551032\n",
      "PXPN_10037 2025-03-24 18.89713558036569 16.05439337200192 69.04866800809813\n",
      "PXPN_10037 2025-03-25 20.75780891808297 19.05646764205282 71.07719493108739\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils_for_preprocessing import mesor, amplitude, acrophase\n",
    "output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "HR_interpolated = pd.read_csv(output_path)\n",
    "\n",
    "HR_interpolated['HR'] = pd.to_numeric(HR_interpolated['HR'])\n",
    "\n",
    "id_list = HR_interpolated['ID'].unique()\n",
    "circadian_data = pd.DataFrame(columns=['ID','date','acr','amp','mesor'])\n",
    "                  \n",
    "for id in id_list:\n",
    "    temp_id = HR_interpolated.loc[(HR_interpolated['ID'] == id)]\n",
    "    temp_id.reset_index(inplace=True)\n",
    "    temp_id = temp_id.drop('index', axis=1)\n",
    "    date_list =temp_id['date'].unique()\n",
    "    for date in date_list:\n",
    "        temp_date = temp_id.loc[(temp_id['date'] == date)]\n",
    "        temp_date.reset_index(inplace=True)\n",
    "        temp_date = temp_date.drop('index', axis=1)\n",
    "        temp_date.reset_index(inplace=True)  \n",
    "        if temp_date.HR.count() > 720:\n",
    "            acr = acrophase(temp_date['index'], temp_date['HR'])\n",
    "            amp = amplitude(temp_date['index'], temp_date['HR'])\n",
    "            mes = mesor(temp_date['index'], temp_date['HR'])\n",
    "            new_row = pd.DataFrame([[id, date, acr, amp, mes]], columns=['ID','date','acr','amp','mesor'])\n",
    "            circadian_data = pd.concat([circadian_data, new_row], ignore_index=True)\n",
    "            print(id, date, acr, amp, mes)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian_data.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7c8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian = pd.read_csv(output_path)\n",
    "circadian['date'] = pd.to_datetime(circadian['date'])\n",
    "id_list = circadian['ID'].unique()\n",
    "\n",
    "circadian_delta = pd.DataFrame(columns=['ID', 'date', 'acr', 'amp', 'mesor','acr_delta', 'acr_delta2', 'amp_delta', 'amp_delta2', 'mesor_delta', 'mesor_delta2'])\n",
    "for id in id_list:\n",
    "    circadian_id = circadian.loc[(circadian.ID == id)]\n",
    "    time_per_day = pd.date_range(circadian_id.date.min(), circadian_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    circadian_id = pd.merge(circadian_id, temp, how='right', on='date')\n",
    "    circadian_id.ID = id\n",
    "    circadian_id['acr_delta'] = circadian_id['acr'].diff()\n",
    "    circadian_id['acr_delta2'] = circadian_id['acr'].diff(periods=2)\n",
    "    circadian_id['amp_delta'] = circadian_id['amp'].diff()\n",
    "    circadian_id['amp_delta2'] = circadian_id['amp'].diff(periods=2)\n",
    "    circadian_id['mesor_delta'] = circadian_id['mesor'].diff()\n",
    "    circadian_id['mesor_delta2'] = circadian_id['mesor'].diff(periods=2)\n",
    "    circadian_delta = pd.concat([circadian_delta, circadian_id], axis=0)\n",
    "\n",
    "circadian_delta['date'] = circadian_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "circadian_delta.reset_index(drop=True, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_delta_720.csv\")\n",
    "circadian_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62908999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c8d45b0c814b128a1c53d9c699dbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IDs:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f10edfe6e364dcb9bf3860a40d120d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils_for_preprocessing import (\n",
    "    check_bandpower_value_a,\n",
    "    check_bandpower_value_b,\n",
    "    check_bandpower_value_c,\n",
    "    check_bandpower_value_d,\n",
    ")\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "# 1) 파일 로드 & 타입 변환\n",
    "HR = pd.read_csv(\n",
    "    output_path,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "HR[\"HR\"] = pd.to_numeric(HR[\"HR\"], errors=\"coerce\")\n",
    "\n",
    "# ——— build per-minute DataFrame ———\n",
    "df_per_min = pd.DataFrame(columns=['ID','HR','date'])\n",
    "for id in HR['ID'].unique():\n",
    "    df_id = HR[HR['ID'] == id]\n",
    "    time_per_min = pd.date_range(df_id['date'].min(), df_id['date'].max(), freq='min')\n",
    "    temp = pd.DataFrame({'date': time_per_min})\n",
    "    df_id = pd.merge(df_id, temp, how='right', on='date')\n",
    "    df_id['ID'] = id\n",
    "    df_per_min = pd.concat([df_per_min, df_id], axis=0)\n",
    "\n",
    "df_per_min[\"day\"] = df_per_min[\"date\"].dt.date\n",
    "\n",
    "# 4) 하루 그룹 하나당 밴드파워 계산 함수\n",
    "def compute_bandpower_for_group(group):\n",
    "    (id_, day), sub = group\n",
    "    valid_count = sub['HR'].notna().sum()\n",
    "    if valid_count <= 720:\n",
    "        return None\n",
    "    idx = np.arange(len(sub))\n",
    "    hr  = sub[\"HR\"].to_numpy()\n",
    "    return {\n",
    "        \"ID\":           id_,\n",
    "        \"date\":         pd.Timestamp(day),\n",
    "        \"bandpower_a\":  check_bandpower_value_a(idx, hr),\n",
    "        \"bandpower_b\":  check_bandpower_value_b(idx, hr),\n",
    "        \"bandpower_c\":  check_bandpower_value_c(idx, hr),\n",
    "        \"bandpower_d\":  check_bandpower_value_d(idx, hr),\n",
    "    }\n",
    "\n",
    "groups = df_per_min.groupby([\"ID\",\"day\"], sort=False)\n",
    "\n",
    "# 5) tqdm_joblib 로 진행률 표시하며 병렬 처리\n",
    "with tqdm_joblib(tqdm(total=df_per_min[\"ID\"].nunique(), desc=\"IDs\")):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda g: compute_bandpower_for_group(g))(grp)\n",
    "        for grp in groups\n",
    "    )\n",
    "\n",
    "# 6) None 삭제 & DataFrame 생성\n",
    "records = [r for r in results if r is not None]\n",
    "bandpower_df = pd.DataFrame(records)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"bandpower_720.csv\")\n",
    "bandpower_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24633c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20f631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b35e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3521422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
