{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bda39e0",
   "metadata": {},
   "source": [
    "Required Packages:\n",
    "- mne\n",
    "- CosinorPy\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f150e",
   "metadata": {},
   "source": [
    "# 📚 | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from library.path_utils import get_file_path, to_absolute_path\n",
    "from utils_for_preprocessing import read_all_data, mesor, amplitude, acrophase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PXPN_DIR = \"./raw_data/PXPN\"\n",
    "# 엑셀 파일 경로 (실제 경로로 수정)\n",
    "enroll_file_name = \"pxpn_enroll_info\"\n",
    "zip_file_name = \"pixelpanic_raw_data.zip\"\n",
    "output_folder_name = \"./_tmp/PXPN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1aa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")\n",
    "zip_path = get_file_path(RAW_PXPN_DIR, f\"{zip_file_name}\")\n",
    "output_folder = to_absolute_path(output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21415454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 1) ZIP 파일 경로 확인\n",
    "print(\"ZIP exists:\", os.path.exists(zip_path), \"→\", zip_path)\n",
    "\n",
    "# 2) ZIP 열어서 전체 목록 출력\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    names = z.namelist()\n",
    "    print(f\"총 엔트리 수: {len(names)}\\n\")\n",
    "\n",
    "    # (a) 맨 위 20개만 찍어보기\n",
    "    print(\"=== 처음 20개 항목 ===\")\n",
    "    for name in names[:20]:\n",
    "        print(\" \", name)\n",
    "    print(\"...\")\n",
    "\n",
    "    # (b) PassiveData 포함된 목록만 찍기\n",
    "    pd_entries = [n for n in names if 'PassiveData/' in n]\n",
    "    print(f\"\\nPassiveData 포함 항목 수: {len(pd_entries)}\")\n",
    "    for n in pd_entries[:20]:\n",
    "        print(\" \", n)\n",
    "    if len(pd_entries) > 20:\n",
    "        print(\"  ...\")\n",
    "\n",
    "    # (c) 특정 ID 찾기 (예: PID='12345' 일 때)\n",
    "    sample_id = '12345'  # 실제 디버깅하고 싶은 PXPN ID로 바꿔주세요\n",
    "    matches = [n for n in names if f\"{sample_id}_PassiveData.zip\" in n]\n",
    "    print(f\"\\nID={sample_id} 관련 ZIP 매칭 개수:\", len(matches))\n",
    "    for m in matches:\n",
    "        print(\" \", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e89fde",
   "metadata": {},
   "source": [
    "# ⚙️ | Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05718a85",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "step = read_all_data('Step', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "step['started_at'] = step['started_at'].str.replace(r'\\.\\d+', '', regex=True)\n",
    "# Convert obtained_at to datetime then split into date and time\n",
    "step['started_at'] = pd.to_datetime(step['started_at'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "step['date'] = step['started_at'].dt.date.astype(str)\n",
    "step['time'] = step['started_at'].dt.time.astype(str)\n",
    "\n",
    "\n",
    "# Drop unneeded columns and reset index\n",
    "step = step.drop(columns=['started_at', 'ended_at', 'obtained_at']).reset_index(drop=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, \"step.csv\")\n",
    "step.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "step['steps'] = pd.to_numeric(step['steps'])\n",
    "step_nonzero = step[step.steps != 0]\n",
    "\n",
    "#statistical analysis\n",
    "step_mean = step_nonzero.groupby(['ID','date'])['steps'].mean().reset_index().rename(columns={'steps':'step_mean'})\n",
    "step_var = step_nonzero.groupby(['ID','date'])['steps'].var().reset_index().rename(columns={'steps':'step_var'})\n",
    "step_max = step_nonzero.groupby(['ID','date'])['steps'].max().reset_index().rename(columns={'steps':'step_max'})\n",
    "\n",
    "#calculation of step_hvar_mean\n",
    "step_nonzero['hour'] = pd.to_datetime(step_nonzero['time']).dt.hour \n",
    "step_hvar = step_nonzero.groupby(['ID','date','hour'])['steps'].var().reset_index()\n",
    "\n",
    "step_hvar_mean = step_hvar.groupby(['ID','date'])['steps'].mean().reset_index().rename(columns={'steps':'step_hvar_mean'})\n",
    "\n",
    "# create total daily steps\n",
    "daily_steps = step.groupby(['ID','date'])['steps'].sum().reset_index()\n",
    "\n",
    "#data merge\n",
    "step_statistics_merged= pd.merge(left=step, right=step_var, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_max, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_mean, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_hvar_mean, how=\"outer\", on =['date','ID'])\n",
    "\n",
    "#data preprocessing\n",
    "step_statistics_merged['datetime'] = step_statistics_merged['date'] + ' ' + step_statistics_merged['time']\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_stactistics.csv\")\n",
    "step_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "step_date= pd.merge(left=daily_steps, right=step_var, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_max, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_mean, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_hvar_mean, how=\"left\", on =['date','ID'])\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_date.csv\")\n",
    "step_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = step_date\n",
    "step['date'] = pd.to_datetime(step['date'])\n",
    "id_list = step['ID'].unique()\n",
    "\n",
    "step_delta = pd.DataFrame(columns=['ID', 'date', 'steps', 'step_max', 'step_mean', 'step_hvar_mean', 'step_delta', 'step_max_delta',\n",
    "                                   'step_mean_delta', 'step_hvar_mean_delta', 'step_delta2', 'step_max_delta2',\n",
    "                                   'step_mean_delta2', 'step_hvar_mean_delta2'])\n",
    "for id in id_list:\n",
    "    step_id = step.loc[(step.ID == id)]\n",
    "    time_per_day = pd.date_range(step_id.date.min(), step_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    step_id = pd.merge(step_id, temp, how='right', on='date')\n",
    "    step_id.ID = id\n",
    "    step_id['step_delta'] = step_id['steps'].diff()\n",
    "    step_id['step_delta2'] = step_id['steps'].diff(periods=2)\n",
    "    step_id['step_max_delta'] = step_id['step_max'].diff()\n",
    "    step_id['step_max_delta2'] = step_id['step_max'].diff(periods=2)\n",
    "    step_id['step_mean_delta'] = step_id['step_mean'].diff()\n",
    "    step_id['step_mean_delta2'] = step_id['step_mean'].diff(periods=2)\n",
    "    step_id['step_hvar_mean_delta'] = step_id['step_hvar_mean'].diff()\n",
    "    step_id['step_hvar_mean_delta2'] = step_id['step_hvar_mean'].diff(periods=2)\n",
    "    step_delta = pd.concat([step_delta, step_id], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "step_delta['date'] = step_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "step_delta.reset_index(drop=True, inplace=True)\n",
    "# Drop rows where steps, step_delta, and step_delta2 are all zero\n",
    "step_delta = step_delta[~((step_delta['steps'] == 0) & (step_delta['step_delta'] == 0) & (step_delta['step_delta2'] == 0))]\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_delta.csv\")\n",
    "step_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep = read_all_data('Sleep', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Map various sleep type labels into standardized SLT codes\n",
    "sleep['type'] = sleep['type'].replace({\n",
    "    'SLT1': 'SLT3',\n",
    "    'SLT0': 'SLT2',\n",
    "    'asleepCore': 'SLT4',\n",
    "    'asleepDeep': 'SLT5',\n",
    "    'asleepREM': 'SLT6',\n",
    "    'asleepUnspecified': 'SLT2',\n",
    "    'awake': 'SLT1'\n",
    "})\n",
    "\n",
    "# Ensure datetime types for start and end\n",
    "sleep['started_at'] = pd.to_datetime(sleep['started_at'], format='mixed')\n",
    "sleep['ended_at'] = pd.to_datetime(sleep['ended_at'], format='mixed')\n",
    "\n",
    "# Calculate session duration\n",
    "sleep['duration'] = sleep['ended_at'] - sleep['started_at']\n",
    "\n",
    "# Extract date for grouping\n",
    "sleep['date'] = sleep['started_at'].dt.date\n",
    "\n",
    "# Pivot to sum durations per SLT type\n",
    "sleep_summary = sleep.pivot_table(\n",
    "    index=['ID', 'date'],\n",
    "    columns='type',\n",
    "    values='duration',\n",
    "    aggfunc='sum',\n",
    "    fill_value=pd.Timedelta(0)\n",
    ")\n",
    "\n",
    "# Ensure all SLT1–SLT6 columns exist\n",
    "for slt in ['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6']:\n",
    "    if slt not in sleep_summary.columns:\n",
    "        sleep_summary[slt] = pd.Timedelta(0)\n",
    "\n",
    "# Compute total sleep as sum of all SLT durations\n",
    "sleep_summary['total_sleep'] = sleep_summary[['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6']].sum(axis=1)\n",
    "\n",
    "# Convert SLT and total_sleep durations from Timedelta to hours (float)\n",
    "for col in ['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep']:\n",
    "    sleep_summary[col] = sleep_summary[col] / pd.Timedelta(hours=1)\n",
    "\n",
    "# Convert index back to columns\n",
    "sleep_summary = sleep_summary.reset_index()\n",
    "\n",
    "# Overwrite sleep with the summary table\n",
    "sleep = sleep_summary\n",
    "\n",
    "output_path = os.path.join(output_folder, \"sleep_type.csv\")\n",
    "sleep.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep = read_all_data('Sleep', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "sleep = sleep.drop(columns='type')\n",
    "output_path = os.path.join(output_folder, \"sleep_log.csv\")\n",
    "sleep.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ec4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate = read_all_data('HeartRate', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Convert obtained_at to datetime then split into date and time\n",
    "heartrate['obtained_at'] = pd.to_datetime(heartrate['obtained_at'], format='mixed')\n",
    "heartrate['date'] = heartrate['obtained_at'].dt.date.astype(str)\n",
    "heartrate['time'] = heartrate['obtained_at'].dt.time.astype(str)\n",
    "\n",
    "# Drop unneeded columns and reset index\n",
    "heartrate = heartrate.drop(columns=['started_at', 'ended_at', 'obtained_at']).reset_index(drop=True)\n",
    "output_path = os.path.join(output_folder, \"HR.csv\")\n",
    "heartrate.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "HR= heartrate\n",
    "HR = HR.rename(columns={'heart_rate' : 'HR'})\n",
    "#data preprocessing\n",
    "HR['HR'] = pd.to_numeric(HR['HR'])\n",
    "HR_nonzero = HR[HR.HR != 0]\n",
    "\n",
    "#statistical analysis\n",
    "HR_mean = HR_nonzero.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "HR_var = HR_nonzero.groupby(['ID','date'])['HR'].var().reset_index()\n",
    "HR_min = HR_nonzero.groupby(['ID','date'])['HR'].min().reset_index()\n",
    "HR_max = HR_nonzero.groupby(['ID','date'])['HR'].max().reset_index()\n",
    "\n",
    "#calculation of HR_hvar_mean\n",
    "HR['hour'] = pd.to_datetime(HR['time']).dt.hour \n",
    "HR_hvar = HR.groupby(['ID','date','hour'])['HR'].var().reset_index()\n",
    "HR_hvar_mean = HR_hvar.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "\n",
    "#data merge\n",
    "HR_statistics_merged = pd.merge(left=HR, right=HR_var, how=\"outer\", on =['date','ID'], suffixes=['', '_var'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_min, how=\"outer\", on =['date','ID'], suffixes=['', '_min'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_max, how=\"outer\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_hvar_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "\n",
    "#data preprocessing\n",
    "HR_statistics_merged['datetime'] = HR_statistics_merged['date'] + ' ' + HR_statistics_merged['time']\n",
    "HR_statistics_merged.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"hr_stactistics_fixed.csv\")\n",
    "HR_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "HR_date = pd.merge(left=HR_var, right=HR_max, how=\"left\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_mean, how=\"left\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_hvar_mean, how=\"left\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "HR_date.rename(columns = {'HR':'HR_var'}, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"HR_date_fixed.csv\")\n",
    "HR_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_path = os.path.join(output_folder, \"HR.csv\")\n",
    "    HR = pd.read_csv(output_path)\n",
    "    if 'heart_rate' in HR.columns:\n",
    "        HR.rename(columns={'heart_rate': 'HR'}, inplace=True)\n",
    "    HR['HR'] = pd.to_numeric(HR['HR'], errors='coerce')\n",
    "    HR['datetime'] = pd.to_datetime(HR['date'] + ' ' + HR['time'], errors='coerce')\n",
    "    \n",
    "    out = []\n",
    "    for pid, grp in tqdm(HR.groupby('ID'), desc='Processing IDs'):\n",
    "        grp = grp.dropna(subset=['datetime']).set_index('datetime').sort_index()\n",
    "        for day, day_grp in grp.groupby(grp.index.date):\n",
    "            orig_count = day_grp['HR'].dropna().shape[0]\n",
    "            base = pd.to_datetime(f\"{day}\") + pd.to_timedelta(np.arange(1440), unit='m')\n",
    "            df_full = pd.DataFrame(index=base)\n",
    "            if orig_count > 720:\n",
    "                tmp = day_grp[['HR']].resample('1min').mean()\n",
    "                tmp = tmp.reindex(df_full.index)\n",
    "                tmp['HR'] = tmp['HR'].interpolate(method='time', limit=30, limit_direction='both')\n",
    "                df_full['HR'] = tmp['HR']\n",
    "            else:\n",
    "                df_full['HR'] = np.nan\n",
    "            df_full['ID'] = pid\n",
    "            df_full['date'] = pd.to_datetime(day).date()\n",
    "            df_full['time'] = df_full.index.time.astype(str)\n",
    "            out.append(df_full[['HR', 'ID', 'date', 'time']])\n",
    "    \n",
    "    HR_interp = pd.concat(out, ignore_index=True)\n",
    "    output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "    HR_interp.to_csv(output_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "HR_interpolated = pd.read_csv(output_path)\n",
    "\n",
    "HR_interpolated['HR'] = pd.to_numeric(HR_interpolated['HR'])\n",
    "\n",
    "id_list = HR_interpolated['ID'].unique()\n",
    "circadian_data = pd.DataFrame(columns=['ID','date','acr','amp','mesor'])\n",
    "                  \n",
    "for id in id_list:\n",
    "    temp_id = HR_interpolated.loc[(HR_interpolated['ID'] == id)]\n",
    "    temp_id.reset_index(inplace=True)\n",
    "    temp_id = temp_id.drop('index', axis=1)\n",
    "    date_list =temp_id['date'].unique()\n",
    "    for date in date_list:\n",
    "        temp_date = temp_id.loc[(temp_id['date'] == date)]\n",
    "        temp_date.reset_index(inplace=True)\n",
    "        temp_date = temp_date.drop('index', axis=1)\n",
    "        temp_date.reset_index(inplace=True)  \n",
    "        if temp_date.HR.count() > 720:\n",
    "            acr = acrophase(temp_date['index'], temp_date['HR'])\n",
    "            amp = amplitude(temp_date['index'], temp_date['HR'])\n",
    "            mes = mesor(temp_date['index'], temp_date['HR'])\n",
    "            new_row = pd.DataFrame([[id, date, acr, amp, mes]], columns=['ID','date','acr','amp','mesor'])\n",
    "            circadian_data = pd.concat([circadian_data, new_row], ignore_index=True)\n",
    "            print(id, date, acr, amp, mes)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian_data.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian = pd.read_csv(output_path)\n",
    "circadian['date'] = pd.to_datetime(circadian['date'])\n",
    "id_list = circadian['ID'].unique()\n",
    "\n",
    "circadian_delta = pd.DataFrame(columns=['ID', 'date', 'acr', 'amp', 'mesor','acr_delta', 'acr_delta2', 'amp_delta', 'amp_delta2', 'mesor_delta', 'mesor_delta2'])\n",
    "for id in id_list:\n",
    "    circadian_id = circadian.loc[(circadian.ID == id)]\n",
    "    time_per_day = pd.date_range(circadian_id.date.min(), circadian_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    circadian_id = pd.merge(circadian_id, temp, how='right', on='date')\n",
    "    circadian_id.ID = id\n",
    "    circadian_id['acr_delta'] = circadian_id['acr'].diff()\n",
    "    circadian_id['acr_delta2'] = circadian_id['acr'].diff(periods=2)\n",
    "    circadian_id['amp_delta'] = circadian_id['amp'].diff()\n",
    "    circadian_id['amp_delta2'] = circadian_id['amp'].diff(periods=2)\n",
    "    circadian_id['mesor_delta'] = circadian_id['mesor'].diff()\n",
    "    circadian_id['mesor_delta2'] = circadian_id['mesor'].diff(periods=2)\n",
    "    circadian_delta = pd.concat([circadian_delta, circadian_id], axis=0)\n",
    "\n",
    "circadian_delta['date'] = circadian_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "circadian_delta.reset_index(drop=True, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_delta_720.csv\")\n",
    "circadian_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62908999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import (\n",
    "    check_bandpower_value_a,\n",
    "    check_bandpower_value_b,\n",
    "    check_bandpower_value_c,\n",
    "    check_bandpower_value_d,\n",
    ")\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "# 1) 파일 로드 & 타입 변환\n",
    "HR = pd.read_csv(\n",
    "    output_path,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "HR[\"HR\"] = pd.to_numeric(HR[\"HR\"], errors=\"coerce\")\n",
    "\n",
    "# ——— build per-minute DataFrame ———\n",
    "df_per_min = pd.DataFrame(columns=['ID','HR','date'])\n",
    "for id in HR['ID'].unique():\n",
    "    df_id = HR[HR['ID'] == id]\n",
    "    time_per_min = pd.date_range(df_id['date'].min(), df_id['date'].max(), freq='min')\n",
    "    temp = pd.DataFrame({'date': time_per_min})\n",
    "    df_id = pd.merge(df_id, temp, how='right', on='date')\n",
    "    df_id['ID'] = id\n",
    "    df_per_min = pd.concat([df_per_min, df_id], axis=0)\n",
    "\n",
    "df_per_min[\"day\"] = df_per_min[\"date\"].dt.date\n",
    "\n",
    "# 4) 하루 그룹 하나당 밴드파워 계산 함수\n",
    "def compute_bandpower_for_group(group):\n",
    "    (id_, day), sub = group\n",
    "    valid_count = sub['HR'].notna().sum()\n",
    "    if valid_count <= 720:\n",
    "        return None\n",
    "    idx = np.arange(len(sub))\n",
    "    hr  = sub[\"HR\"].to_numpy()\n",
    "    return {\n",
    "        \"ID\":           id_,\n",
    "        \"date\":         pd.Timestamp(day),\n",
    "        \"bandpower_a\":  check_bandpower_value_a(idx, hr),\n",
    "        \"bandpower_b\":  check_bandpower_value_b(idx, hr),\n",
    "        \"bandpower_c\":  check_bandpower_value_c(idx, hr),\n",
    "        \"bandpower_d\":  check_bandpower_value_d(idx, hr),\n",
    "    }\n",
    "\n",
    "groups = df_per_min.groupby([\"ID\",\"day\"], sort=False)\n",
    "\n",
    "# 5) tqdm_joblib 로 진행률 표시하며 병렬 처리\n",
    "with tqdm_joblib(tqdm(total=df_per_min[\"ID\"].nunique(), desc=\"IDs\")):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda g: compute_bandpower_for_group(g))(grp)\n",
    "        for grp in groups\n",
    "    )\n",
    "\n",
    "# 6) None 삭제 & DataFrame 생성\n",
    "records = [r for r in results if r is not None]\n",
    "bandpower_df = pd.DataFrame(records)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"bandpower_720.csv\")\n",
    "bandpower_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combined_nipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
