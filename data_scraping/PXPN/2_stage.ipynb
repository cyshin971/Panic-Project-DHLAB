{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bda39e0",
   "metadata": {},
   "source": [
    "Required Packages:\n",
    "- mne\n",
    "- CosinorPy\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f150e",
   "metadata": {},
   "source": [
    "# 📚 | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2c8259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - (__init__.py) wrapper: matplotlib data path: c:\\Users\\cyshi\\anaconda3\\envs\\dhlab-panic\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG - (__init__.py) wrapper: CONFIGDIR=C:\\Users\\cyshi\\.matplotlib\n",
      "DEBUG - (__init__.py) <module>: interactive is False\n",
      "DEBUG - (__init__.py) <module>: platform is win32\n",
      "DEBUG - (__init__.py) wrapper: CACHEDIR=C:\\Users\\cyshi\\.matplotlib\n",
      "DEBUG - (font_manager.py) _load_fontmanager: Using fontManager instance from C:\\Users\\cyshi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import config as cfg\n",
    "\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from library.path_utils import get_file_path\n",
    "from utils_for_preprocessing import read_all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e89fde",
   "metadata": {},
   "source": [
    "# ⚙️ | Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbf7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PXPN_DIR = \"./raw_data/PXPN\"\n",
    "# 엑셀 파일 경로 (실제 경로로 수정)\n",
    "enroll_file_name = \"1. 픽셀패닉 enroll 정보_250516\"\n",
    "zip_file_name = \"pixelpanic_raw_data.zip\"\n",
    "output_folder_name = \"./_tmp/PXPN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05718a85",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1aa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")\n",
    "zip_path = get_file_path(RAW_PXPN_DIR, f\"{zip_file_name}\")\n",
    "output_folder = output_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c89079c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"2024-11-28 02:12:04\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 97. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m step \u001b[38;5;241m=\u001b[39m read_all_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep\u001b[39m\u001b[38;5;124m'\u001b[39m, BASE_PASSIVE_DIR, exclude_keywords\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresting\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariability\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Convert obtained_at to datetime then split into date and time\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstarted_at\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      8\u001b[0m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cyshi\\anaconda3\\envs\\dhlab-panic\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1112\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32mc:\\Users\\cyshi\\anaconda3\\envs\\dhlab-panic\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    491\u001b[0m     arg,\n\u001b[0;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cyshi\\anaconda3\\envs\\dhlab-panic\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:519\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    509\u001b[0m     arg,\n\u001b[0;32m    510\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[1;32mstrptime.pyx:534\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:355\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"2024-11-28 02:12:04\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 97. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "step = read_all_data('Step', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Convert obtained_at to datetime then split into date and time\n",
    "step['started_at'] = pd.to_datetime(step['started_at'])\n",
    "step['date'] = step['started_at'].dt.date.astype(str)\n",
    "step['time'] = step['started_at'].dt.time.astype(str)\n",
    "\n",
    "\n",
    "# Drop unneeded columns and reset index\n",
    "step = step.drop(columns=['started_at', 'ended_at', 'obtained_at']).reset_index(drop=True)\n",
    "output_path = os.path.join(output_folder, \"step.csv\")\n",
    "step.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data preprocessing\n",
    "step['steps'] = pd.to_numeric(step['steps'])\n",
    "step_nonzero = step[step.steps != 0]\n",
    "\n",
    "#statistical analysis\n",
    "step_mean = step_nonzero.groupby(['ID','date'])['steps'].mean().reset_index().rename(columns={'steps':'step_mean'})\n",
    "step_var = step_nonzero.groupby(['ID','date'])['steps'].var().reset_index().rename(columns={'steps':'step_var'})\n",
    "step_max = step_nonzero.groupby(['ID','date'])['steps'].max().reset_index().rename(columns={'steps':'step_max'})\n",
    "\n",
    "#calculation of step_hvar_mean\n",
    "step_nonzero['hour'] = pd.to_datetime(step_nonzero['time']).dt.hour \n",
    "step_hvar = step_nonzero.groupby(['ID','date','hour'])['steps'].var().reset_index()\n",
    "\n",
    "step_hvar_mean = step_hvar.groupby(['ID','date'])['steps'].mean().reset_index().rename(columns={'steps':'step_hvar_mean'})\n",
    "\n",
    "# create total daily steps\n",
    "daily_steps = step.groupby(['ID','date'])['steps'].sum().reset_index()\n",
    "\n",
    "#data merge\n",
    "step_statistics_merged= pd.merge(left=step, right=step_var, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_max, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_mean, how=\"outer\", on =['date','ID'])\n",
    "step_statistics_merged= pd.merge(left=step_statistics_merged, right=step_hvar_mean, how=\"outer\", on =['date','ID'])\n",
    "\n",
    "#data preprocessing\n",
    "step_statistics_merged['datetime'] = step_statistics_merged['date'] + ' ' + step_statistics_merged['time']\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_stactistics.csv\")\n",
    "step_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "step_date= pd.merge(left=daily_steps, right=step_var, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_max, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_mean, how=\"left\", on =['date','ID'])\n",
    "step_date= pd.merge(left=step_date, right=step_hvar_mean, how=\"left\", on =['date','ID'])\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_date.csv\")\n",
    "step_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step = step_date\n",
    "step['date'] = pd.to_datetime(step['date'])\n",
    "id_list = step['ID'].unique()\n",
    "\n",
    "step_delta = pd.DataFrame(columns=['ID', 'date', 'steps', 'step_max', 'step_mean', 'step_hvar_mean', 'step_delta', 'step_max_delta',\n",
    "                                   'step_mean_delta', 'step_hvar_mean_delta', 'step_delta2', 'step_max_delta2',\n",
    "                                   'step_mean_delta2', 'step_hvar_mean_delta2'])\n",
    "for id in id_list:\n",
    "    step_id = step.loc[(step.ID == id)]\n",
    "    time_per_day = pd.date_range(step_id.date.min(), step_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    step_id = pd.merge(step_id, temp, how='right', on='date')\n",
    "    step_id.ID = id\n",
    "    step_id['step_delta'] = step_id['steps'].diff()\n",
    "    step_id['step_delta2'] = step_id['steps'].diff(periods=2)\n",
    "    step_id['step_max_delta'] = step_id['step_max'].diff()\n",
    "    step_id['step_max_delta2'] = step_id['step_max'].diff(periods=2)\n",
    "    step_id['step_mean_delta'] = step_id['step_mean'].diff()\n",
    "    step_id['step_mean_delta2'] = step_id['step_mean'].diff(periods=2)\n",
    "    step_id['step_hvar_mean_delta'] = step_id['step_hvar_mean'].diff()\n",
    "    step_id['step_hvar_mean_delta2'] = step_id['step_hvar_mean'].diff(periods=2)\n",
    "    step_delta = pd.concat([step_delta, step_id], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "step_delta['date'] = step_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "step_delta.reset_index(drop=True, inplace=True)\n",
    "# Drop rows where steps, step_delta, and step_delta2 are all zero\n",
    "step_delta = step_delta[~((step_delta['steps'] == 0) & (step_delta['step_delta'] == 0) & (step_delta['step_delta2'] == 0))]\n",
    "\n",
    "output_path = os.path.join(output_folder, \"step_delta.csv\")\n",
    "step_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import read_all_data\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "sleep = read_all_data('Sleep', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Map various sleep type labels into standardized SLT codes\n",
    "sleep['type'] = sleep['type'].replace({\n",
    "    'SLT1': 'SLT3',\n",
    "    'SLT0': 'SLT2',\n",
    "    'asleepCore': 'SLT4',\n",
    "    'asleepDeep': 'SLT5',\n",
    "    'asleepREM': 'SLT6',\n",
    "    'asleepUnspecified': 'SLT2',\n",
    "    'awake': 'SLT1'\n",
    "})\n",
    "\n",
    "# Ensure datetime types for start and end\n",
    "sleep['started_at'] = pd.to_datetime(sleep['started_at'])\n",
    "sleep['ended_at'] = pd.to_datetime(sleep['ended_at'])\n",
    "\n",
    "# Calculate session duration\n",
    "sleep['duration'] = sleep['ended_at'] - sleep['started_at']\n",
    "\n",
    "# Extract date for grouping\n",
    "sleep['date'] = sleep['started_at'].dt.date\n",
    "\n",
    "# Pivot to sum durations per SLT type\n",
    "sleep_summary = sleep.pivot_table(\n",
    "    index=['ID', 'date'],\n",
    "    columns='type',\n",
    "    values='duration',\n",
    "    aggfunc='sum',\n",
    "    fill_value=pd.Timedelta(0)\n",
    ")\n",
    "\n",
    "# Ensure all SLT1–SLT6 columns exist\n",
    "for slt in ['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6']:\n",
    "    if slt not in sleep_summary.columns:\n",
    "        sleep_summary[slt] = pd.Timedelta(0)\n",
    "\n",
    "# Compute total sleep as sum of all SLT durations\n",
    "sleep_summary['total_sleep'] = sleep_summary[['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6']].sum(axis=1)\n",
    "\n",
    "# Convert SLT and total_sleep durations from Timedelta to hours (float)\n",
    "for col in ['SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep']:\n",
    "    sleep_summary[col] = sleep_summary[col] / pd.Timedelta(hours=1)\n",
    "\n",
    "# Convert index back to columns\n",
    "sleep_summary = sleep_summary.reset_index()\n",
    "\n",
    "# Overwrite sleep with the summary table\n",
    "sleep = sleep_summary\n",
    "\n",
    "output_path = os.path.join(output_folder, \"sleep_type.csv\")\n",
    "sleep.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import read_all_data\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "sleep = read_all_data('Sleep', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "sleep = sleep.drop(columns='type')\n",
    "output_path = os.path.join(output_folder, \"sleep_log.csv\")\n",
    "sleep.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ec4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_for_preprocessing import read_all_data\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PASSIVE_DIR = zip_path\n",
    "\n",
    "heartrate = read_all_data('HeartRate', BASE_PASSIVE_DIR, exclude_keywords=['resting', 'variability'])\n",
    "\n",
    "# Convert obtained_at to datetime then split into date and time\n",
    "heartrate['obtained_at'] = pd.to_datetime(heartrate['obtained_at'])\n",
    "heartrate['date'] = heartrate['obtained_at'].dt.date.astype(str)\n",
    "heartrate['time'] = heartrate['obtained_at'].dt.time.astype(str)\n",
    "\n",
    "# Drop unneeded columns and reset index\n",
    "heartrate = heartrate.drop(columns=['started_at', 'ended_at', 'obtained_at']).reset_index(drop=True)\n",
    "output_path = os.path.join(output_folder, \"HR.csv\")\n",
    "heartrate.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data\n",
    "HR= heartrate\n",
    "HR = HR.rename(columns={'heart_rate' : 'HR'})\n",
    "#data preprocessing\n",
    "HR['HR'] = pd.to_numeric(HR['HR'])\n",
    "HR_nonzero = HR[HR.HR != 0]\n",
    "\n",
    "#statistical analysis\n",
    "HR_mean = HR_nonzero.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "HR_var = HR_nonzero.groupby(['ID','date'])['HR'].var().reset_index()\n",
    "HR_min = HR_nonzero.groupby(['ID','date'])['HR'].min().reset_index()\n",
    "HR_max = HR_nonzero.groupby(['ID','date'])['HR'].max().reset_index()\n",
    "\n",
    "#calculation of HR_hvar_mean\n",
    "HR['hour'] = pd.to_datetime(HR['time']).dt.hour \n",
    "HR_hvar = HR.groupby(['ID','date','hour'])['HR'].var().reset_index()\n",
    "HR_hvar_mean = HR_hvar.groupby(['ID','date'])['HR'].mean().reset_index()\n",
    "\n",
    "#data merge\n",
    "HR_statistics_merged = pd.merge(left=HR, right=HR_var, how=\"outer\", on =['date','ID'], suffixes=['', '_var'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_min, how=\"outer\", on =['date','ID'], suffixes=['', '_min'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_max, how=\"outer\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_statistics_merged = pd.merge(left=HR_statistics_merged, right=HR_hvar_mean, how=\"outer\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "\n",
    "#data preprocessing\n",
    "HR_statistics_merged['datetime'] = HR_statistics_merged['date'] + ' ' + HR_statistics_merged['time']\n",
    "HR_statistics_merged.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"hr_stactistics_fixed.csv\")\n",
    "HR_statistics_merged.to_csv(output_path, index=False)\n",
    "\n",
    "#data per date\n",
    "HR_date = pd.merge(left=HR_var, right=HR_max, how=\"left\", on =['date','ID'], suffixes=['', '_max'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_mean, how=\"left\", on =['date','ID'], suffixes=['', '_mean'])\n",
    "HR_date = pd.merge(left=HR_date, right=HR_hvar_mean, how=\"left\", on =['date','ID'], suffixes=['', '_hvar_mean'])\n",
    "HR_date.rename(columns = {'HR':'HR_var'}, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"HR_date_fixed.csv\")\n",
    "HR_date.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    output_path = os.path.join(output_folder, \"HR.csv\")\n",
    "    HR = pd.read_csv(output_path)\n",
    "    if 'heart_rate' in HR.columns:\n",
    "        HR.rename(columns={'heart_rate': 'HR'}, inplace=True)\n",
    "    HR['HR'] = pd.to_numeric(HR['HR'], errors='coerce')\n",
    "    HR['datetime'] = pd.to_datetime(HR['date'] + ' ' + HR['time'], errors='coerce')\n",
    "    \n",
    "    out = []\n",
    "    for pid, grp in tqdm(HR.groupby('ID'), desc='Processing IDs'):\n",
    "        grp = grp.dropna(subset=['datetime']).set_index('datetime').sort_index()\n",
    "        for day, day_grp in grp.groupby(grp.index.date):\n",
    "            orig_count = day_grp['HR'].dropna().shape[0]\n",
    "            base = pd.to_datetime(f\"{day}\") + pd.to_timedelta(np.arange(1440), unit='m')\n",
    "            df_full = pd.DataFrame(index=base)\n",
    "            if orig_count > 720:\n",
    "                tmp = day_grp[['HR']].resample('1min').mean()\n",
    "                tmp = tmp.reindex(df_full.index)\n",
    "                tmp['HR'] = tmp['HR'].interpolate(method='time', limit=30, limit_direction='both')\n",
    "                df_full['HR'] = tmp['HR']\n",
    "            else:\n",
    "                df_full['HR'] = np.nan\n",
    "            df_full['ID'] = pid\n",
    "            df_full['date'] = pd.to_datetime(day).date()\n",
    "            df_full['time'] = df_full.index.time.astype(str)\n",
    "            out.append(df_full[['HR', 'ID', 'date', 'time']])\n",
    "    \n",
    "    HR_interp = pd.concat(out, ignore_index=True)\n",
    "    output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "    HR_interp.to_csv(output_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils_for_preprocessing import mesor, amplitude, acrophase\n",
    "output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "HR_interpolated = pd.read_csv(output_path)\n",
    "\n",
    "HR_interpolated['HR'] = pd.to_numeric(HR_interpolated['HR'])\n",
    "\n",
    "id_list = HR_interpolated['ID'].unique()\n",
    "circadian_data = pd.DataFrame(columns=['ID','date','acr','amp','mesor'])\n",
    "                  \n",
    "for id in id_list:\n",
    "    temp_id = HR_interpolated.loc[(HR_interpolated['ID'] == id)]\n",
    "    temp_id.reset_index(inplace=True)\n",
    "    temp_id = temp_id.drop('index', axis=1)\n",
    "    date_list =temp_id['date'].unique()\n",
    "    for date in date_list:\n",
    "        temp_date = temp_id.loc[(temp_id['date'] == date)]\n",
    "        temp_date.reset_index(inplace=True)\n",
    "        temp_date = temp_date.drop('index', axis=1)\n",
    "        temp_date.reset_index(inplace=True)  \n",
    "        if temp_date.HR.count() > 720:\n",
    "            acr = acrophase(temp_date['index'], temp_date['HR'])\n",
    "            amp = amplitude(temp_date['index'], temp_date['HR'])\n",
    "            mes = mesor(temp_date['index'], temp_date['HR'])\n",
    "            new_row = pd.DataFrame([[id, date, acr, amp, mes]], columns=['ID','date','acr','amp','mesor'])\n",
    "            circadian_data = pd.concat([circadian_data, new_row], ignore_index=True)\n",
    "            print(id, date, acr, amp, mes)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian_data.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = os.path.join(output_folder, \"circadian_parameter_720.csv\")\n",
    "circadian = pd.read_csv(output_path)\n",
    "circadian['date'] = pd.to_datetime(circadian['date'])\n",
    "id_list = circadian['ID'].unique()\n",
    "\n",
    "circadian_delta = pd.DataFrame(columns=['ID', 'date', 'acr', 'amp', 'mesor','acr_delta', 'acr_delta2', 'amp_delta', 'amp_delta2', 'mesor_delta', 'mesor_delta2'])\n",
    "for id in id_list:\n",
    "    circadian_id = circadian.loc[(circadian.ID == id)]\n",
    "    time_per_day = pd.date_range(circadian_id.date.min(), circadian_id.date.max(), freq='D')\n",
    "    temp = pd.DataFrame()\n",
    "    temp['date'] = time_per_day\n",
    "    circadian_id = pd.merge(circadian_id, temp, how='right', on='date')\n",
    "    circadian_id.ID = id\n",
    "    circadian_id['acr_delta'] = circadian_id['acr'].diff()\n",
    "    circadian_id['acr_delta2'] = circadian_id['acr'].diff(periods=2)\n",
    "    circadian_id['amp_delta'] = circadian_id['amp'].diff()\n",
    "    circadian_id['amp_delta2'] = circadian_id['amp'].diff(periods=2)\n",
    "    circadian_id['mesor_delta'] = circadian_id['mesor'].diff()\n",
    "    circadian_id['mesor_delta2'] = circadian_id['mesor'].diff(periods=2)\n",
    "    circadian_delta = pd.concat([circadian_delta, circadian_id], axis=0)\n",
    "\n",
    "circadian_delta['date'] = circadian_delta['date'].dt.strftime('%Y-%m-%d')\n",
    "circadian_delta.reset_index(drop=True, inplace=True)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"circadian_delta_720.csv\")\n",
    "circadian_delta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62908999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils_for_preprocessing import (\n",
    "    check_bandpower_value_a,\n",
    "    check_bandpower_value_b,\n",
    "    check_bandpower_value_c,\n",
    "    check_bandpower_value_d,\n",
    ")\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "output_path = os.path.join(output_folder, \"HR_interpolated_720.csv\")\n",
    "# 1) 파일 로드 & 타입 변환\n",
    "HR = pd.read_csv(\n",
    "    output_path,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "HR[\"HR\"] = pd.to_numeric(HR[\"HR\"], errors=\"coerce\")\n",
    "\n",
    "# ——— build per-minute DataFrame ———\n",
    "df_per_min = pd.DataFrame(columns=['ID','HR','date'])\n",
    "for id in HR['ID'].unique():\n",
    "    df_id = HR[HR['ID'] == id]\n",
    "    time_per_min = pd.date_range(df_id['date'].min(), df_id['date'].max(), freq='min')\n",
    "    temp = pd.DataFrame({'date': time_per_min})\n",
    "    df_id = pd.merge(df_id, temp, how='right', on='date')\n",
    "    df_id['ID'] = id\n",
    "    df_per_min = pd.concat([df_per_min, df_id], axis=0)\n",
    "\n",
    "df_per_min[\"day\"] = df_per_min[\"date\"].dt.date\n",
    "\n",
    "# 4) 하루 그룹 하나당 밴드파워 계산 함수\n",
    "def compute_bandpower_for_group(group):\n",
    "    (id_, day), sub = group\n",
    "    valid_count = sub['HR'].notna().sum()\n",
    "    if valid_count <= 720:\n",
    "        return None\n",
    "    idx = np.arange(len(sub))\n",
    "    hr  = sub[\"HR\"].to_numpy()\n",
    "    return {\n",
    "        \"ID\":           id_,\n",
    "        \"date\":         pd.Timestamp(day),\n",
    "        \"bandpower_a\":  check_bandpower_value_a(idx, hr),\n",
    "        \"bandpower_b\":  check_bandpower_value_b(idx, hr),\n",
    "        \"bandpower_c\":  check_bandpower_value_c(idx, hr),\n",
    "        \"bandpower_d\":  check_bandpower_value_d(idx, hr),\n",
    "    }\n",
    "\n",
    "groups = df_per_min.groupby([\"ID\",\"day\"], sort=False)\n",
    "\n",
    "# 5) tqdm_joblib 로 진행률 표시하며 병렬 처리\n",
    "with tqdm_joblib(tqdm(total=df_per_min[\"ID\"].nunique(), desc=\"IDs\")):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda g: compute_bandpower_for_group(g))(grp)\n",
    "        for grp in groups\n",
    "    )\n",
    "\n",
    "# 6) None 삭제 & DataFrame 생성\n",
    "records = [r for r in results if r is not None]\n",
    "bandpower_df = pd.DataFrame(records)\n",
    "\n",
    "output_path = os.path.join(output_folder, \"bandpower_720.csv\")\n",
    "bandpower_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24633c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20f631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b35e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3521422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhlab-panic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
