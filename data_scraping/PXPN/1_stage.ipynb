{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Packages:\n",
    "- pandas  : 1.5.3\n",
    "- openpyxl: 3.1.5\n",
    "- xlrd    : 2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlrd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "from library.path_utils import get_file_path, to_absolute_path\n",
    "\n",
    "print(\"pandas  :\", pd.__version__)\n",
    "print(\"openpyxl:\", openpyxl.__version__)\n",
    "print(\"xlrd    :\", xlrd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ | Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PXPN_DIR = \"./raw_data/PXPN\"\n",
    "\n",
    "enroll_file_name = \"pxpn_enroll_info\"\n",
    "zip_file_name = \"pixelpanic_raw_data.zip\"\n",
    "output_folder_name = \"./_tmp/PXPN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.xlsx\")\n",
    "zip_path = get_file_path(RAW_PXPN_DIR, f\"{zip_file_name}\")\n",
    "output_folder = to_absolute_path(output_folder_name)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "csv_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")\n",
    "\n",
    "# ì—‘ì…€ ì½ê³  csvë¡œ ì €ì¥\n",
    "df = pd.read_excel(enroll_path)\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "outer_zip_path = zip_path\n",
    "\n",
    "# 1) ë°”ê¹¥ ZIP ì—´ê¸°\n",
    "with zipfile.ZipFile(outer_zip_path, 'r') as outer_zip:\n",
    "    # 2) ActiveData í´ë” ì•ˆì˜ inner-zip ê²½ë¡œ ìˆ˜ì§‘\n",
    "    active_data_zips = {}\n",
    "    for member in outer_zip.namelist():\n",
    "        # ê²½ë¡œì— 'ActiveData/' í¬í•¨í•˜ê³ , '_ActiveData.zip' ìœ¼ë¡œ ëë‚˜ëŠ” íŒŒì¼ë§Œ ì„ íƒ\n",
    "        if 'ActiveData/' in member and member.endswith('_ActiveData.zip'):\n",
    "            # íŒŒì¼ëª…ì—ì„œ patient_code(ì˜ˆ: 'ABC123') ì¶”ì¶œ\n",
    "            base = os.path.basename(member)                # e.g. 'ABC123_ActiveData.zip'\n",
    "            patient_code = base.replace('_ActiveData.zip', '')\n",
    "            active_data_zips[patient_code] = member\n",
    "\n",
    "    # 3) ëª¨ì€ ê²½ë¡œë“¤ì„ ìˆœíšŒí•˜ë©° ë‚´ë¶€ ZIP ì—´ê¸°\n",
    "    for patient_code, inner_path in active_data_zips.items():\n",
    "        # outer_zip.read()ë¡œ ë°”ì´íŠ¸ ì½ê¸° â†’ BytesIOë¡œ ê°ì‹¸ê¸°\n",
    "        inner_bytes = BytesIO(outer_zip.read(inner_path))\n",
    "        with zipfile.ZipFile(inner_bytes, 'r') as inner_zip:\n",
    "            # ì›í•˜ëŠ” CSV íŒŒì¼ëª…\n",
    "            survey_csv = f'{patient_code}_SurveyResponse.csv'\n",
    "            if survey_csv not in inner_zip.namelist():\n",
    "                print(f'[ê²½ê³ ] {patient_code} ë‚´ë¶€ì— {survey_csv} ì—†ìŒ')\n",
    "                continue\n",
    "\n",
    "            # CSV ì—´ì–´ì„œ DataFrameìœ¼ë¡œ ì½ê¸°\n",
    "            with inner_zip.open(survey_csv) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                print(f'{patient_code}: ì½ì€ í–‰ ìˆ˜ =', len(df))\n",
    "                # TODO: df ì²˜ë¦¬ ë¡œì§ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "                 \n",
    "\n",
    "\n",
    "# 1) ì„¤ë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "top_5 = [\n",
    "    'íŠ¹ì„± ë¶ˆì•ˆ ì„¤ë¬¸', 'í•œêµ­í˜• íšŒë³µíƒ„ë ¥ì„± ì§€ìˆ˜', 'í•œêµ­ì–´íŒ ì•„ì¹¨í˜•-ì €ë…í˜• ì„¤ë¬¸ì§€',\n",
    "    'í•œê¸€íŒ ìƒë¬¼í•™ì  ë¦¬ë“¬ í‰ê°€ ì„¤ë¬¸ì§€', 'ìœ ë…„ê¸° ì™¸ìƒ ì²™ë„', 'í•œêµ­í˜• ê¸°ë¶„ì¥ì•  ì„¤ë¬¸ì§€',\n",
    "    'ê´‘ì¥ê³µí¬ ì¸ì§€ ì„¤ë¬¸ì§€', 'ì•Œë°”ë‹ˆ ê³µí™©-ê³µí¬ ì§ˆë¬¸ì§€', 'ì‹ ì²´ê°ê° ì„¤ë¬¸ì§€',\n",
    "    'í•œê¸€íŒ ë²”ë¶ˆì•ˆ ì¥ì• ', 'í•œêµ­ì–´íŒ ìš°ìš¸ì¦ ì„ ë³„ë„êµ¬'\n",
    "]\n",
    "result = pd.DataFrame(columns=['patient_code', 'ë‚ ì§œ'])\n",
    "# 2) ê²°ê³¼ DataFrame ì´ˆê¸°í™”\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "outer_zip_path = zip_path\n",
    "\n",
    "# 1) ë°”ê¹¥ ZIP ì—´ê¸°\n",
    "with zipfile.ZipFile(outer_zip_path, 'r') as outer_zip:\n",
    "    # 2) ActiveData í´ë” ì•ˆì˜ inner-zip ê²½ë¡œ ìˆ˜ì§‘\n",
    "    active_data_zips = {}\n",
    "    for member in outer_zip.namelist():\n",
    "        # ê²½ë¡œì— 'ActiveData/' í¬í•¨í•˜ê³ , '_ActiveData.zip' ìœ¼ë¡œ ëë‚˜ëŠ” íŒŒì¼ë§Œ ì„ íƒ\n",
    "        if 'ActiveData/' in member and member.endswith('_ActiveData.zip'):\n",
    "            # íŒŒì¼ëª…ì—ì„œ patient_code(ì˜ˆ: 'ABC123') ì¶”ì¶œ\n",
    "            base = os.path.basename(member)                # e.g. 'ABC123_ActiveData.zip'\n",
    "            patient_code = base.replace('_ActiveData.zip', '')\n",
    "            active_data_zips[patient_code] = member\n",
    "\n",
    "    # 3) ëª¨ì€ ê²½ë¡œë“¤ì„ ìˆœíšŒí•˜ë©° ë‚´ë¶€ ZIP ì—´ê¸°\n",
    "    for patient_code, inner_path in active_data_zips.items():\n",
    "        # outer_zip.read()ë¡œ ë°”ì´íŠ¸ ì½ê¸° â†’ BytesIOë¡œ ê°ì‹¸ê¸°\n",
    "        inner_bytes = BytesIO(outer_zip.read(inner_path))\n",
    "        with zipfile.ZipFile(inner_bytes, 'r') as inner_zip:\n",
    "            # ì›í•˜ëŠ” CSV íŒŒì¼ëª…\n",
    "            survey_csv = f'{patient_code}_SurveyResponse.csv'\n",
    "            if survey_csv not in inner_zip.namelist():\n",
    "                print(f'[ê²½ê³ ] {patient_code} ë‚´ë¶€ì— {survey_csv} ì—†ìŒ')\n",
    "                continue\n",
    "\n",
    "            # CSV ì—´ì–´ì„œ DataFrameìœ¼ë¡œ ì½ê¸°\n",
    "            with inner_zip.open(survey_csv) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                # TODO: df ì²˜ë¦¬ ë¡œì§ ì¶”ê°€\n",
    "            # ì‘ì„±ì¼ ì»¬ëŸ¼ì—ì„œ ë‚ ì§œë§Œ ë½‘ê¸°\n",
    "            date_value = pd.to_datetime(df['ì‘ì„±ì¼'].iloc[0]).date()\n",
    "\n",
    "            # ê²°ê³¼ DataFrameì— (patient_code, ë‚ ì§œ) í–‰ ì¶”ê°€\n",
    "            mask = (result['patient_code'] == patient_code) & (result['ë‚ ì§œ'] == date_value)\n",
    "            if not mask.any():\n",
    "                result = pd.concat([\n",
    "                    result,\n",
    "                    pd.DataFrame([{'patient_code': patient_code, 'ë‚ ì§œ': date_value}])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            # 5) ì„¤ë¬¸ë³„ ì ìˆ˜ ì¶”ì¶œ ë° ê²°ê³¼ì— ì‚½ì…\n",
    "            for survey in top_5:\n",
    "                sub = df[df['ì„¤ë¬¸ëª…'] == survey].reset_index(drop=True)\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "\n",
    "                # ì‹¤ì œ ì ìˆ˜(real_score) ë¦¬ìŠ¤íŠ¸\n",
    "                scores = []\n",
    "                for _, row in sub.iterrows():\n",
    "                    if row.get('ì—­ì±„ì ì¸ ê²½ìš° ì—­ì±„ì  ì ìˆ˜', '-') != '-':\n",
    "                        scores.append(float(row['ì—­ì±„ì ì¸ ê²½ìš° ì—­ì±„ì  ì ìˆ˜']))\n",
    "                    else:\n",
    "                        v = row.get('ì ìˆ˜')\n",
    "                        scores.append(float(v) if pd.notna(v) else '***')\n",
    "\n",
    "                # ì„¤ë¬¸ëª… â†’ ì»¬ëŸ¼ ì ‘ë‘ì‚¬ ë§¤í•‘\n",
    "                prefix_map = {\n",
    "                    'íŠ¹ì„± ë¶ˆì•ˆ ì„¤ë¬¸': 'STAI_X2',\n",
    "                    'í•œêµ­í˜• íšŒë³µíƒ„ë ¥ì„± ì§€ìˆ˜': 'KRQ',\n",
    "                    'í•œêµ­ì–´íŒ ì•„ì¹¨í˜•-ì €ë…í˜• ì„¤ë¬¸ì§€': 'CSM',\n",
    "                    'í•œê¸€íŒ ìƒë¬¼í•™ì  ë¦¬ë“¬ í‰ê°€ ì„¤ë¬¸ì§€': 'BRIAN',\n",
    "                    'í•œêµ­í˜• ê¸°ë¶„ì¥ì•  ì„¤ë¬¸ì§€': 'MDQ',\n",
    "                    'ê´‘ì¥ê³µí¬ ì¸ì§€ ì„¤ë¬¸ì§€': 'ACQ',\n",
    "                    'ì‹ ì²´ê°ê° ì„¤ë¬¸ì§€': 'BSQ',\n",
    "                    'í•œê¸€íŒ ë²”ë¶ˆì•ˆ ì¥ì• ': 'GAD',\n",
    "                    'í•œêµ­ì–´íŒ ìš°ìš¸ì¦ ì„ ë³„ë„êµ¬': 'PHQ',\n",
    "                    # ìœ ë…„ê¸° ì™¸ìƒ ì²™ë„ â†’ CTQ, ì•Œë°”ë‹ˆ ê³µí™©-ê³µí¬ â†’ APPQ (ì£¼ì œë³„)\n",
    "                    'ìœ ë…„ê¸° ì™¸ìƒ ì²™ë„': 'CTQ',\n",
    "                    'ì•Œë°”ë‹ˆ ê³µí™©-ê³µí¬ ì§ˆë¬¸ì§€': 'APPQ'\n",
    "                }\n",
    "                prefix = prefix_map[survey]\n",
    "\n",
    "                # ì£¼ì œë³„ ë¶„ë¦¬ í•„ìš”í•œ ì„¤ë¬¸\n",
    "                if survey in ['ìœ ë…„ê¸° ì™¸ìƒ ì²™ë„', 'ì•Œë°”ë‹ˆ ê³µí™©-ê³µí¬ ì§ˆë¬¸ì§€']:\n",
    "                    sub['real_score'] = scores\n",
    "                    topics = sorted(sub['ì£¼ì œ'].dropna().unique())\n",
    "                    for ti, topic in enumerate(topics, start=1):\n",
    "                        tdf = sub[sub['ì£¼ì œ'] == topic].reset_index(drop=True)\n",
    "                        for qi, sc in enumerate(tdf['real_score'], start=1):\n",
    "                            col = f\"{prefix}-{ti}-{qi}\"\n",
    "                            result.loc[\n",
    "                                (result['patient_code'] == patient_code) &\n",
    "                                (result['ë‚ ì§œ'] == date_value),\n",
    "                                col\n",
    "                            ] = sc\n",
    "                else:\n",
    "                    for idx, sc in enumerate(scores, start=1):\n",
    "                        col = f\"{prefix}-{idx}\"\n",
    "                        result.loc[\n",
    "                            (result['patient_code'] == patient_code) &\n",
    "                            (result['ë‚ ì§œ'] == date_value),\n",
    "                            col\n",
    "                        ] = sc\n",
    "\n",
    "# 6) ì»¬ëŸ¼ ìˆœì„œ ì¬ë°°ì—´\n",
    "cols = ['patient_code', 'ë‚ ì§œ'] + [c for c in result.columns if c not in ['patient_code', 'ë‚ ì§œ']]\n",
    "result = result[cols]\n",
    "\n",
    "# 7) ë¶ˆí•„ìš” ì»¬ëŸ¼ ì‚­ì œ (ì˜ˆ: ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ MDQ-14,15 ë“±)\n",
    "result = result.drop(columns=['MDQ-14', 'MDQ-15', 'PHQ-10'], errors='ignore')\n",
    "\n",
    "# 8) ì €ì¥\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, \"questionnaire.csv\")\n",
    "result.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©í•  ì ‘ë‘ì–´ ëª©ë¡\n",
    "prefixes = [\"PHQ\", \"STAI_X2\", \"CSM\", \"CTQ-1\", \"CTQ-2\", \"CTQ-3\", \"CTQ-4\", \"CTQ-5\", \"KRQ\", \"MDQ\", \"ACQ\", \"APPQ-1\", \"APPQ-2\", \"APPQ-3\", \"BSQ\", \"GAD\", \"BRIAN\"]\n",
    "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„\n",
    "aggregated_df = result[['patient_code', 'ë‚ ì§œ']].copy()\n",
    "\n",
    "for prefix in prefixes:\n",
    "    # í•´ë‹¹ ì ‘ë‘ì–´ë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    matched_cols = [col for col in result.columns if col.startswith(prefix)]\n",
    "    \n",
    "    # ê°’ í•©ì‚°í•´ì„œ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€\n",
    "    aggregated_df[f'{prefix.rstrip(\"-\")}'] = result[matched_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1)\n",
    "\n",
    "display(aggregated_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—‘ì…€ íŒŒì¼ ê²½ë¡œ\n",
    "file_path = enroll_path\n",
    "\n",
    "# ì—‘ì…€ ì‹œíŠ¸ ì½ê¸°\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n",
    "df = df.rename(columns={'íšŒì›ì½”ë“œ': 'patient_code', '2. ì„±ë³„': 'gender'})\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "df = df[['patient_code', 'ì—°êµ¬ì‹œì‘ì¼', 'ì—°êµ¬ì¢…ë£Œì¼', 'gender']]\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "df['ì—°êµ¬ì‹œì‘ì¼'] = pd.to_datetime(df['ì—°êµ¬ì‹œì‘ì¼'], errors='coerce')\n",
    "df['ì—°êµ¬ì¢…ë£Œì¼'] = pd.to_datetime(df['ì—°êµ¬ì¢…ë£Œì¼'], errors='coerce')\n",
    "\n",
    "# ê° í™˜ìì— ëŒ€í•´ ë‚ ì§œ ìƒì„±\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notnull(row['ì—°êµ¬ì‹œì‘ì¼']) and pd.notnull(row['ì—°êµ¬ì¢…ë£Œì¼']):\n",
    "        date_range = pd.date_range(start=row['ì—°êµ¬ì‹œì‘ì¼'], end=row['ì—°êµ¬ì¢…ë£Œì¼'])\n",
    "        for date in date_range:\n",
    "            expanded_rows.append({'patient_code': row['patient_code'], 'ë‚ ì§œ': date, 'gender': row['gender']})\n",
    "\n",
    "# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df = expanded_df.replace({'ë‚¨': '0', 'ì—¬': '1'})\n",
    "print(expanded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—‘ì…€ íŒŒì¼ ê²½ë¡œ\n",
    "file_path = enroll_path\n",
    "\n",
    "# CSV ì½ê¸°\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ (ìƒë…„ì›”ì¼ ì»¬ëŸ¼ë„ ì¶”ê°€)\n",
    "df = df.rename(columns={\n",
    "    'íšŒì›ì½”ë“œ': 'patient_code',\n",
    "    '2. ì„±ë³„': 'gender'  # ìƒë…„ì›”ì¼ ì»¬ëŸ¼ ì´ë¦„ì´ ì‹¤ì œ ë‹¤ë¥´ë©´ ì´ ë¶€ë¶„ ìˆ˜ì • í•„ìš”\n",
    "})\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "df = df[['patient_code', 'ì—°êµ¬ì‹œì‘ì¼', 'ì—°êµ¬ì¢…ë£Œì¼', 'gender', '3. ìƒë…„ì›”ì¼']]\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "df['ì—°êµ¬ì‹œì‘ì¼'] = pd.to_datetime(df['ì—°êµ¬ì‹œì‘ì¼'], errors='coerce')\n",
    "df['ì—°êµ¬ì¢…ë£Œì¼'] = pd.to_datetime(df['ì—°êµ¬ì¢…ë£Œì¼'], errors='coerce')\n",
    "df['3. ìƒë…„ì›”ì¼'] = pd.to_datetime(df['3. ìƒë…„ì›”ì¼'], errors='coerce')\n",
    "\n",
    "\n",
    "# ê° í™˜ìì— ëŒ€í•´ ë‚ ì§œ ìƒì„±\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notnull(row['ì—°êµ¬ì‹œì‘ì¼']) and pd.notnull(row['ì—°êµ¬ì¢…ë£Œì¼']):\n",
    "        date_range = pd.date_range(start=row['ì—°êµ¬ì‹œì‘ì¼'], end=row['ì—°êµ¬ì¢…ë£Œì¼'])\n",
    "        for date in date_range:\n",
    "            expanded_rows.append({\n",
    "                'patient_code': row['patient_code'],\n",
    "                'ë‚ ì§œ': date,\n",
    "                'gender': '0' if row['gender'] == 'ë‚¨' else '1'\n",
    "            })\n",
    "\n",
    "# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['ë‚ ì§œ']    = pd.to_datetime(expanded_df['ë‚ ì§œ'])\n",
    "aggregated_df['ë‚ ì§œ'] = pd.to_datetime(aggregated_df['ë‚ ì§œ'])\n",
    "expanded_answer = pd.merge(expanded_df, aggregated_df, on=['patient_code', 'ë‚ ì§œ'], how='outer')\n",
    "expanded_answer = expanded_answer.rename(columns={'patient_code': 'ID', 'ë‚ ì§œ': 'date', 'GAD': 'GAD_7', 'CTQ-1': 'CTQ_1', 'CTQ-2': 'CTQ_2', 'CTQ-3': 'CTQ_3', 'CTQ-4': 'CTQ_4', 'CTQ-5': 'CTQ_5', 'APPQ-1': 'APPQ_1', 'APPQ-2': 'APPQ_2', 'APPQ-3': 'APPQ_3', 'PHQ': 'PHQ_9'})\n",
    "output_path = os.path.join(output_folder, \"questionnaire_test.csv\")\n",
    "expanded_answer.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. ë‚ ì§œ í˜•ì‹ í†µì¼\n",
    "expanded_answer['date'] = pd.to_datetime(expanded_answer['date'])\n",
    "# 0) ê²°ê³¼ DataFrame ì¤€ë¹„\n",
    "PXPN_panic_dates = pd.DataFrame(columns=['ID', 'date'])\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    for member in outer_zip.namelist():\n",
    "        # â¤ ëì´ _ActiveData.zip ì´ê³ , ê²½ë¡œ ì–´ë”˜ê°€ì— ActiveData/ í¬í•¨, ìˆ¨ê¹€íŒŒì¼ ì œì™¸\n",
    "        if (\n",
    "            member.endswith('_ActiveData.zip') and\n",
    "            'ActiveData/' in member and\n",
    "            not any(x in member for x in ['__MACOSX', '/._', '.DS_Store'])\n",
    "        ):\n",
    "            # patient ID ì¶”ì¶œ\n",
    "            pid = os.path.basename(member).replace('_ActiveData.zip', '')\n",
    "\n",
    "            # ë‚´ë¶€ ZIP ì—´ê¸°\n",
    "            buf = BytesIO(outer_zip.read(member))\n",
    "            buf.seek(0)\n",
    "            if not zipfile.is_zipfile(buf):\n",
    "                print(f\"âŒ {member} ëŠ” ZIPì´ ì•„ë‹™ë‹ˆë‹¤.\")\n",
    "                continue\n",
    "            buf.seek(0)\n",
    "\n",
    "            with zipfile.ZipFile(buf, 'r') as inner_zip:\n",
    "                # Panic.csv ì°¾ê¸°\n",
    "                panic_files = [f for f in inner_zip.namelist() if f.endswith('Panic.csv')]\n",
    "                if not panic_files:\n",
    "                    print(f\"âš ï¸ {pid}: Panic.csv ì—†ìŒ\")\n",
    "                    continue\n",
    "\n",
    "                # (ì—¬ëŸ¬ ê°œ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ëª¨ë‘ ì²˜ë¦¬)\n",
    "                for panic_fname in panic_files:\n",
    "                    with inner_zip.open(panic_fname) as f:\n",
    "                        df_panic = pd.read_csv(f)\n",
    "                        if 'ì‘ì„±ì¼' not in df_panic.columns:\n",
    "                            print(f\"âš ï¸ {pid}: ì‘ì„±ì¼ ì»¬ëŸ¼ ì—†ìŒ in {panic_fname}\")\n",
    "                            continue\n",
    "\n",
    "                        # ë‚ ì§œë³„ë¡œ í•œ í–‰ì”© ì¶”ê°€\n",
    "                        for d in pd.to_datetime(df_panic['ì‘ì„±ì¼'], errors='coerce').dt.date.unique():\n",
    "                            PXPN_panic_dates = pd.concat([\n",
    "                                PXPN_panic_dates,\n",
    "                                pd.DataFrame({'ID': [pid], 'date': [d]})\n",
    "                            ], ignore_index=True)\n",
    "\n",
    "# 2) panic ê°’, í¬ë§· ì •ë¦¬\n",
    "PXPN_panic_dates['panic'] = 2\n",
    "PXPN_panic_dates['date']  = pd.to_datetime(PXPN_panic_dates['date'])\\\n",
    "                                .dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3) expanded_answer ìª½ë„ date ì»¬ëŸ¼ ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ\n",
    "expanded_answer['date'] = pd.to_datetime(expanded_answer['date'], errors='coerce')\\\n",
    "                               .dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3. outer merge\n",
    "merged = pd.merge(\n",
    "    PXPN_panic_dates,\n",
    "    expanded_answer,\n",
    "    on=['ID', 'date'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# 4. ìš°ì„ ìˆœìœ„ panic ê°’ ìœ ì§€\n",
    "merged = (\n",
    "    merged\n",
    "    .sort_values(['ID', 'date', 'panic'], ascending=[True, True, False])\n",
    "    .drop_duplicates(subset=['ID', 'date'], keep='first')\n",
    ")\n",
    "\n",
    "# 5. ì „ë‚  panic=1 ì ìš©\n",
    "df = merged.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "for _, row in df[df['panic'] == 2].iterrows():\n",
    "    pid = row['ID']\n",
    "    curr_date = row['date']\n",
    "    prev_date = curr_date - pd.Timedelta(days=1)\n",
    "\n",
    "    # ì—°ì†ëœ 2 ì œê±°\n",
    "    while ((df['ID'] == pid) & (df['date'] == prev_date) & (df['panic'] == 2)).any():\n",
    "        prev_date -= pd.Timedelta(days=1)\n",
    "\n",
    "    mask = (df['ID'] == pid) & (df['date'] == prev_date)\n",
    "    if mask.any():\n",
    "        df.loc[mask & (df['panic'].fillna(0) < 1), 'panic'] = 1\n",
    "\n",
    "df = df.sort_values(['ID', 'date']).reset_index(drop=True)\n",
    "df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "df['panic'] = df['panic'].fillna(0)\n",
    "\n",
    "# 6. Enroll ë³‘í•© (age)\n",
    "enroll_df = pd.read_csv(enroll_path, encoding='utf-8')\n",
    "enroll_df = enroll_df.rename(columns={\n",
    "    'íšŒì›ì½”ë“œ': 'patient_code',\n",
    "    '3. ìƒë…„ì›”ì¼': 'birthdate',\n",
    "    'ì—°êµ¬ì¢…ë£Œì¼': 'end_date'\n",
    "})\n",
    "enroll_df['birthdate'] = pd.to_datetime(enroll_df['birthdate'], errors='coerce')\n",
    "enroll_df['end_date'] = pd.to_datetime(enroll_df['end_date'], errors='coerce')\n",
    "enroll_df['age'] = enroll_df['end_date'].dt.year - enroll_df['birthdate'].dt.year\n",
    "\n",
    "age_df = enroll_df[['patient_code', 'age']].drop_duplicates().rename(columns={'patient_code': 'ID'})\n",
    "final_df = df.merge(age_df, on='ID', how='left')\n",
    "\n",
    "# 7. ì €ì¥\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_date.csv\")\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¼ ê²½ë¡œ\n",
    "csv_path = output_path\n",
    "zip_path = zip_path\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df[\"ID\"].str.startswith(\"PXPN\")].copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# ì»¬ëŸ¼ ì´ˆê¸°í™”\n",
    "for col in ['marriage', 'job', 'alcohol', 'coffee', 'smoking', 'menstruation', 'exercise',\n",
    "            'smkHx', 'drinkHx', 'suicideHx', 'suicide_need']:\n",
    "    df[col] = np.nan\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    all_files = outer_zip.namelist()\n",
    "\n",
    "    # ActiveData ZIP íŒŒì¼ë§Œ ê³¨ë¼ì„œ {pid: ê²½ë¡œ} ë§¤í•‘\n",
    "    active_data_zips = {\n",
    "        os.path.basename(f).replace('_ActiveData.zip', ''): f\n",
    "        for f in all_files\n",
    "        if f.startswith(\"pixelpanic_raw_data/ActiveData/\") and f.endswith(\"_ActiveData.zip\")\n",
    "           and \"__MACOSX\" not in f and \"/._\" not in f and \".DS_Store\" not in f\n",
    "    }\n",
    "\n",
    "    for pid in df['ID'].unique():\n",
    "        pid = str(pid).strip()\n",
    "        if pid not in active_data_zips:\n",
    "            continue\n",
    "\n",
    "        # inner ZIP ë°”ì´íŠ¸ë¡œ ì½ì–´ì„œ ì—´ê¸°\n",
    "        inner_path = active_data_zips[pid]\n",
    "        buf = BytesIO(outer_zip.read(inner_path))\n",
    "        with zipfile.ZipFile(buf, 'r') as inner_zip:\n",
    "\n",
    "            # Sociodemographic.csv ì²˜ë¦¬\n",
    "            soc_file = f\"{pid}_Sociodemographic.csv\"\n",
    "            if soc_file in inner_zip.namelist():\n",
    "                # header=None, index_col=0 ë¡œ ì½ê³  transpose\n",
    "                soc_df = pd.read_csv(inner_zip.open(soc_file), header=None, index_col=0).T\n",
    "                if 'ê²°í˜¼' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'marriage'] = 1 if soc['ê²°í˜¼'].values[0] == 'ê¸°í˜¼' else 0\n",
    "                if 'í˜„ì¬ ì§ì—… ìœ ë¬´' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'job'] = 1 if soc['í˜„ì¬ ì§ì—… ìœ ë¬´'].values[0] == 'Y' else 0\n",
    "                if 'ê³¼ê±° í¡ì—° ì—¬ë¶€' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'smkHx'] = 1 if soc['ê³¼ê±° í¡ì—° ì—¬ë¶€'].values[0] == 'Y' else 0\n",
    "                if 'ì§€ê¸ˆê¹Œì§€ ìŒì£¼ ì—¬ë¶€' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'drinkHx'] = 1 if soc['ì§€ê¸ˆê¹Œì§€ ìŒì£¼ ì—¬ë¶€'].values[0] == 'Y' else 0\n",
    "                if 'ê³¼ê±° ìì‚´ ì‹œë„ ì—¬ë¶€' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'suicideHx'] = 1 if soc['ê³¼ê±° ìì‚´ ì‹œë„ ì—¬ë¶€'].values[0] == 'Y' else 0\n",
    "                if 'ì§€ë‚œ 1ë‹¬ê°„ ìì‚´ì‹œë„ ì—¬ë¶€' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'suicide_need'] = 1 if soc['ì§€ë‚œ 1ë‹¬ê°„ ìì‚´ì‹œë„ ì—¬ë¶€'].values[0] == 'Y' else 0\n",
    "\n",
    "\n",
    "            # 2. Pattern ì²˜ë¦¬\n",
    "            pat_path = f\"{pid}_Pattern.csv\"\n",
    "            if pat_path in inner_zip.namelist():\n",
    "                pat = pd.read_csv(inner_zip.open(pat_path))\n",
    "                pat['ì‘ì„±ì¼'] = pd.to_datetime(pat['ì‘ì„±ì¼'], errors='coerce')\n",
    "\n",
    "                for idx, row in df[df[\"ID\"] == pid].iterrows():\n",
    "                    d = row[\"date\"]\n",
    "                    today_rows = pat[pat[\"ì‘ì„±ì¼\"] == d]\n",
    "                    for _, r in today_rows.iterrows():\n",
    "                        t = r.get('ì¢…ë¥˜', '')\n",
    "                        st = r.get('ì„¸ë¶€ì¢…ë¥˜', '')\n",
    "                        amount = r.get('ì–‘', None)  # 'ì–‘' ì»¬ëŸ¼ ê°’\n",
    "                        # ìš´ë™\n",
    "                        if t == 'ìš´ë™':\n",
    "                            # ì–‘ ê°’ì´ ìˆìœ¼ë©´ ê·¸ ê°’ì„, ì—†ìœ¼ë©´ 1 ë¡œ ë””í´íŠ¸\n",
    "                            df.at[idx, 'exercise'] = amount if pd.notna(amount) else 1\n",
    "                        # ì¹´í˜ì¸\n",
    "                        if t == 'ì¹´í˜ì¸':\n",
    "                            df.at[idx, 'coffee'] = amount if pd.notna(amount) else 1\n",
    "                        # í¡ì—°\n",
    "                        if t == 'í¡ì—°':\n",
    "                            df.at[idx, 'smoking'] = amount if pd.notna(amount) else 1\n",
    "                        # ìŒì£¼(ì–‘ì´ ì•„ë‹Œ ë‹¨ìˆœ ì—¬ë¶€ë§Œ ì›í•˜ë©´ ê¸°ì¡´ì²˜ëŸ¼ 1ë¡œ)\n",
    "                        if t == 'ìŒì£¼':\n",
    "                            df.at[idx, 'alcohol'] = amount if pd.notna(amount) else 1\n",
    "                        # ìƒë¦¬\n",
    "                        if t == 'ìƒë¦¬' and st == 'ìƒë¦¬ì¤‘':\n",
    "                            df.at[idx, 'menstruation'] = 1\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_dates_and_demo.csv\")\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- ì„¤ì •: ìƒëŒ€ ê²½ë¡œ ì§€ì • ---\n",
    "csv_path   = output_path\n",
    "\n",
    "# 1) ì›ë³¸ CSV ì½ê¸° ë° í•„í„°ë§\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df['ID'].astype(str).str.startswith('PXPN')].copy()\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# 2) ì»¬ëŸ¼ ì´ˆê¸°í™”\n",
    "cols_to_add = ['marriage','job','alcohol','coffee','smoking',\n",
    "               'menstruation','exercise','smkHx','drinkHx',\n",
    "               'suicideHx','suicide_need']\n",
    "for col in cols_to_add:\n",
    "    df[col] = np.nan\n",
    "\n",
    "# 3) ZIP ì—´ê¸° ë° ë‚´ë¶€ ZIP ë§¤í•‘\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    all_files = outer_zip.namelist()\n",
    "    active_data_zips = {\n",
    "        os.path.basename(f).replace('_ActiveData.zip',''): f\n",
    "        for f in all_files\n",
    "        if 'ActiveData/' in f and f.endswith('_ActiveData.zip')\n",
    "           and '__MACOSX' not in f and '/._' not in f and '.DS_Store' not in f\n",
    "    }\n",
    "\n",
    "    # 4) ê° IDë³„ Sociodemographic & Pattern ì²˜ë¦¬\n",
    "    for pid in df['ID'].astype(str).unique():\n",
    "        inner_path = active_data_zips.get(pid)\n",
    "        if not inner_path:\n",
    "            print(f\"âš ï¸ {pid}: ActiveData ZIP ë¯¸ë°œê²¬\")\n",
    "            continue\n",
    "\n",
    "        buf = BytesIO(outer_zip.read(inner_path))\n",
    "        buf.seek(0)\n",
    "        with zipfile.ZipFile(buf,'r') as inner_zip:\n",
    "            names = inner_zip.namelist()\n",
    "\n",
    "            # Sociodemographic.csv ì²˜ë¦¬\n",
    "            soc_file = f\"{pid}_Sociodemographic.csv\"\n",
    "            if soc_file in names:\n",
    "                soc_df = pd.read_csv(inner_zip.open(soc_file), header=None, index_col=0).T\n",
    "                m = lambda v,p='Y': 1 if str(v).strip()==p else 0\n",
    "                # ê²°í˜¼ ì—¬ë¶€\n",
    "                if 'ê²°í˜¼' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'marriage'] = m(soc_df['ê²°í˜¼'].iloc[0], 'ê¸°í˜¼')\n",
    "                # í˜„ì¬ ì§ì—… ìœ ë¬´\n",
    "                if 'í˜„ì¬ ì§ì—… ìœ ë¬´' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'job'] = m(soc_df['í˜„ì¬ ì§ì—… ìœ ë¬´'].iloc[0])\n",
    "                # ê³¼ê±° í¡ì—° ì—¬ë¶€\n",
    "                if 'ê³¼ê±° í¡ì—° ì—¬ë¶€' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'smkHx'] = m(soc_df['ê³¼ê±° í¡ì—° ì—¬ë¶€'].iloc[0])\n",
    "                # ì§€ê¸ˆê¹Œì§€ ìŒì£¼ ì—¬ë¶€\n",
    "                if 'ì§€ê¸ˆê¹Œì§€ ìŒì£¼ ì—¬ë¶€' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'drinkHx'] = m(soc_df['ì§€ê¸ˆê¹Œì§€ ìŒì£¼ ì—¬ë¶€'].iloc[0])\n",
    "                # ê³¼ê±° ìì‚´ ì‹œë„ ì—¬ë¶€\n",
    "                if 'ê³¼ê±° ìì‚´ ì‹œë„ ì—¬ë¶€' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'suicideHx'] = m(soc_df['ê³¼ê±° ìì‚´ ì‹œë„ ì—¬ë¶€'].iloc[0])\n",
    "                # ì§€ë‚œ 1ë‹¬ê°„ ìì‚´ì‹œë„ ì—¬ë¶€\n",
    "                if 'ì§€ë‚œ 1ë‹¬ê°„ ìì‚´ì‹œë„ ì—¬ë¶€' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'suicide_need'] = m(soc_df['ì§€ë‚œ 1ë‹¬ê°„ ìì‚´ì‹œë„ ì—¬ë¶€'].iloc[0])\n",
    "            else:\n",
    "                print(f\"âš ï¸ {pid}: {soc_file} ì—†ìŒ. ë‚´ë¶€ íŒŒì¼ë“¤: {names}\")\n",
    "\n",
    "            # Pattern ì²˜ë¦¬\n",
    "            pat_file = f\"{pid}_Pattern.csv\"\n",
    "            if pat_file in names:\n",
    "                pat_df = pd.read_csv(inner_zip.open(pat_file))\n",
    "                pat_df['ì‘ì„±ì¼'] = pd.to_datetime(pat_df['ì‘ì„±ì¼'], errors='coerce')\n",
    "                for idx, row in df[df['ID']==pid].iterrows():\n",
    "                    d = row['date'].date()\n",
    "                    today = pat_df[pat_df['ì‘ì„±ì¼'].dt.date==d]\n",
    "                    for _, r in today.iterrows():\n",
    "                        t, st = r.get('ì¢…ë¥˜',''), r.get('ì„¸ë¶€ì¢…ë¥˜','')\n",
    "                        amt = r.get('ì–‘', np.nan)\n",
    "                        if t=='ìš´ë™': df.at[idx,'exercise']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='ì¹´í˜ì¸': df.at[idx,'coffee']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='í¡ì—°': df.at[idx,'smoking']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='ìŒì£¼': df.at[idx,'alcohol']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='ìƒë¦¬' and st=='ìƒë¦¬ì¤‘': df.at[idx,'menstruation']=1\n",
    "            else:\n",
    "                print(f\"âš ï¸ {pid}: {pat_file} ì—†ìŒ. ë‚´ë¶€ íŒŒì¼ë“¤: {names}\")\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_dates_and_demo.csv\")\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "zip_path = zip_path\n",
    "processed = df  \n",
    "\n",
    "# 3. PXPN ID ëª©ë¡ ì¶”ì¶œ\n",
    "pxpn_ids = processed[processed['ID'].astype(str).str.startswith('PXPN')]['ID'].unique()\n",
    "\n",
    "# 4. ê°ì • ê´€ë ¨ ì»¬ëŸ¼ ì´ˆê¸°í™”\n",
    "emotion_cols = ['positive_feeling', 'negative', 'positive_E', 'negative_E', 'anxiety', 'annoying']\n",
    "for col in emotion_cols:\n",
    "    if col not in processed.columns:\n",
    "        processed[col] = np.nan\n",
    "\n",
    "# 5. ë””ë²„ê·¸ìš© ì¹´ìš´í„° ë° ì •ë³´\n",
    "match_count = 0\n",
    "no_date_match = 0\n",
    "processed_ids = set()\n",
    "debug_info = []\n",
    "\n",
    "# 6. ì™¸ë¶€ zip ì—´ê¸°\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "debug_info = []\n",
    "\n",
    "# 1) ZIP ì—´ê¸°\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    all_files = outer_zip.namelist()\n",
    "\n",
    "    # 2) ActiveData ZIP ë§¤í•‘ (PID â†’ ë‚´ë¶€ ê²½ë¡œ)\n",
    "    active_data_zips = {\n",
    "        os.path.basename(f).replace('_ActiveData.zip', ''): f\n",
    "        for f in all_files\n",
    "        if 'ActiveData/' in f and f.endswith('_ActiveData.zip')\n",
    "           and '__MACOSX' not in f and '/._' not in f and '.DS_Store' not in f\n",
    "    }\n",
    "\n",
    "    # 3) ëª¨ë“  PXPN IDì— ëŒ€í•´ ë°˜ë³µ\n",
    "    for pid in pxpn_ids:\n",
    "        pid = str(pid).strip()\n",
    "        inner_path = active_data_zips.get(pid)\n",
    "        if not inner_path:\n",
    "            debug_info.append(f\"âŒ ID {pid}: ActiveData ZIP ì—†ìŒ\")\n",
    "            continue\n",
    "\n",
    "        # 4) ë‚´ë¶€ ZIP ì—´ê¸°\n",
    "        buf = BytesIO(outer_zip.read(inner_path))\n",
    "        buf.seek(0)\n",
    "        if not zipfile.is_zipfile(buf):\n",
    "            debug_info.append(f\"âŒ ID {pid}: ìœ íš¨í•œ ZIP ì•„ë‹˜\")\n",
    "            continue\n",
    "        buf.seek(0)\n",
    "\n",
    "        with zipfile.ZipFile(buf, 'r') as inner_zip:\n",
    "            # 5) Checkup.csv ê²½ë¡œ í™•ì¸\n",
    "            checkup_file = f\"{pid}_Checkup.csv\"\n",
    "            if checkup_file not in inner_zip.namelist():\n",
    "                debug_info.append(f\"âš ï¸ ID {pid}: {checkup_file} ì—†ìŒ\")\n",
    "                continue\n",
    "\n",
    "            # 6) Checkup CSV ì½ê¸°\n",
    "            with inner_zip.open(checkup_file) as f:\n",
    "                checkup = pd.read_csv(f)\n",
    "\n",
    "            # ë‚ ì§œ íƒ€ì… ë³€í™˜\n",
    "            processed_pid = processed[processed['ID'] == pid].copy()\n",
    "            processed_pid['date'] = pd.to_datetime(processed_pid['date'], errors='coerce')\n",
    "            checkup['ì‘ì„±ì¼'] = pd.to_datetime(checkup['ì‘ì„±ì¼'], errors='coerce')\n",
    "\n",
    "            # ê°ì • ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬\n",
    "            for category in ['ê¸°ë¶„', 'ì—ë„ˆì§€', 'ë¶ˆì•ˆ', 'ì§œì¦']:\n",
    "                category_data = checkup[checkup['ì¢…ë¥˜'] == category]\n",
    "\n",
    "                for _, row in category_data.iterrows():\n",
    "                    checkup_date = row['ì‘ì„±ì¼']\n",
    "                    score = row['ì²™ë„']\n",
    "\n",
    "                    for idx, proc_row in processed_pid.iterrows():\n",
    "                        proc_date = proc_row['date']\n",
    "                        if (\n",
    "                            proc_date.year == checkup_date.year and\n",
    "                            proc_date.month == checkup_date.month and\n",
    "                            proc_date.day == checkup_date.day\n",
    "                        ):\n",
    "                            if category == 'ê¸°ë¶„':\n",
    "                                if score > 0:\n",
    "                                    processed.at[idx, 'positive_feeling'] = score\n",
    "                                elif score < 0:\n",
    "                                    processed.at[idx, 'negative'] = score\n",
    "                            elif category == 'ì—ë„ˆì§€':\n",
    "                                if score > 0:\n",
    "                                    processed.at[idx, 'positive_E'] = score\n",
    "                                elif score < 0:\n",
    "                                    processed.at[idx, 'negative_E'] = score\n",
    "                            elif category == 'ë¶ˆì•ˆ':\n",
    "                                processed.at[idx, 'anxiety'] = score\n",
    "                            elif category == 'ì§œì¦':\n",
    "                                processed.at[idx, 'annoying'] = score\n",
    "\n",
    "                            match_count += 1\n",
    "                            processed_ids.add(pid)\n",
    "\n",
    "\n",
    "\n",
    "# 8. ê¸°ë¶„ ë° ì—ë„ˆì§€ ì¶©ëŒ ì¡°ì • (ì ˆëŒ€ê°’ ê¸°ì¤€)\n",
    "mask_mood = processed['positive_feeling'].notna() & processed['negative'].notna()\n",
    "for idx in processed[mask_mood].index:\n",
    "    pos = processed.at[idx, 'positive_feeling']\n",
    "    neg = processed.at[idx, 'negative']\n",
    "    if abs(pos) > abs(neg):\n",
    "        processed.at[idx, 'negative'] = 0\n",
    "    elif abs(pos) < abs(neg):\n",
    "        processed.at[idx, 'positive_feeling'] = 0\n",
    "    else:\n",
    "        processed.at[idx, 'positive_feeling'] = 0  # ë™ì¼í•˜ë©´ ê¸ì • ì œê±°, ë¶€ì • ìœ ì§€\n",
    "\n",
    "mask_energy = processed['positive_E'].notna() & processed['negative_E'].notna()\n",
    "for idx in processed[mask_energy].index:\n",
    "    posE = processed.at[idx, 'positive_E']\n",
    "    negE = processed.at[idx, 'negative_E']\n",
    "    if abs(posE) > abs(negE):\n",
    "        processed.at[idx, 'negative_E'] = 0\n",
    "    elif abs(posE) < abs(negE):\n",
    "        processed.at[idx, 'positive_E'] = 0\n",
    "    else:\n",
    "        processed.at[idx, 'positive_E'] = 0\n",
    "\n",
    "# 9. ê°’ì´ í•œìª½ë§Œ ìˆì„ ê²½ìš° ë‹¤ë¥¸ ìª½ì„ 0ìœ¼ë¡œ ì„¤ì •\n",
    "processed.loc[\n",
    "    processed['positive_feeling'].notna() & processed['negative'].isna(),\n",
    "    'negative'\n",
    "] = 0\n",
    "processed.loc[\n",
    "    processed['negative'].notna() & processed['positive_feeling'].isna(),\n",
    "    'positive_feeling'\n",
    "] = 0\n",
    "\n",
    "processed.loc[\n",
    "    processed['positive_E'].notna() & processed['negative_E'].isna(),\n",
    "    'negative_E'\n",
    "] = 0\n",
    "processed.loc[\n",
    "    processed['negative_E'].notna() & processed['positive_E'].isna(),\n",
    "    'positive_E'\n",
    "] = 0\n",
    "\n",
    "# 10. ë””ë²„ê·¸ ì¶œë ¥ (ìµœëŒ€ 20ê°œ)\n",
    "for info in debug_info[:20]:\n",
    "    print(info)\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_panic_demo_mood.csv\")\n",
    "processed.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype, is_string_dtype, is_datetime64_any_dtype\n",
    "\n",
    "# ì „ì²´ ì»¬ëŸ¼ ìˆœíšŒí•˜ë©° í˜•ì‹ í†µì¼\n",
    "for col in processed.columns:\n",
    "    if col == 'ID':\n",
    "        processed[col] = processed[col].astype(str).str.strip()\n",
    "    elif col == 'date':\n",
    "        processed[col] = pd.to_datetime(processed[col], errors='coerce')  # ë¬¸ìì—´ í¬í•¨ ì‹œ ìë™ ì²˜ë¦¬\n",
    "    elif is_datetime64_any_dtype(processed[col]):\n",
    "        processed[col] = pd.to_datetime(processed[col], errors='coerce')  # datetimeì´ë©´ ê·¸ëŒ€ë¡œ\n",
    "    elif is_string_dtype(processed[col]):\n",
    "        processed[col] = processed[col].astype(str).str.strip()  # ë¬¸ìì—´ì´ë©´ ì •ë¦¬\n",
    "    else:\n",
    "        # ì˜ˆì™¸ì ì¸ ê²½ìš°ë„ ë¬¸ìì—´ë¡œ í†µì¼\n",
    "        processed[col] = processed[col].astype(str).str.strip()\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"processed.csv\")\n",
    "processed.to_csv(output_path, index=False)\n",
    "print(\"ëª¨ë“  ì»¬ëŸ¼ í˜•ì‹ í†µì¼ ì™„ë£Œ ë° ì €ì¥ë¨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combined_nipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
