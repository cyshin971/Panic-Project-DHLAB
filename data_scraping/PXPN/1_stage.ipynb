{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Packages:\n",
    "- pandas  : 1.5.3\n",
    "- openpyxl: 3.1.5\n",
    "- xlrd    : 2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlrd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "from library.path_utils import get_file_path, to_absolute_path\n",
    "\n",
    "print(\"pandas  :\", pd.__version__)\n",
    "print(\"openpyxl:\", openpyxl.__version__)\n",
    "print(\"xlrd    :\", xlrd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ | Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PXPN_DIR = \"./raw_data/PXPN\"\n",
    "\n",
    "enroll_file_name = \"pxpn_enroll_info\"\n",
    "zip_file_name = \"pixelpanic_raw_data.zip\"\n",
    "output_folder_name = \"./_tmp/PXPN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.xlsx\")\n",
    "zip_path = get_file_path(RAW_PXPN_DIR, f\"{zip_file_name}\")\n",
    "output_folder = to_absolute_path(output_folder_name)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "csv_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")\n",
    "\n",
    "# 엑셀 읽고 csv로 저장\n",
    "df = pd.read_excel(enroll_path)\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "outer_zip_path = zip_path\n",
    "\n",
    "# 1) 바깥 ZIP 열기\n",
    "with zipfile.ZipFile(outer_zip_path, 'r') as outer_zip:\n",
    "    # 2) ActiveData 폴더 안의 inner-zip 경로 수집\n",
    "    active_data_zips = {}\n",
    "    for member in outer_zip.namelist():\n",
    "        # 경로에 'ActiveData/' 포함하고, '_ActiveData.zip' 으로 끝나는 파일만 선택\n",
    "        if 'ActiveData/' in member and member.endswith('_ActiveData.zip'):\n",
    "            # 파일명에서 patient_code(예: 'ABC123') 추출\n",
    "            base = os.path.basename(member)                # e.g. 'ABC123_ActiveData.zip'\n",
    "            patient_code = base.replace('_ActiveData.zip', '')\n",
    "            active_data_zips[patient_code] = member\n",
    "\n",
    "    # 3) 모은 경로들을 순회하며 내부 ZIP 열기\n",
    "    for patient_code, inner_path in active_data_zips.items():\n",
    "        # outer_zip.read()로 바이트 읽기 → BytesIO로 감싸기\n",
    "        inner_bytes = BytesIO(outer_zip.read(inner_path))\n",
    "        with zipfile.ZipFile(inner_bytes, 'r') as inner_zip:\n",
    "            # 원하는 CSV 파일명\n",
    "            survey_csv = f'{patient_code}_SurveyResponse.csv'\n",
    "            if survey_csv not in inner_zip.namelist():\n",
    "                print(f'[경고] {patient_code} 내부에 {survey_csv} 없음')\n",
    "                continue\n",
    "\n",
    "            # CSV 열어서 DataFrame으로 읽기\n",
    "            with inner_zip.open(survey_csv) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                print(f'{patient_code}: 읽은 행 수 =', len(df))\n",
    "                # TODO: df 처리 로직 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "                 \n",
    "\n",
    "\n",
    "# 1) 설문 리스트\n",
    "top_5 = [\n",
    "    '특성 불안 설문', '한국형 회복탄력성 지수', '한국어판 아침형-저녁형 설문지',\n",
    "    '한글판 생물학적 리듬 평가 설문지', '유년기 외상 척도', '한국형 기분장애 설문지',\n",
    "    '광장공포 인지 설문지', '알바니 공황-공포 질문지', '신체감각 설문지',\n",
    "    '한글판 범불안 장애', '한국어판 우울증 선별도구'\n",
    "]\n",
    "result = pd.DataFrame(columns=['patient_code', '날짜'])\n",
    "# 2) 결과 DataFrame 초기화\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "outer_zip_path = zip_path\n",
    "\n",
    "# 1) 바깥 ZIP 열기\n",
    "with zipfile.ZipFile(outer_zip_path, 'r') as outer_zip:\n",
    "    # 2) ActiveData 폴더 안의 inner-zip 경로 수집\n",
    "    active_data_zips = {}\n",
    "    for member in outer_zip.namelist():\n",
    "        # 경로에 'ActiveData/' 포함하고, '_ActiveData.zip' 으로 끝나는 파일만 선택\n",
    "        if 'ActiveData/' in member and member.endswith('_ActiveData.zip'):\n",
    "            # 파일명에서 patient_code(예: 'ABC123') 추출\n",
    "            base = os.path.basename(member)                # e.g. 'ABC123_ActiveData.zip'\n",
    "            patient_code = base.replace('_ActiveData.zip', '')\n",
    "            active_data_zips[patient_code] = member\n",
    "\n",
    "    # 3) 모은 경로들을 순회하며 내부 ZIP 열기\n",
    "    for patient_code, inner_path in active_data_zips.items():\n",
    "        # outer_zip.read()로 바이트 읽기 → BytesIO로 감싸기\n",
    "        inner_bytes = BytesIO(outer_zip.read(inner_path))\n",
    "        with zipfile.ZipFile(inner_bytes, 'r') as inner_zip:\n",
    "            # 원하는 CSV 파일명\n",
    "            survey_csv = f'{patient_code}_SurveyResponse.csv'\n",
    "            if survey_csv not in inner_zip.namelist():\n",
    "                print(f'[경고] {patient_code} 내부에 {survey_csv} 없음')\n",
    "                continue\n",
    "\n",
    "            # CSV 열어서 DataFrame으로 읽기\n",
    "            with inner_zip.open(survey_csv) as f:\n",
    "                df = pd.read_csv(f)\n",
    "                # TODO: df 처리 로직 추가\n",
    "            # 작성일 컬럼에서 날짜만 뽑기\n",
    "            date_value = pd.to_datetime(df['작성일'].iloc[0]).date()\n",
    "\n",
    "            # 결과 DataFrame에 (patient_code, 날짜) 행 추가\n",
    "            mask = (result['patient_code'] == patient_code) & (result['날짜'] == date_value)\n",
    "            if not mask.any():\n",
    "                result = pd.concat([\n",
    "                    result,\n",
    "                    pd.DataFrame([{'patient_code': patient_code, '날짜': date_value}])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            # 5) 설문별 점수 추출 및 결과에 삽입\n",
    "            for survey in top_5:\n",
    "                sub = df[df['설문명'] == survey].reset_index(drop=True)\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "\n",
    "                # 실제 점수(real_score) 리스트\n",
    "                scores = []\n",
    "                for _, row in sub.iterrows():\n",
    "                    if row.get('역채점인 경우 역채점 점수', '-') != '-':\n",
    "                        scores.append(float(row['역채점인 경우 역채점 점수']))\n",
    "                    else:\n",
    "                        v = row.get('점수')\n",
    "                        scores.append(float(v) if pd.notna(v) else '***')\n",
    "\n",
    "                # 설문명 → 컬럼 접두사 매핑\n",
    "                prefix_map = {\n",
    "                    '특성 불안 설문': 'STAI_X2',\n",
    "                    '한국형 회복탄력성 지수': 'KRQ',\n",
    "                    '한국어판 아침형-저녁형 설문지': 'CSM',\n",
    "                    '한글판 생물학적 리듬 평가 설문지': 'BRIAN',\n",
    "                    '한국형 기분장애 설문지': 'MDQ',\n",
    "                    '광장공포 인지 설문지': 'ACQ',\n",
    "                    '신체감각 설문지': 'BSQ',\n",
    "                    '한글판 범불안 장애': 'GAD',\n",
    "                    '한국어판 우울증 선별도구': 'PHQ',\n",
    "                    # 유년기 외상 척도 → CTQ, 알바니 공황-공포 → APPQ (주제별)\n",
    "                    '유년기 외상 척도': 'CTQ',\n",
    "                    '알바니 공황-공포 질문지': 'APPQ'\n",
    "                }\n",
    "                prefix = prefix_map[survey]\n",
    "\n",
    "                # 주제별 분리 필요한 설문\n",
    "                if survey in ['유년기 외상 척도', '알바니 공황-공포 질문지']:\n",
    "                    sub['real_score'] = scores\n",
    "                    topics = sorted(sub['주제'].dropna().unique())\n",
    "                    for ti, topic in enumerate(topics, start=1):\n",
    "                        tdf = sub[sub['주제'] == topic].reset_index(drop=True)\n",
    "                        for qi, sc in enumerate(tdf['real_score'], start=1):\n",
    "                            col = f\"{prefix}-{ti}-{qi}\"\n",
    "                            result.loc[\n",
    "                                (result['patient_code'] == patient_code) &\n",
    "                                (result['날짜'] == date_value),\n",
    "                                col\n",
    "                            ] = sc\n",
    "                else:\n",
    "                    for idx, sc in enumerate(scores, start=1):\n",
    "                        col = f\"{prefix}-{idx}\"\n",
    "                        result.loc[\n",
    "                            (result['patient_code'] == patient_code) &\n",
    "                            (result['날짜'] == date_value),\n",
    "                            col\n",
    "                        ] = sc\n",
    "\n",
    "# 6) 컬럼 순서 재배열\n",
    "cols = ['patient_code', '날짜'] + [c for c in result.columns if c not in ['patient_code', '날짜']]\n",
    "result = result[cols]\n",
    "\n",
    "# 7) 불필요 컬럼 삭제 (예: 범위를 벗어난 MDQ-14,15 등)\n",
    "result = result.drop(columns=['MDQ-14', 'MDQ-15', 'PHQ-10'], errors='ignore')\n",
    "\n",
    "# 8) 저장\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, \"questionnaire.csv\")\n",
    "result.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 접두어 목록\n",
    "prefixes = [\"PHQ\", \"STAI_X2\", \"CSM\", \"CTQ-1\", \"CTQ-2\", \"CTQ-3\", \"CTQ-4\", \"CTQ-5\", \"KRQ\", \"MDQ\", \"ACQ\", \"APPQ-1\", \"APPQ-2\", \"APPQ-3\", \"BSQ\", \"GAD\", \"BRIAN\"]\n",
    "# 결과를 저장할 데이터프레임\n",
    "aggregated_df = result[['patient_code', '날짜']].copy()\n",
    "\n",
    "for prefix in prefixes:\n",
    "    # 해당 접두어로 시작하는 컬럼 찾기\n",
    "    matched_cols = [col for col in result.columns if col.startswith(prefix)]\n",
    "    \n",
    "    # 값 합산해서 새로운 컬럼으로 추가\n",
    "    aggregated_df[f'{prefix.rstrip(\"-\")}'] = result[matched_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1)\n",
    "\n",
    "display(aggregated_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 경로\n",
    "file_path = enroll_path\n",
    "\n",
    "# 엑셀 시트 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 컬럼 이름 변경\n",
    "df = df.rename(columns={'회원코드': 'patient_code', '2. 성별': 'gender'})\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "df = df[['patient_code', '연구시작일', '연구종료일', 'gender']]\n",
    "\n",
    "# 날짜 형식으로 변환\n",
    "df['연구시작일'] = pd.to_datetime(df['연구시작일'], errors='coerce')\n",
    "df['연구종료일'] = pd.to_datetime(df['연구종료일'], errors='coerce')\n",
    "\n",
    "# 각 환자에 대해 날짜 생성\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notnull(row['연구시작일']) and pd.notnull(row['연구종료일']):\n",
    "        date_range = pd.date_range(start=row['연구시작일'], end=row['연구종료일'])\n",
    "        for date in date_range:\n",
    "            expanded_rows.append({'patient_code': row['patient_code'], '날짜': date, 'gender': row['gender']})\n",
    "\n",
    "# 결과 데이터프레임 생성\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df = expanded_df.replace({'남': '0', '여': '1'})\n",
    "print(expanded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 경로\n",
    "file_path = enroll_path\n",
    "\n",
    "# CSV 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 컬럼 이름 변경 (생년월일 컬럼도 추가)\n",
    "df = df.rename(columns={\n",
    "    '회원코드': 'patient_code',\n",
    "    '2. 성별': 'gender'  # 생년월일 컬럼 이름이 실제 다르면 이 부분 수정 필요\n",
    "})\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "df = df[['patient_code', '연구시작일', '연구종료일', 'gender', '3. 생년월일']]\n",
    "\n",
    "# 날짜 형식으로 변환\n",
    "df['연구시작일'] = pd.to_datetime(df['연구시작일'], errors='coerce')\n",
    "df['연구종료일'] = pd.to_datetime(df['연구종료일'], errors='coerce')\n",
    "df['3. 생년월일'] = pd.to_datetime(df['3. 생년월일'], errors='coerce')\n",
    "\n",
    "\n",
    "# 각 환자에 대해 날짜 생성\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notnull(row['연구시작일']) and pd.notnull(row['연구종료일']):\n",
    "        date_range = pd.date_range(start=row['연구시작일'], end=row['연구종료일'])\n",
    "        for date in date_range:\n",
    "            expanded_rows.append({\n",
    "                'patient_code': row['patient_code'],\n",
    "                '날짜': date,\n",
    "                'gender': '0' if row['gender'] == '남' else '1'\n",
    "            })\n",
    "\n",
    "# 결과 데이터프레임 생성\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# 출력\n",
    "print(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['날짜']    = pd.to_datetime(expanded_df['날짜'])\n",
    "aggregated_df['날짜'] = pd.to_datetime(aggregated_df['날짜'])\n",
    "expanded_answer = pd.merge(expanded_df, aggregated_df, on=['patient_code', '날짜'], how='outer')\n",
    "expanded_answer = expanded_answer.rename(columns={'patient_code': 'ID', '날짜': 'date', 'GAD': 'GAD_7', 'CTQ-1': 'CTQ_1', 'CTQ-2': 'CTQ_2', 'CTQ-3': 'CTQ_3', 'CTQ-4': 'CTQ_4', 'CTQ-5': 'CTQ_5', 'APPQ-1': 'APPQ_1', 'APPQ-2': 'APPQ_2', 'APPQ-3': 'APPQ_3', 'PHQ': 'PHQ_9'})\n",
    "output_path = os.path.join(output_folder, \"questionnaire_test.csv\")\n",
    "expanded_answer.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 날짜 형식 통일\n",
    "expanded_answer['date'] = pd.to_datetime(expanded_answer['date'])\n",
    "# 0) 결과 DataFrame 준비\n",
    "PXPN_panic_dates = pd.DataFrame(columns=['ID', 'date'])\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    for member in outer_zip.namelist():\n",
    "        # ➤ 끝이 _ActiveData.zip 이고, 경로 어딘가에 ActiveData/ 포함, 숨김파일 제외\n",
    "        if (\n",
    "            member.endswith('_ActiveData.zip') and\n",
    "            'ActiveData/' in member and\n",
    "            not any(x in member for x in ['__MACOSX', '/._', '.DS_Store'])\n",
    "        ):\n",
    "            # patient ID 추출\n",
    "            pid = os.path.basename(member).replace('_ActiveData.zip', '')\n",
    "\n",
    "            # 내부 ZIP 열기\n",
    "            buf = BytesIO(outer_zip.read(member))\n",
    "            buf.seek(0)\n",
    "            if not zipfile.is_zipfile(buf):\n",
    "                print(f\"❌ {member} 는 ZIP이 아닙니다.\")\n",
    "                continue\n",
    "            buf.seek(0)\n",
    "\n",
    "            with zipfile.ZipFile(buf, 'r') as inner_zip:\n",
    "                # Panic.csv 찾기\n",
    "                panic_files = [f for f in inner_zip.namelist() if f.endswith('Panic.csv')]\n",
    "                if not panic_files:\n",
    "                    print(f\"⚠️ {pid}: Panic.csv 없음\")\n",
    "                    continue\n",
    "\n",
    "                # (여러 개 있을 수 있으니 모두 처리)\n",
    "                for panic_fname in panic_files:\n",
    "                    with inner_zip.open(panic_fname) as f:\n",
    "                        df_panic = pd.read_csv(f)\n",
    "                        if '작성일' not in df_panic.columns:\n",
    "                            print(f\"⚠️ {pid}: 작성일 컬럼 없음 in {panic_fname}\")\n",
    "                            continue\n",
    "\n",
    "                        # 날짜별로 한 행씩 추가\n",
    "                        for d in pd.to_datetime(df_panic['작성일'], errors='coerce').dt.date.unique():\n",
    "                            PXPN_panic_dates = pd.concat([\n",
    "                                PXPN_panic_dates,\n",
    "                                pd.DataFrame({'ID': [pid], 'date': [d]})\n",
    "                            ], ignore_index=True)\n",
    "\n",
    "# 2) panic 값, 포맷 정리\n",
    "PXPN_panic_dates['panic'] = 2\n",
    "PXPN_panic_dates['date']  = pd.to_datetime(PXPN_panic_dates['date'])\\\n",
    "                                .dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3) expanded_answer 쪽도 date 컬럼 문자열 포맷으로\n",
    "expanded_answer['date'] = pd.to_datetime(expanded_answer['date'], errors='coerce')\\\n",
    "                               .dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3. outer merge\n",
    "merged = pd.merge(\n",
    "    PXPN_panic_dates,\n",
    "    expanded_answer,\n",
    "    on=['ID', 'date'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# 4. 우선순위 panic 값 유지\n",
    "merged = (\n",
    "    merged\n",
    "    .sort_values(['ID', 'date', 'panic'], ascending=[True, True, False])\n",
    "    .drop_duplicates(subset=['ID', 'date'], keep='first')\n",
    ")\n",
    "\n",
    "# 5. 전날 panic=1 적용\n",
    "df = merged.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "for _, row in df[df['panic'] == 2].iterrows():\n",
    "    pid = row['ID']\n",
    "    curr_date = row['date']\n",
    "    prev_date = curr_date - pd.Timedelta(days=1)\n",
    "\n",
    "    # 연속된 2 제거\n",
    "    while ((df['ID'] == pid) & (df['date'] == prev_date) & (df['panic'] == 2)).any():\n",
    "        prev_date -= pd.Timedelta(days=1)\n",
    "\n",
    "    mask = (df['ID'] == pid) & (df['date'] == prev_date)\n",
    "    if mask.any():\n",
    "        df.loc[mask & (df['panic'].fillna(0) < 1), 'panic'] = 1\n",
    "\n",
    "df = df.sort_values(['ID', 'date']).reset_index(drop=True)\n",
    "df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "df['panic'] = df['panic'].fillna(0)\n",
    "\n",
    "# 6. Enroll 병합 (age)\n",
    "enroll_df = pd.read_csv(enroll_path, encoding='utf-8')\n",
    "enroll_df = enroll_df.rename(columns={\n",
    "    '회원코드': 'patient_code',\n",
    "    '3. 생년월일': 'birthdate',\n",
    "    '연구종료일': 'end_date'\n",
    "})\n",
    "enroll_df['birthdate'] = pd.to_datetime(enroll_df['birthdate'], errors='coerce')\n",
    "enroll_df['end_date'] = pd.to_datetime(enroll_df['end_date'], errors='coerce')\n",
    "enroll_df['age'] = enroll_df['end_date'].dt.year - enroll_df['birthdate'].dt.year\n",
    "\n",
    "age_df = enroll_df[['patient_code', 'age']].drop_duplicates().rename(columns={'patient_code': 'ID'})\n",
    "final_df = df.merge(age_df, on='ID', how='left')\n",
    "\n",
    "# 7. 저장\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_date.csv\")\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "csv_path = output_path\n",
    "zip_path = zip_path\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df[\"ID\"].str.startswith(\"PXPN\")].copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# 컬럼 초기화\n",
    "for col in ['marriage', 'job', 'alcohol', 'coffee', 'smoking', 'menstruation', 'exercise',\n",
    "            'smkHx', 'drinkHx', 'suicideHx', 'suicide_need']:\n",
    "    df[col] = np.nan\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    all_files = outer_zip.namelist()\n",
    "\n",
    "    # ActiveData ZIP 파일만 골라서 {pid: 경로} 매핑\n",
    "    active_data_zips = {\n",
    "        os.path.basename(f).replace('_ActiveData.zip', ''): f\n",
    "        for f in all_files\n",
    "        if f.startswith(\"pixelpanic_raw_data/ActiveData/\") and f.endswith(\"_ActiveData.zip\")\n",
    "           and \"__MACOSX\" not in f and \"/._\" not in f and \".DS_Store\" not in f\n",
    "    }\n",
    "\n",
    "    for pid in df['ID'].unique():\n",
    "        pid = str(pid).strip()\n",
    "        if pid not in active_data_zips:\n",
    "            continue\n",
    "\n",
    "        # inner ZIP 바이트로 읽어서 열기\n",
    "        inner_path = active_data_zips[pid]\n",
    "        buf = BytesIO(outer_zip.read(inner_path))\n",
    "        with zipfile.ZipFile(buf, 'r') as inner_zip:\n",
    "\n",
    "            # Sociodemographic.csv 처리\n",
    "            soc_file = f\"{pid}_Sociodemographic.csv\"\n",
    "            if soc_file in inner_zip.namelist():\n",
    "                # header=None, index_col=0 로 읽고 transpose\n",
    "                soc_df = pd.read_csv(inner_zip.open(soc_file), header=None, index_col=0).T\n",
    "                if '결혼' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'marriage'] = 1 if soc['결혼'].values[0] == '기혼' else 0\n",
    "                if '현재 직업 유무' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'job'] = 1 if soc['현재 직업 유무'].values[0] == 'Y' else 0\n",
    "                if '과거 흡연 여부' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'smkHx'] = 1 if soc['과거 흡연 여부'].values[0] == 'Y' else 0\n",
    "                if '지금까지 음주 여부' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'drinkHx'] = 1 if soc['지금까지 음주 여부'].values[0] == 'Y' else 0\n",
    "                if '과거 자살 시도 여부' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'suicideHx'] = 1 if soc['과거 자살 시도 여부'].values[0] == 'Y' else 0\n",
    "                if '지난 1달간 자살시도 여부' in soc.columns:\n",
    "                    df.loc[df['ID'] == pid, 'suicide_need'] = 1 if soc['지난 1달간 자살시도 여부'].values[0] == 'Y' else 0\n",
    "\n",
    "\n",
    "            # 2. Pattern 처리\n",
    "            pat_path = f\"{pid}_Pattern.csv\"\n",
    "            if pat_path in inner_zip.namelist():\n",
    "                pat = pd.read_csv(inner_zip.open(pat_path))\n",
    "                pat['작성일'] = pd.to_datetime(pat['작성일'], errors='coerce')\n",
    "\n",
    "                for idx, row in df[df[\"ID\"] == pid].iterrows():\n",
    "                    d = row[\"date\"]\n",
    "                    today_rows = pat[pat[\"작성일\"] == d]\n",
    "                    for _, r in today_rows.iterrows():\n",
    "                        t = r.get('종류', '')\n",
    "                        st = r.get('세부종류', '')\n",
    "                        amount = r.get('양', None)  # '양' 컬럼 값\n",
    "                        # 운동\n",
    "                        if t == '운동':\n",
    "                            # 양 값이 있으면 그 값을, 없으면 1 로 디폴트\n",
    "                            df.at[idx, 'exercise'] = amount if pd.notna(amount) else 1\n",
    "                        # 카페인\n",
    "                        if t == '카페인':\n",
    "                            df.at[idx, 'coffee'] = amount if pd.notna(amount) else 1\n",
    "                        # 흡연\n",
    "                        if t == '흡연':\n",
    "                            df.at[idx, 'smoking'] = amount if pd.notna(amount) else 1\n",
    "                        # 음주(양이 아닌 단순 여부만 원하면 기존처럼 1로)\n",
    "                        if t == '음주':\n",
    "                            df.at[idx, 'alcohol'] = amount if pd.notna(amount) else 1\n",
    "                        # 생리\n",
    "                        if t == '생리' and st == '생리중':\n",
    "                            df.at[idx, 'menstruation'] = 1\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_dates_and_demo.csv\")\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 설정: 상대 경로 지정 ---\n",
    "csv_path   = output_path\n",
    "\n",
    "# 1) 원본 CSV 읽기 및 필터링\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df['ID'].astype(str).str.startswith('PXPN')].copy()\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# 2) 컬럼 초기화\n",
    "cols_to_add = ['marriage','job','alcohol','coffee','smoking',\n",
    "               'menstruation','exercise','smkHx','drinkHx',\n",
    "               'suicideHx','suicide_need']\n",
    "for col in cols_to_add:\n",
    "    df[col] = np.nan\n",
    "\n",
    "# 3) ZIP 열기 및 내부 ZIP 매핑\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    all_files = outer_zip.namelist()\n",
    "    active_data_zips = {\n",
    "        os.path.basename(f).replace('_ActiveData.zip',''): f\n",
    "        for f in all_files\n",
    "        if 'ActiveData/' in f and f.endswith('_ActiveData.zip')\n",
    "           and '__MACOSX' not in f and '/._' not in f and '.DS_Store' not in f\n",
    "    }\n",
    "\n",
    "    # 4) 각 ID별 Sociodemographic & Pattern 처리\n",
    "    for pid in df['ID'].astype(str).unique():\n",
    "        inner_path = active_data_zips.get(pid)\n",
    "        if not inner_path:\n",
    "            print(f\"⚠️ {pid}: ActiveData ZIP 미발견\")\n",
    "            continue\n",
    "\n",
    "        buf = BytesIO(outer_zip.read(inner_path))\n",
    "        buf.seek(0)\n",
    "        with zipfile.ZipFile(buf,'r') as inner_zip:\n",
    "            names = inner_zip.namelist()\n",
    "\n",
    "            # Sociodemographic.csv 처리\n",
    "            soc_file = f\"{pid}_Sociodemographic.csv\"\n",
    "            if soc_file in names:\n",
    "                soc_df = pd.read_csv(inner_zip.open(soc_file), header=None, index_col=0).T\n",
    "                m = lambda v,p='Y': 1 if str(v).strip()==p else 0\n",
    "                # 결혼 여부\n",
    "                if '결혼' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'marriage'] = m(soc_df['결혼'].iloc[0], '기혼')\n",
    "                # 현재 직업 유무\n",
    "                if '현재 직업 유무' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'job'] = m(soc_df['현재 직업 유무'].iloc[0])\n",
    "                # 과거 흡연 여부\n",
    "                if '과거 흡연 여부' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'smkHx'] = m(soc_df['과거 흡연 여부'].iloc[0])\n",
    "                # 지금까지 음주 여부\n",
    "                if '지금까지 음주 여부' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'drinkHx'] = m(soc_df['지금까지 음주 여부'].iloc[0])\n",
    "                # 과거 자살 시도 여부\n",
    "                if '과거 자살 시도 여부' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'suicideHx'] = m(soc_df['과거 자살 시도 여부'].iloc[0])\n",
    "                # 지난 1달간 자살시도 여부\n",
    "                if '지난 1달간 자살시도 여부' in soc_df.columns:\n",
    "                    df.loc[df['ID']==pid,'suicide_need'] = m(soc_df['지난 1달간 자살시도 여부'].iloc[0])\n",
    "            else:\n",
    "                print(f\"⚠️ {pid}: {soc_file} 없음. 내부 파일들: {names}\")\n",
    "\n",
    "            # Pattern 처리\n",
    "            pat_file = f\"{pid}_Pattern.csv\"\n",
    "            if pat_file in names:\n",
    "                pat_df = pd.read_csv(inner_zip.open(pat_file))\n",
    "                pat_df['작성일'] = pd.to_datetime(pat_df['작성일'], errors='coerce')\n",
    "                for idx, row in df[df['ID']==pid].iterrows():\n",
    "                    d = row['date'].date()\n",
    "                    today = pat_df[pat_df['작성일'].dt.date==d]\n",
    "                    for _, r in today.iterrows():\n",
    "                        t, st = r.get('종류',''), r.get('세부종류','')\n",
    "                        amt = r.get('양', np.nan)\n",
    "                        if t=='운동': df.at[idx,'exercise']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='카페인': df.at[idx,'coffee']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='흡연': df.at[idx,'smoking']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='음주': df.at[idx,'alcohol']=amt if pd.notna(amt) else 1\n",
    "                        elif t=='생리' and st=='생리중': df.at[idx,'menstruation']=1\n",
    "            else:\n",
    "                print(f\"⚠️ {pid}: {pat_file} 없음. 내부 파일들: {names}\")\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_and_panic_dates_and_demo.csv\")\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 경로 설정\n",
    "zip_path = zip_path\n",
    "processed = df  \n",
    "\n",
    "# 3. PXPN ID 목록 추출\n",
    "pxpn_ids = processed[processed['ID'].astype(str).str.startswith('PXPN')]['ID'].unique()\n",
    "\n",
    "# 4. 감정 관련 컬럼 초기화\n",
    "emotion_cols = ['positive_feeling', 'negative', 'positive_E', 'negative_E', 'anxiety', 'annoying']\n",
    "for col in emotion_cols:\n",
    "    if col not in processed.columns:\n",
    "        processed[col] = np.nan\n",
    "\n",
    "# 5. 디버그용 카운터 및 정보\n",
    "match_count = 0\n",
    "no_date_match = 0\n",
    "processed_ids = set()\n",
    "debug_info = []\n",
    "\n",
    "# 6. 외부 zip 열기\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "debug_info = []\n",
    "\n",
    "# 1) ZIP 열기\n",
    "with zipfile.ZipFile(zip_path, 'r') as outer_zip:\n",
    "    all_files = outer_zip.namelist()\n",
    "\n",
    "    # 2) ActiveData ZIP 매핑 (PID → 내부 경로)\n",
    "    active_data_zips = {\n",
    "        os.path.basename(f).replace('_ActiveData.zip', ''): f\n",
    "        for f in all_files\n",
    "        if 'ActiveData/' in f and f.endswith('_ActiveData.zip')\n",
    "           and '__MACOSX' not in f and '/._' not in f and '.DS_Store' not in f\n",
    "    }\n",
    "\n",
    "    # 3) 모든 PXPN ID에 대해 반복\n",
    "    for pid in pxpn_ids:\n",
    "        pid = str(pid).strip()\n",
    "        inner_path = active_data_zips.get(pid)\n",
    "        if not inner_path:\n",
    "            debug_info.append(f\"❌ ID {pid}: ActiveData ZIP 없음\")\n",
    "            continue\n",
    "\n",
    "        # 4) 내부 ZIP 열기\n",
    "        buf = BytesIO(outer_zip.read(inner_path))\n",
    "        buf.seek(0)\n",
    "        if not zipfile.is_zipfile(buf):\n",
    "            debug_info.append(f\"❌ ID {pid}: 유효한 ZIP 아님\")\n",
    "            continue\n",
    "        buf.seek(0)\n",
    "\n",
    "        with zipfile.ZipFile(buf, 'r') as inner_zip:\n",
    "            # 5) Checkup.csv 경로 확인\n",
    "            checkup_file = f\"{pid}_Checkup.csv\"\n",
    "            if checkup_file not in inner_zip.namelist():\n",
    "                debug_info.append(f\"⚠️ ID {pid}: {checkup_file} 없음\")\n",
    "                continue\n",
    "\n",
    "            # 6) Checkup CSV 읽기\n",
    "            with inner_zip.open(checkup_file) as f:\n",
    "                checkup = pd.read_csv(f)\n",
    "\n",
    "            # 날짜 타입 변환\n",
    "            processed_pid = processed[processed['ID'] == pid].copy()\n",
    "            processed_pid['date'] = pd.to_datetime(processed_pid['date'], errors='coerce')\n",
    "            checkup['작성일'] = pd.to_datetime(checkup['작성일'], errors='coerce')\n",
    "\n",
    "            # 감정 카테고리별 처리\n",
    "            for category in ['기분', '에너지', '불안', '짜증']:\n",
    "                category_data = checkup[checkup['종류'] == category]\n",
    "\n",
    "                for _, row in category_data.iterrows():\n",
    "                    checkup_date = row['작성일']\n",
    "                    score = row['척도']\n",
    "\n",
    "                    for idx, proc_row in processed_pid.iterrows():\n",
    "                        proc_date = proc_row['date']\n",
    "                        if (\n",
    "                            proc_date.year == checkup_date.year and\n",
    "                            proc_date.month == checkup_date.month and\n",
    "                            proc_date.day == checkup_date.day\n",
    "                        ):\n",
    "                            if category == '기분':\n",
    "                                if score > 0:\n",
    "                                    processed.at[idx, 'positive_feeling'] = score\n",
    "                                elif score < 0:\n",
    "                                    processed.at[idx, 'negative'] = score\n",
    "                            elif category == '에너지':\n",
    "                                if score > 0:\n",
    "                                    processed.at[idx, 'positive_E'] = score\n",
    "                                elif score < 0:\n",
    "                                    processed.at[idx, 'negative_E'] = score\n",
    "                            elif category == '불안':\n",
    "                                processed.at[idx, 'anxiety'] = score\n",
    "                            elif category == '짜증':\n",
    "                                processed.at[idx, 'annoying'] = score\n",
    "\n",
    "                            match_count += 1\n",
    "                            processed_ids.add(pid)\n",
    "\n",
    "\n",
    "\n",
    "# 8. 기분 및 에너지 충돌 조정 (절대값 기준)\n",
    "mask_mood = processed['positive_feeling'].notna() & processed['negative'].notna()\n",
    "for idx in processed[mask_mood].index:\n",
    "    pos = processed.at[idx, 'positive_feeling']\n",
    "    neg = processed.at[idx, 'negative']\n",
    "    if abs(pos) > abs(neg):\n",
    "        processed.at[idx, 'negative'] = 0\n",
    "    elif abs(pos) < abs(neg):\n",
    "        processed.at[idx, 'positive_feeling'] = 0\n",
    "    else:\n",
    "        processed.at[idx, 'positive_feeling'] = 0  # 동일하면 긍정 제거, 부정 유지\n",
    "\n",
    "mask_energy = processed['positive_E'].notna() & processed['negative_E'].notna()\n",
    "for idx in processed[mask_energy].index:\n",
    "    posE = processed.at[idx, 'positive_E']\n",
    "    negE = processed.at[idx, 'negative_E']\n",
    "    if abs(posE) > abs(negE):\n",
    "        processed.at[idx, 'negative_E'] = 0\n",
    "    elif abs(posE) < abs(negE):\n",
    "        processed.at[idx, 'positive_E'] = 0\n",
    "    else:\n",
    "        processed.at[idx, 'positive_E'] = 0\n",
    "\n",
    "# 9. 값이 한쪽만 있을 경우 다른 쪽을 0으로 설정\n",
    "processed.loc[\n",
    "    processed['positive_feeling'].notna() & processed['negative'].isna(),\n",
    "    'negative'\n",
    "] = 0\n",
    "processed.loc[\n",
    "    processed['negative'].notna() & processed['positive_feeling'].isna(),\n",
    "    'positive_feeling'\n",
    "] = 0\n",
    "\n",
    "processed.loc[\n",
    "    processed['positive_E'].notna() & processed['negative_E'].isna(),\n",
    "    'negative_E'\n",
    "] = 0\n",
    "processed.loc[\n",
    "    processed['negative_E'].notna() & processed['positive_E'].isna(),\n",
    "    'positive_E'\n",
    "] = 0\n",
    "\n",
    "# 10. 디버그 출력 (최대 20개)\n",
    "for info in debug_info[:20]:\n",
    "    print(info)\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"questionnaire_panic_demo_mood.csv\")\n",
    "processed.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype, is_string_dtype, is_datetime64_any_dtype\n",
    "\n",
    "# 전체 컬럼 순회하며 형식 통일\n",
    "for col in processed.columns:\n",
    "    if col == 'ID':\n",
    "        processed[col] = processed[col].astype(str).str.strip()\n",
    "    elif col == 'date':\n",
    "        processed[col] = pd.to_datetime(processed[col], errors='coerce')  # 문자열 포함 시 자동 처리\n",
    "    elif is_datetime64_any_dtype(processed[col]):\n",
    "        processed[col] = pd.to_datetime(processed[col], errors='coerce')  # datetime이면 그대로\n",
    "    elif is_string_dtype(processed[col]):\n",
    "        processed[col] = processed[col].astype(str).str.strip()  # 문자열이면 정리\n",
    "    else:\n",
    "        # 예외적인 경우도 문자열로 통일\n",
    "        processed[col] = processed[col].astype(str).str.strip()\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"processed.csv\")\n",
    "processed.to_csv(output_path, index=False)\n",
    "print(\"모든 컬럼 형식 통일 완료 및 저장됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combined_nipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
