{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import os \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "from library.path_utils import get_file_path, to_absolute_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd71b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PXPN_DIR = \"./raw_data/PXPN\"\n",
    "# 엑셀 파일 경로 (실제 경로로 수정)\n",
    "enroll_file_name = \"1. 픽셀패닉 enroll 정보_250516\"\n",
    "zip_file_name = \"pixelpanic_raw_data.zip\"\n",
    "output_folder_name = \"./_tmp/PXPN\"\n",
    "result_folder_name = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6694285",
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_path = get_file_path(RAW_PXPN_DIR, f\"{enroll_file_name}.csv\")\n",
    "zip_path = get_file_path(RAW_PXPN_DIR, f\"{zip_file_name}\")\n",
    "output_folder = to_absolute_path(output_folder_name)\n",
    "result_folder = to_absolute_path(result_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27201fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) CSV 파일 로드 & 'Unnamed' 인덱스 컬럼 제거 함수\n",
    "def load_and_clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # Strip whitespace from column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "    return df\n",
    "\n",
    "\n",
    "output_path = os.path.join(output_folder, \"processed.csv\")\n",
    "preprocessed = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"bandpower_720.csv\")\n",
    "band_power            = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"circadian_delta_720.csv\")\n",
    "circadian_delta       = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"step_delta.csv\")\n",
    "step_delta = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"HR_date_fixed.csv\")\n",
    "HR_date               = load_and_clean(output_path)\n",
    "output_path = os.path.join(output_folder, \"sleep_type.csv\")\n",
    "sleep                 = load_and_clean(output_path)\n",
    "# (3) 날짜 기반 데이터 리스트\n",
    "date_dfs = [\n",
    "    preprocessed,\n",
    "    band_power,\n",
    "    circadian_delta,\n",
    "    step_delta,\n",
    "    HR_date,\n",
    "    sleep,\n",
    "]\n",
    "\n",
    "# (3.5) 모든 date 컬럼을 datetime 타입으로 변환\n",
    "for df in date_dfs:\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# (5) 모든 (ID, date) 조합을 마스터 키로 생성\n",
    "all_keys = pd.concat([df[['ID', 'date']] for df in date_dfs])\n",
    "all_keys = all_keys.drop_duplicates().dropna()\n",
    "\n",
    "# (6) 각 df를 마스터 키 기준으로 align\n",
    "def align_to_master(df):\n",
    "    return pd.merge(all_keys, df, how='left', on=['ID', 'date'])\n",
    "\n",
    "aligned_dfs = [align_to_master(df) for df in date_dfs]\n",
    "\n",
    "# (7) 모든 날짜별 테이블을 outer join 으로 순차 병합 (순서 영향 없음)\n",
    "merged_all = reduce(lambda left, right: pd.merge(left, right, how='outer', on=['ID', 'date']), aligned_dfs)\n",
    "\n",
    "\n",
    "merged_full = merged_all.sort_values(['ID','date'])\n",
    "\n",
    "# (11) 컬럼 정리 및 결측 처리\n",
    "merged_full.rename(columns={\n",
    "    'amp': 'HR_amplitude',           'mesor': 'HR_mesor',         'acr': 'HR_acrophase',\n",
    "    'amp_delta': 'HR_amplitude_difference',  'mesor_delta': 'HR_mesor_difference',   'acr_delta': 'HR_acrophase_difference',\n",
    "    'amp_delta2': 'HR_amplitude_difference_2d',  'mesor_delta2': 'HR_mesor_difference_2d',  'acr_delta2': 'HR_acrophase_difference_2d',\n",
    "    'step_max': 'steps_maximum',    'step_var': 'steps_variance',\n",
    "    'step_mean': 'steps_mean', \n",
    "    'negative': 'negative_feeling',\n",
    "    'bandpower_a': 'bandpower(0.001-0.0005Hz)', \n",
    "    'bandpower_b': 'bandpower(0.0005-0.0001Hz)',\n",
    "    'bandpower_c': 'bandpower(0.0001-0.00005Hz)', \n",
    "    'bandpower_d': 'bandpower(0.00005-0.00001Hz)',\n",
    "    'suicide_need_in_month': 'suicide_need'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# (12) date 컬럼이 datetime 타입인 경우, 문자열 YYYY-MM-DD로 변환\n",
    "if 'date' in merged_full.columns:\n",
    "    merged_full['date'] = merged_full['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# (13) 컬럼 순서: ID, date, panic → 나머지\n",
    "cols = merged_full.columns.tolist()\n",
    "ordered_cols = ['ID', 'date', 'panic'] + [c for c in cols if c not in ['ID', 'date', 'panic']]\n",
    "merged_full = merged_full[ordered_cols]\n",
    "\n",
    "output_path = os.path.join(output_folder, \"result_before_severity.csv\")\n",
    "merged_full.to_csv(output_path, index=False)\n",
    "\n",
    "# 1. Load all_data and prepare the 'severity' column\n",
    "all_data = pd.read_csv(output_path, dtype={\"ID\": str})\n",
    "\n",
    "# Ensure 'date' is in datetime.date format for matching\n",
    "all_data[\"date\"] = pd.to_datetime(all_data[\"date\"]).dt.date\n",
    "\n",
    "# Initialize a new column 'severity' with NaN\n",
    "all_data[\"severity\"] = pd.NA\n",
    "\n",
    "\n",
    "# 2. Fill 'severity' for PXPN-group patients by reading each patient's panic CSV inside the nested ZIP\n",
    "zip_path = zip_path\n",
    "with zipfile.ZipFile(zip_path, \"r\") as outer_zip:\n",
    "    # patient indices run from 6 to 40 (inclusive)\n",
    "    for i in range(6, 41):\n",
    "        formatted_index = f\"{i:02d}\"\n",
    "        patient_code = f\"PXPN_100{formatted_index}\"\n",
    "        inner_zip_name = f\"ActiveData/{patient_code}_ActiveData.zip\"\n",
    "\n",
    "        # Skip if the inner zip for this patient isn't present\n",
    "        if inner_zip_name not in outer_zip.namelist():\n",
    "            continue\n",
    "\n",
    "        # Read the inner ZIP from the outer ZIP into memory\n",
    "        inner_zip_bytes = BytesIO(outer_zip.read(inner_zip_name))\n",
    "        with zipfile.ZipFile(inner_zip_bytes, \"r\") as inner_zip:\n",
    "            inner_file_name = f\"{patient_code}_Panic.csv\"\n",
    "            if inner_file_name not in inner_zip.namelist():\n",
    "                continue\n",
    "\n",
    "            # Open the patient's panic CSV\n",
    "            with inner_zip.open(inner_file_name) as f:\n",
    "                df_panic = pd.read_csv(f, dtype={\"강도\": float})\n",
    "\n",
    "            # Convert the 작성일 column to datetime.date\n",
    "            df_panic[\"작성일\"] = pd.to_datetime(df_panic[\"작성일\"]).dt.date\n",
    "\n",
    "            # For each row in df_panic, match ID & date, then assign '강도' to all_data['severity']\n",
    "            for _, row in df_panic.iterrows():\n",
    "                panic_date = row[\"작성일\"]\n",
    "                severity_val = row[\"강도\"]\n",
    "\n",
    "                mask = (all_data[\"ID\"] == patient_code) & (all_data[\"date\"] == panic_date)\n",
    "                if mask.any():\n",
    "                    all_data.loc[mask, \"severity\"] = severity_val\n",
    "\n",
    "\n",
    "# 5. (Optional) Check how many rows still have NaN in severity\n",
    "num_missing = all_data[\"severity\"].isna().sum()\n",
    "print(f\"Number of rows with missing severity: {num_missing}\")\n",
    "\n",
    "# 6. Save the updated DataFrame\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "result_path = os.path.join(result_folder, \"PXPN_720.csv\")\n",
    "\n",
    "all_data.to_csv(result_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
