{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f4fd25",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5634a08",
   "metadata": {},
   "source": [
    "author:  `@cyshin971`  \n",
    "\n",
    "date:    `2025-07-14`  \n",
    "\n",
    "Instructions:\n",
    "- Run `data_preprocessing.ipynb`\n",
    "- Output can be found in `./data/`\n",
    "\n",
    "version: `3.1`\n",
    "\n",
    "> version `1.1`: preprocessing separated to `data_preprocessing.ipynb` (version `1.0`)  \n",
    "> version `2.0`: Updated to consensus on progress meeting `20250619`  \n",
    "> version `3.0`: Release Version  \n",
    "> -  version `3.1`: Directory organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '3-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42654e1",
   "metadata": {},
   "source": [
    "# ðŸ“š | Import Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf941b53",
   "metadata": {},
   "source": [
    "- `python` (`3.10`)\n",
    "- `pandas`  \n",
    "- `numpy`\n",
    "- `json`\n",
    "- `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "from library.pandas_utils import move_column, aggregate_by_column, create_empty_df, read_csv\n",
    "from library.text_utils import save_as_csv\n",
    "from library.json_utils import save_dict_to_file, load_dict_from_file\n",
    "from library.path_utils import get_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c430a",
   "metadata": {},
   "source": [
    "# âš™ï¸ | Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dev = False  # Set to True (Default: False) if you want to run the notebook in development mode\n",
    "manual_scraped_data_filename = None # Keep as None if you don't want to manually specify a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Notebook running with:\\n\" +\n",
    "             f\"mode: {'development' if is_dev else 'production'}\\n\" +\n",
    "             f\"manual_scraped_data_filename: {manual_scraped_data_filename}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa403ca",
   "metadata": {},
   "source": [
    "# ðŸ“ | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda073d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\"\n",
    "TMP_PATH = \"./_tmp\"\n",
    "OUT_PATH = TMP_PATH\n",
    "if is_dev: OUT_PATH = \"./_output\"\n",
    "file_desc = \"\"\n",
    "\n",
    "try:\n",
    "\tcurrent_config = load_dict_from_file(OUT_PATH, 'current_config')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {get_file_path(OUT_PATH, 'current_config.csv')}.\\nPlease run data_preprocessing.ipynb first.\")\n",
    "\n",
    "print(f\"Loaded current config with {len(current_config)} keys:\")\n",
    "scraped_data_filename = None\n",
    "for k, v in current_config.items():\n",
    "    if k == 'scraped_data_filename':\n",
    "        scraped_data_filename = v\n",
    "    elif k == 'preproc_version':\n",
    "        preproc_version = v\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "if is_dev and manual_scraped_data_filename is not None:\n",
    "\tmanual_scraped_data_filename = manual_scraped_data_filename.replace('.csv', '')  # Remove .csv if present\n",
    "\tif not get_file_path(OUT_PATH, f\"{manual_scraped_data_filename}\").is_dir():\n",
    "\t\traise FileNotFoundError(f\"Manually specified scraped data directory '{manual_scraped_data_filename}' does not exist in {OUT_PATH}.\")\n",
    "\tlogging.warning(f\"Using manually specified scraped_data_filename: {manual_scraped_data_filename}\")\n",
    "\tscraped_data_filename = manual_scraped_data_filename\n",
    "\n",
    "if scraped_data_filename is None:\n",
    "\traise ValueError(\"scraped_data_filename not found in current_config. Please ensure that data_preprocessing.ipynb has been run successfully before running this notebook.\")\n",
    "\n",
    "if is_dev:\n",
    "    OUTPUT_FILE_PATH = f\"{OUT_PATH}/{scraped_data_filename}\"\n",
    "    PREPROC_PATH = f\"{OUTPUT_FILE_PATH}/preprocessed\"\n",
    "    OUTPUT_PATH = f\"{OUTPUT_FILE_PATH}/analysis\"\n",
    "    file_desc = f\"_{version}({scraped_data_filename})\"\n",
    "else:\n",
    "    OUTPUT_FILE_PATH = DATA_PATH\n",
    "    PREPROC_PATH = DATA_PATH\n",
    "    OUTPUT_PATH = f\"{DATA_PATH}/analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024277b",
   "metadata": {},
   "source": [
    "# âš’ï¸ | Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe059b",
   "metadata": {},
   "source": [
    "### Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237181b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\tfeatures_dict = load_dict_from_file(OUTPUT_FILE_PATH, 'panic_features_dict')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {get_file_path(OUT_PATH, 'panic_features_dict.csv')}.\\nPlease run data_preprocessing.ipynb first.\")\n",
    "\n",
    "print(f\"Loaded features dict with {len(features_dict)} keys:\")\n",
    "for k, v in features_dict.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1f06",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = read_csv(get_file_path(PREPROC_PATH, f'panic_pre_data{file_desc}.csv'))\n",
    "display(pre_data.head(2))\n",
    "metadata = read_csv(get_file_path(PREPROC_PATH, f'panic_metadata{file_desc}.csv'))\n",
    "display(metadata.head(2))\n",
    "demography_data = read_csv(get_file_path(PREPROC_PATH, f'panic_demography_data{file_desc}.csv'))\n",
    "display(demography_data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5553a",
   "metadata": {},
   "source": [
    "# ðŸ” | Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c9af5",
   "metadata": {},
   "source": [
    "## ðŸ•³ï¸ | Null Value Analysis\n",
    "\n",
    "Produces csv file of the percentage of the null values for each feature in Daily Log, and Life Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f625e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value_analysis = create_empty_df()\n",
    "null_value_analysis = pre_data[features_dict['lifelog']+features_dict['dailylog']].isnull().sum().reset_index()\n",
    "null_value_analysis.rename(columns={0: 'null_count'}, inplace=True)\n",
    "null_value_analysis.rename(columns={'index': 'feature'}, inplace=True)\n",
    "\n",
    "# add a column for the data group using the feqtures_dict\n",
    "null_value_analysis['data_group'] = null_value_analysis['feature'].apply(\n",
    "    lambda x: 'mood' if x in features_dict['mood'] else\n",
    "\t\t\t  'dailylog_life' if x in features_dict['dailylog_life'] else\n",
    "\t\t\t  'lifelog_HR' if x in features_dict['lifelog_HR'] else\n",
    "\t\t\t  'lifelog_steps' if x in features_dict['lifelog_steps'] else\n",
    "\t\t\t  'lifelog_sleep' if x in features_dict['lifelog_sleep'] else\n",
    "\t\t\t  'other'\n",
    ")\n",
    "\n",
    "# add column for total count\n",
    "null_value_analysis['total_count'] = len(pre_data)\n",
    "# find the existing percentage\n",
    "null_value_analysis['null%'] = (null_value_analysis['null_count'] / null_value_analysis['total_count']) * 100\n",
    "\n",
    "move_column(null_value_analysis, 'data_group', 1)\n",
    "move_column(null_value_analysis, 'null%', 2)\n",
    "\n",
    "save_as_csv(null_value_analysis, OUTPUT_PATH, f\"panic_null_value_analysis{file_desc}\")\n",
    "\n",
    "display(null_value_analysis.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1615ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 10))  # Adjusted to make it thinner horizontally and longer vertically\n",
    "colors = ['skyblue'] * len(null_value_analysis)\n",
    "ax = null_value_analysis.sort_values('null%', ascending=True).plot(\n",
    "\tkind='barh', x='feature', y='null%', color=colors, legend=False, width=0.5  # Reduced bar width to increase gaps\n",
    ")\n",
    "plt.title(\"Percentage of Null Values by Feature\")\n",
    "plt.xlabel(\"Percentage of Null Values (%)\")\n",
    "plt.xlim(0, 100)  # Set the x-axis limit to 0-100\n",
    "# get rid of the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Adjust y-axis labels for better visibility\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=5, rotation=0, ha='right')\n",
    "\n",
    "# Add labels with values to the right of each bar\n",
    "for p in ax.patches:\n",
    "\tvalue = f\"{p.get_width():.2f}%\"\n",
    "\tax.annotate(value,\n",
    "\t\t\t\t(p.get_width() + 1, p.get_y() + p.get_height() / 2),\n",
    "\t\t\t\tha='left', va='center', fontsize=5, color='black', xytext=(0, 0),\n",
    "\t\t\t\ttextcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d403f88",
   "metadata": {},
   "source": [
    "## ðŸ¤¯ | Patient-level Analysis\n",
    "\n",
    "**Description**\n",
    "- `n_entries`: Number of entries per patient\n",
    "- `n_valid_3_entries`: Number of valid (3 prior consecutive days of data) entries per patient\n",
    "- `n_valid_2_entries`: Number of valid (2 prior consecutive days of data) entries per patient\n",
    "- `n_valid_1_entries`: Number of valid (1 prior consecutive days of data) entries per patient\n",
    "- `n_panic`: Number of panic events per patient\n",
    "- `max_severity`: Maximum panic severity experienced by patient\n",
    "- `min_severity`: Minimum panic severity experienced by patient\n",
    "- `mean_severity`: Average panic severity experienced by patient\n",
    "- `n_dailylog`: Number of daily log entries per patient\n",
    "- `n_lifelog`: Number of life log entries per patient\n",
    "- `n_questionnaire`: Number of questionnaire entries per patient\n",
    "- `sum_dtype`: Number of data (by group) per patient\n",
    "- `mean_dtype`: Average number of data types per entry per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b24e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('n_entries', 'entry_id', 'count'),\n",
    "\t('n_valid_3_entries', 'valid_entry_3', 'sum'),\n",
    "\t('n_valid_2_entries', 'valid_entry_2', 'sum'),\n",
    "\t('n_valid_1_entries', 'valid_entry_1', 'sum'),\n",
    "\t('n_panic', 'panic_label', 'sum'),\n",
    "    ('max_severity', 'severity', 'max'),\n",
    "\t('min_severity', 'severity', 'min'),\n",
    "\t('mean_severity', 'severity', 'mean'),\n",
    "\t('n_dailylog', 'dailylog_data', 'sum'),\n",
    "\t('n_lifelog', 'lifelog_data', 'sum'),\n",
    "\t('n_questionnaire', 'questionnaire_data', 'sum'),\n",
    "\t('sum_dtype', 'dtype_n', 'sum'),\n",
    "\t('mean_dtype', 'dtype_n', 'mean'),\n",
    "]\n",
    "patient_analysis_data = create_empty_df()\n",
    "patient_analysis_data = aggregate_by_column(metadata, 'ID', agg_matrix)\n",
    "\n",
    "display(patient_analysis_data.head(5))\n",
    "save_as_csv(patient_analysis_data, OUTPUT_PATH, f\"panic_patient_analysis{file_desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b23f4",
   "metadata": {},
   "source": [
    "## ðŸ¤¯ðŸ•³ï¸ | Patient-level Null Value Analysis\n",
    "\n",
    "Produces csv file of the percentage of the Null values for each feature in Daily Log, and Life Log for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [('n_entries', 'entry_id', 'count')]\n",
    "cols_to_analyze = features_dict['lifelog'] + features_dict['dailylog']\n",
    "\n",
    "for col in cols_to_analyze:\n",
    "    agg_matrix.append((col, col, 'count'))\n",
    "\n",
    "patient_null_analysis_data = create_empty_df()\n",
    "patient_null_analysis_data = aggregate_by_column(pre_data, 'ID', agg_matrix)\n",
    "\n",
    "for col in cols_to_analyze:\n",
    "    patient_null_analysis_data[col] = (patient_null_analysis_data['n_entries'] - patient_null_analysis_data[col]) / patient_null_analysis_data['n_entries'] * 100\n",
    "\n",
    "patient_null_analysis_data['lifelog_avg'] = patient_null_analysis_data[features_dict['lifelog']].mean(axis=1)\n",
    "patient_null_analysis_data['lifelog_HR_avg'] = patient_null_analysis_data[features_dict['lifelog_HR']].mean(axis=1)\n",
    "patient_null_analysis_data['lifelog_steps_avg'] = patient_null_analysis_data[features_dict['lifelog_steps']].mean(axis=1)\n",
    "patient_null_analysis_data['lifelog_sleep_avg'] = patient_null_analysis_data[features_dict['lifelog_sleep']].mean(axis=1)\n",
    "\n",
    "patient_null_analysis_data['dailylog_avg_avg'] = patient_null_analysis_data[features_dict['dailylog']].mean(axis=1)\n",
    "patient_null_analysis_data['dailylog_mood_avg'] = patient_null_analysis_data[features_dict['mood']].mean(axis=1)\n",
    "patient_null_analysis_data['dailylog_life_avg'] = patient_null_analysis_data[features_dict['dailylog_life']].mean(axis=1)\n",
    "\n",
    "patient_null_analysis_data['total_avg'] = patient_null_analysis_data[features_dict['lifelog'] + features_dict['dailylog']].mean(axis=1)\n",
    "\n",
    "display(patient_null_analysis_data.head(5))\n",
    "save_as_csv(patient_null_analysis_data, OUTPUT_PATH, f\"panic_patient_null_analysis{file_desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdba2a",
   "metadata": {},
   "source": [
    "# ðŸ“’ | Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf66dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.matplotlib_utils import plot_histogram_of_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b0af6",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f58181",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids =  pre_data['ID'].unique()\n",
    "print(f\"Total number of unique IDs in pre_data_filled: {len(unique_ids)}\")\n",
    "print(f\"Total number of entries in pre_data_filled: {len(pre_data)}\")\n",
    "panic_entries = pre_data[pre_data['panic_label'] == 1]\n",
    "print(f\"Total number of panic entries in pre_data_filled: {len(panic_entries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_entry_ids = pre_data['entry_id'].unique()\n",
    "print(f\"Total number of daily log entries: {metadata[metadata['dailylog_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata[metadata['dailylog_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of life log entries: {metadata[metadata['lifelog_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata[metadata['lifelog_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of questionnaire entries: {metadata[metadata['questionnaire_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata[metadata['questionnaire_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "panic_patients = metadata[metadata['panic_label'] == 1]['ID'].unique()\n",
    "print(f\"Total number of patients with panic events: {len(panic_patients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4ad05",
   "metadata": {},
   "source": [
    "## ðŸ—’ï¸ | Entry-level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b1ffa",
   "metadata": {},
   "source": [
    "### Data Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_counts(metadata['dtype_n'], title='Number of data types per entry', xlabel='Number of data types', ylabel='Number of entries', zero_start=False, bins_step=1)\n",
    "print(f\"Number of entries with 1 data type: {metadata[metadata['dtype_n'] == 1].shape[0]} ({metadata[metadata['dtype_n'] == 1].shape[0] / metadata.shape[0] * 100:.2f}%)\")\n",
    "print(f\"Number of entries with 2 data types: {metadata[metadata['dtype_n'] == 2].shape[0]} ({metadata[metadata['dtype_n'] == 2].shape[0] / metadata.shape[0] * 100:.2f}%)\")\n",
    "print(f\"Number of entries with 3 data types: {metadata[metadata['dtype_n'] == 3].shape[0]} ({metadata[metadata['dtype_n'] == 3].shape[0] / metadata.shape[0] * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b01fb",
   "metadata": {},
   "source": [
    "### Valid Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8301b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of valid entries (3 days): {metadata['valid_entry_3'].sum()} ({metadata['valid_entry_3'].mean() * 100:.2f}%)\")\n",
    "print(f\"Total number of valid entries (2 days): {metadata['valid_entry_2'].sum()} ({metadata['valid_entry_2'].mean() * 100:.2f}%)\")\n",
    "print(f\"Total number of valid entries (1 day): {metadata['valid_entry_1'].sum()} ({metadata['valid_entry_1'].mean() * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "panic_n = pre_data[pre_data['panic_label'] == 1].shape[0]\n",
    "print(f\"Total number of panic events (panic_label=1): {panic_n}\")\n",
    "valid_panic_events_3 = metadata[(metadata['valid_entry_3'] == 1) & (metadata['dbp'] == 0)].shape[0]\n",
    "print(f\"Total number of valid panic events (n_prior_data >= 3 days): {valid_panic_events_3} ({valid_panic_events_3 / panic_n * 100:.2f}%)\")\n",
    "valid_panic_events_2 = metadata[(metadata['valid_entry_2'] == 1) & (metadata['dbp'] == 0)].shape[0]\n",
    "print(f\"Total number of valid panic events (n_prior_data >= 2 days): {valid_panic_events_2} ({valid_panic_events_2 / panic_n * 100:.2f}%)\")\n",
    "valid_panic_events_1 = metadata[(metadata['valid_entry_1'] == 1) & (metadata['dbp'] == 0)].shape[0]\n",
    "print(f\"Total number of valid panic events (n_prior_data >= 1 day): {valid_panic_events_1} ({valid_panic_events_1 / panic_n * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faecf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dbp = metadata[metadata['dbp'] == 1]\n",
    "print(f\"Total number of entries with dbp=1: {one_dbp.shape[0]} ({one_dbp.shape[0] / metadata.shape[0] * 100:.2f}%)\")\n",
    "two_dvp = metadata[metadata['dbp'] == 2]\n",
    "print(f\"Total number of entries with dbp=2: {two_dvp.shape[0]} ({two_dvp.shape[0] / metadata.shape[0] * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff574743",
   "metadata": {},
   "source": [
    "## ðŸ¤¢ | Patient-level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_ids = pre_data['ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10e315",
   "metadata": {},
   "source": [
    "### Patient-level Valid Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patients with > 100 entries: {patient_analysis_data[patient_analysis_data['n_entries'] > 100].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_entries'] > 100].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "print(f\"Number of patients with <= 100 entries: {patient_analysis_data[patient_analysis_data['n_entries'] <= 100].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_entries'] <= 100].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "print(f\"Number of patients with > 30 entries: {patient_analysis_data[patient_analysis_data['n_entries'] > 30].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_entries'] > 30].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "print(f\"Number of patients with <= 30 entries: {patient_analysis_data[patient_analysis_data['n_entries'] <= 30].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_entries'] <= 30].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "plot_histogram_of_counts(patient_analysis_data['n_entries'], title='Number of Entries per Patient',\n",
    "                         xlabel='Number of Entries', ylabel='Number of Patients', bins_step=20, exclude_zero=True)\n",
    "valid_patients = patient_analysis_data[patient_analysis_data['n_valid_1_entries'] > 0]\n",
    "n_valid_patients_1 = valid_patients.shape[0]\n",
    "print(f\"Number of valid patients (1 days): {valid_patients.shape[0]} / {len(pre_data_ids)}\")\n",
    "print(f\"{len(pre_data_ids) - valid_patients.shape[0]} patients do not have valid entries (i.e., no data for at least 1 days before panic event)\")\n",
    "plot_histogram_of_counts(patient_analysis_data['n_valid_1_entries'], title='Number of Valid Entries (1 days) per Patient',\n",
    "                         xlabel='Number of Valid Entries (1 days)', ylabel='Number of Patients', bins_step=20, exclude_zero=True)\n",
    "valid_patients = patient_analysis_data[patient_analysis_data['n_valid_2_entries'] > 0]\n",
    "n_valid_patients_2 = valid_patients.shape[0]\n",
    "print(f\"Number of valid patients (2 days): {valid_patients.shape[0]} / {len(pre_data_ids)}\")\n",
    "print(f\"{len(pre_data_ids) - valid_patients.shape[0]} patients do not have valid entries (i.e., no data for at least 2 days before panic event)\")\n",
    "plot_histogram_of_counts(patient_analysis_data['n_valid_2_entries'], title='Number of Valid Entries (2 days) per Patient',\n",
    "                         xlabel='Number of Valid Entries (2 days)', ylabel='Number of Patients', bins_step=20, exclude_zero=True)\n",
    "valid_patients = patient_analysis_data[patient_analysis_data['n_valid_3_entries'] > 0]\n",
    "n_valid_patients_3 = valid_patients.shape[0]\n",
    "print(f\"Number of valid patients (3 days): {valid_patients.shape[0]} / {len(pre_data_ids)}\")\n",
    "print(f\"{len(pre_data_ids) - valid_patients.shape[0]} patients do not have valid entries (i.e., no data for at least 3 days before panic event)\")\n",
    "plot_histogram_of_counts(patient_analysis_data['n_valid_3_entries'], title='Number of Valid Entries (3 days) per Patient',\n",
    "                         xlabel='Number of Valid Entries (3 days)', ylabel='Number of Patients', bins_step=20, exclude_zero=True)\n",
    "del valid_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ed563",
   "metadata": {},
   "source": [
    "### Patient-level Panic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patients with > 2 number of panic events: {patient_analysis_data[patient_analysis_data['n_panic'] > 2].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_panic'] > 2].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "print(f\"Number of patients with <= 2 number of panic events: {patient_analysis_data[patient_analysis_data['n_panic'] <= 2].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_panic'] <= 2].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "print(f\"Number of patients with > 10 number of panic events: {patient_analysis_data[patient_analysis_data['n_panic'] > 10].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_panic'] > 10].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "print(f\"Number of patients with <= 10 number of panic events: {patient_analysis_data[patient_analysis_data['n_panic'] <= 10].shape[0]} / {len(pre_data_ids)} ({patient_analysis_data[patient_analysis_data['n_panic'] <= 10].shape[0] / len(pre_data_ids) * 100:.2f}%)\")\n",
    "plot_histogram_of_counts(patient_analysis_data['n_panic'], title='Number of Panic Events per Patient', xlabel='Number of Panic Events', ylabel='Number of Patients', bins_step=5,exclude_zero=False)\n",
    "print(f\"Number of patients with zero panic events: {patient_analysis_data[patient_analysis_data['n_panic'] == 0].shape[0]} / {len(pre_data_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29839359",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_panic_events_patients = patient_analysis_data[(patient_analysis_data['n_panic'] > 0) & (patient_analysis_data['n_valid_3_entries'] > 0)]\n",
    "print(f\"Number of patients with valid panic events (3 days): {valid_panic_events_patients.shape[0]} / {n_valid_patients_3}\")\n",
    "valid_panic_events_patients = patient_analysis_data[(patient_analysis_data['n_panic'] > 0) & (patient_analysis_data['n_valid_2_entries'] > 0)]\n",
    "print(f\"Number of patients with valid panic events (2 days): {valid_panic_events_patients.shape[0]} / {n_valid_patients_2}\")\n",
    "valid_panic_events_patients = patient_analysis_data[(patient_analysis_data['n_panic'] > 0) & (patient_analysis_data['n_valid_1_entries'] > 0)]\n",
    "print(f\"Number of patients with valid panic events (1 days): {valid_panic_events_patients.shape[0]} / {n_valid_patients_1}\")\n",
    "del valid_panic_events_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_with_panic = patient_analysis_data[patient_analysis_data['n_panic'] > 0]\n",
    "print(f\"Number of patients with panic events: {patients_with_panic.shape[0]} / {len(pre_data_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa2db7",
   "metadata": {},
   "source": [
    "### Patient-level Data Group Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bdc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_of_counts(patient_analysis_data['n_dailylog'], title='Number of Daily Log Entries per Patient',\n",
    "\t\t\t\t\t\t xlabel='Number of Daily Log Entries', ylabel='Number of Patients', bins_step=20, exclude_zero=True)\n",
    "print(f\"Number of patients with no daily log entries: {patient_analysis_data[patient_analysis_data['n_dailylog'] == 0].shape[0]} / {len(pre_data_ids)}\")\n",
    "plot_histogram_of_counts(patient_analysis_data['n_lifelog'], title='Number of Life Log Entries per Patient',\n",
    "\t\t\t\t\t\t xlabel='Number of Life Log Entries', ylabel='Number of Patients', bins_step=20, exclude_zero=True)\n",
    "print(f\"Number of patients with no life log entries: {patient_analysis_data[patient_analysis_data['n_lifelog'] == 0].shape[0]} / {len(pre_data_ids)}\")\n",
    "plot_histogram_of_counts(patient_analysis_data['n_questionnaire'], title='Number of Questionnaire Entries per Patient',\n",
    "\t\t\t\t\t\t xlabel='Number of Questionnaire Entries', ylabel='Number of Patients', bins_step=20, exclude_zero=True)\n",
    "print(f\"Number of patients with no questionnaire entries: {patient_analysis_data[patient_analysis_data['n_questionnaire'] == 0].shape[0]} / {len(pre_data_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_panic = metadata[metadata['panic_label'] == 1].copy()\n",
    "print(f\"Number of panic events in preprocessed data: {metadata_panic.shape[0]}\")\n",
    "agg_matrix = [\n",
    "\t('valid_entries_3', 'valid_entry_3', 'sum'),\n",
    "\t('valid_entries_2', 'valid_entry_2', 'sum'),\n",
    "\t('valid_entries_1', 'valid_entry_1', 'sum'),\n",
    "]\n",
    "agg_metadata_panic = aggregate_by_column(metadata_panic, 'ID', agg_matrix)\n",
    "\n",
    "print(f\"Number of valid panic entries (valid_entry_3): {agg_metadata_panic['valid_entries_3'].sum()}\")\n",
    "print(f\"Number of valid panic entries (valid_entry_2): {agg_metadata_panic['valid_entries_2'].sum()}\")\n",
    "print(f\"Number of valid panic entries (valid_entry_1): {agg_metadata_panic['valid_entries_1'].sum()}\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Number of patients with valid panic entries (valid_entry_3): {agg_metadata_panic[agg_metadata_panic['valid_entries_3'] > 0].shape[0]}\")\n",
    "print(f\"Number of patients with valid panic entries (valid_entry_2): {agg_metadata_panic[agg_metadata_panic['valid_entries_2'] > 0].shape[0]}\")\n",
    "print(f\"Number of patients with valid panic entries (valid_entry_1): {agg_metadata_panic[agg_metadata_panic['valid_entries_1'] > 0].shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panic_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
