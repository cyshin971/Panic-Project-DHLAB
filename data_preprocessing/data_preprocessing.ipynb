{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f4fd25",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5634a08",
   "metadata": {},
   "source": [
    "author:  `@cyshin971`  \n",
    "\n",
    "date:    `2025-07-14`  \n",
    "\n",
    "Instructions:\n",
    "- Scrape data (see `README` - `Instructions` - `Data Scraping`)  \n",
    "- Run the notebook (`Run All`) (may take 1 ~ 2 minutes)\n",
    "- preprocesssed data can be found in `./data/`\n",
    "\n",
    "version: `3.1`\n",
    "\n",
    "> version `1.0`: Derived from `data_analysis.ipynb` version `1.0`  \n",
    "> version `2.0`: Updated to consensus on progress meeting `20250619`  \n",
    "> version `3.0`: Release Version  \n",
    "> -  version `3.1`: Directory organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"3-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42654e1",
   "metadata": {},
   "source": [
    "# ðŸ“š | Import Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7205a14",
   "metadata": {},
   "source": [
    "Required Packages:\n",
    "- `python` (`3.10`)\n",
    "- `pandas`  \n",
    "- `numpy`\n",
    "- `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "from library.pandas_utils import move_column, remove_columns, aggregate_by_column, create_empty_df, read_csv\n",
    "from library.text_utils import save_as_csv\n",
    "from library.json_utils import save_dict_to_file\n",
    "from library.path_utils import get_file_path\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc0e3f",
   "metadata": {},
   "source": [
    "# âš™ï¸ | Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_filename = \"merged_df\" # (Default: \"scraped_data\") Name of the scraped data file without extension (.csv)\n",
    "\n",
    "save_removed_data = False  # Set to True (Default: False) if you want to save the removed data to TMP_PATH\n",
    "\n",
    "is_dev = False  # Set to True (Default: False) if you want to run the notebook in development mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_dev and scraped_data_filename != \"merged_df\":\n",
    "    raise ValueError(\"scraped_data_filename must be 'merged_df.csv' in production mode\")\n",
    "\n",
    "logging.info(f\"Notebook running with:\\n\" +\n",
    "             f\"preproc version: {version}\\n\" +\n",
    "             f\"scraped_data_filename = '{scraped_data_filename}.csv'\\n\" +\n",
    "             f\"mode = {('development' if is_dev else 'production')}\\n\" +\n",
    "             f\"save_removed_data = {save_removed_data}\\n\")\n",
    "\n",
    "current_config = {\n",
    "\t\"scraped_data_filename\": scraped_data_filename,\n",
    "    \"preproc_version\": version,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa403ca",
   "metadata": {},
   "source": [
    "# ðŸ“ | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\"\n",
    "TMP_PATH = \"./_tmp\"\n",
    "file_desc = \"\"\n",
    "if is_dev:\n",
    "\tOUT_PATH = f\"./_output\"\n",
    "\tOUT_FILE_PATH = f\"./_output/{scraped_data_filename}\"\n",
    "\tOUTPUT_PATH = f\"{OUT_FILE_PATH}/preprocessed\"\n",
    "\tfile_desc = f\"_{version}({scraped_data_filename})\"\n",
    "else:\n",
    "\tOUT_PATH = TMP_PATH\n",
    "\tOUT_FILE_PATH = DATA_PATH\n",
    "\tOUTPUT_PATH = DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbe0e9",
   "metadata": {},
   "source": [
    "# â›ï¸ | Scraped Data\n",
    "\n",
    "load preprocessed data (by `junyeol_lee`)\n",
    "- Each entry are the datapoints for a patient (`ID`) on a specific date (`date`)\n",
    "- If there were multiple datapoints for a specific date (`date`) for a specific patient (`ID`), the values were statistically processed (`sum`, `avg`, etc.) to a representation for the day\n",
    "- Questionnaire data was treated as a 'semi-trait' variable  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbe912",
   "metadata": {},
   "source": [
    "## Scraped Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = { \n",
    "\t\"demography\": [\n",
    "\t\t'gender', 'age', 'marriage', 'job', 'smkHx', 'drinkHx', 'suicideHx', 'suicide_need'\n",
    "\t],\n",
    "\t\"dailylog\": [\n",
    "\t\t'panic', 'severity', # NOTE: Caution when constructing dataset as these values are typically labels\n",
    "  \t\t'exercise', 'alcohol', 'coffee', 'menstruation', 'smoking',\n",
    "    \t'positive_feeling', 'negative_feeling', 'positive_E', 'negative_E', 'anxiety', 'annoying' # mood\n",
    "\t],\n",
    "\t\"mood\": [\n",
    "\t\t'positive_feeling', 'negative_feeling', 'positive_E', 'negative_E',\n",
    "\t\t'anxiety', 'annoying'\n",
    "\t],\n",
    "\t\"dailylog_life\": [\n",
    "\t\t'exercise', 'alcohol', 'coffee', 'menstruation', 'smoking'\n",
    "\t],\n",
    "\t\"lifelog\": [\n",
    "        'HR_var', 'HR_max', 'HR_mean', 'HR_hvar_mean', 'HR_acrophase', 'HR_amplitude', 'HR_mesor',\n",
    "        'HR_acrophase_difference', 'HR_acrophase_difference_2d', 'HR_amplitude_difference',\n",
    "        'HR_amplitude_difference_2d', 'HR_mesor_difference', 'HR_mesor_difference_2d',\n",
    "        'bandpower(0.001-0.0005Hz)', 'bandpower(0.0005-0.0001Hz)', 'bandpower(0.0001-0.00005Hz)', 'bandpower(0.00005-0.00001Hz)',\n",
    "        'steps', 'SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep',\n",
    "        'steps_maximum', 'steps_mean', 'step_hvar_mean', 'step_delta',\n",
    "        'step_max_delta', 'step_mean_delta', 'step_hvar_mean_delta',\n",
    "        'step_delta2', 'step_max_delta2', 'step_mean_delta2', 'step_hvar_mean_delta2', 'steps_variance'\n",
    "\t],\n",
    "\t\"lifelog_HR\": [\n",
    "        'HR_var', 'HR_max', 'HR_mean', 'HR_hvar_mean', 'HR_acrophase', 'HR_amplitude', 'HR_mesor',\n",
    "        'HR_acrophase_difference', 'HR_acrophase_difference_2d', 'HR_amplitude_difference',\n",
    "        'HR_amplitude_difference_2d', 'HR_mesor_difference', 'HR_mesor_difference_2d',\n",
    "        'bandpower(0.001-0.0005Hz)', 'bandpower(0.0005-0.0001Hz)', 'bandpower(0.0001-0.00005Hz)', 'bandpower(0.00005-0.00001Hz)',\n",
    "\t],\n",
    " \t\"lifelog_steps\": [\n",
    "\t\t'steps', 'steps_maximum', 'steps_mean', 'step_hvar_mean', 'step_delta',\n",
    "        'step_max_delta', 'step_mean_delta', 'step_hvar_mean_delta',\n",
    "        'step_delta2', 'step_max_delta2', 'step_mean_delta2', 'step_hvar_mean_delta2', 'steps_variance'\n",
    " \t],\n",
    "    \"lifelog_sleep\": [\n",
    "\t\t'SLT1', 'SLT2', 'SLT3', 'SLT4', 'SLT5', 'SLT6', 'total_sleep'\n",
    "    ],\n",
    "\t\"questionnaire\": [\n",
    "\t\t'PHQ_9', 'STAI_X2', 'CSM', 'CTQ_1', 'CTQ_2', 'CTQ_3', 'CTQ_4', 'CTQ_5', 'KRQ', 'MDQ',\n",
    "\t\t'ACQ', 'APPQ_1', 'APPQ_2', 'APPQ_3', 'BSQ', 'GAD_7', 'BRIAN'\n",
    "\t],\n",
    "\t\"excluded\": [ # Dropped as variables were only in SYM dataset\n",
    "\t\t'SPAQ_1', 'SPAQ_2', 'BFNE', 'CES_D', 'KOSSSF', 'SADS', 'STAI_X1', 'medication_in_month',\n",
    "        'Unnamed: 0' # Placeholder column\n",
    "\t],\n",
    "    \"id\": [\n",
    "        'ID', 'date'\n",
    "    ],\n",
    "    \"label\": [\n",
    "        'panic', 'severity'\n",
    "    ],\n",
    "    \"metadata\": []\n",
    "}\n",
    "\n",
    "demo_vars = features_dict[\"demography\"]\n",
    "dailylog_vars = features_dict[\"dailylog\"]\n",
    "lifelog_vars = features_dict[\"lifelog\"]\n",
    "questionnaire_vars = features_dict[\"questionnaire\"]\n",
    "\n",
    "state_vars = demo_vars\n",
    "trait_vars = dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "all_vars = state_vars + dailylog_vars + lifelog_vars + questionnaire_vars\n",
    "all_cols = features_dict[\"id\"] + all_vars\n",
    "\n",
    "print(f'Number of variables: {len(all_vars)}')\n",
    "print(f'   Demographic variables: {len(state_vars)}')\n",
    "print(f'   Daily log variables: {len(dailylog_vars)}')\n",
    "print(f'   Life log variables: {len(lifelog_vars)}')\n",
    "print(f'   Questionnaire variables: {len(questionnaire_vars)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81218b4a",
   "metadata": {},
   "source": [
    "## Load Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\tscraped_data = read_csv(get_file_path(DATA_PATH, scraped_data_filename + '.csv'))\n",
    "except FileNotFoundError:\n",
    "\traise FileNotFoundError(f\"File not found: {get_file_path(DATA_PATH, scraped_data_filename + '.csv')}. Please run data scraping first.\")\n",
    "\n",
    "# check if all columns are present\n",
    "missing_cols = [col for col in all_vars if col not in scraped_data.columns]\n",
    "if missing_cols:\n",
    "\tlogging.warning(f\"Missing columns in scraped_data: {missing_cols}\")\n",
    "else:\n",
    "\tlogging.info(\"All expected columns are present in scraped_data.\")\n",
    "extra_cols = [col for col in scraped_data.columns if col not in all_cols + features_dict[\"excluded\"]]\n",
    "if extra_cols:\n",
    "\tlogging.warning(f\"Extra columns in scraped_data: {extra_cols}\")\n",
    "\n",
    "# convert date column to datetime format\n",
    "scraped_data['date'] = pd.to_datetime(scraped_data['date'], format='%Y-%m-%d', errors='coerce')\n",
    "# remove_columns(scraped_data, ['Unnamed: 0'])\n",
    "\n",
    "# remove any of the columns in features_dict[\"excluded\"] if they exist\n",
    "for col in features_dict[\"excluded\"]:\n",
    "\tif col in scraped_data.columns:\n",
    "\t\tlogging.info(f\"Removing excluded column: {col}\")\n",
    "\t\tscraped_data.drop(columns=[col], inplace=True)\n",
    "\n",
    "print(f\"Number of rows: {scraped_data.shape[0]}\")\n",
    "print(f\"Number of columns: {scraped_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024277b",
   "metadata": {},
   "source": [
    "# âš’ï¸ | Data Preprocessing\n",
    "\n",
    "Changes from scraped data:\n",
    "- add `entry_id` to identify each entry: `'ID'_'date'`\n",
    "- add `dataset` to identify source: `SYM1`, `SYM2`, `PXPN`\n",
    "- convert `panic` (`0`, `1`, `2` = panic) to days before panic (`dbp`) (panic = `0`, `1`, `2`)\n",
    "  - dbp removed and only added in metdata (`20250626`)\n",
    "- add `panic_label` : whether a panic occurred in the entry (`boolean`)\n",
    "- demographic features were removed from preprocessed data (`data_pre`) and extracted\n",
    "- the data was filtered to remove entries with only demgraphic data (no `dailylog`, `lifelog`, `questionnaire`, or `diary` entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1f06",
   "metadata": {},
   "source": [
    "## Initialize Preprocessed Data\n",
    "\n",
    "- add `entry_id` to identify each entry: `'ID'_'date'`\n",
    "- add `dataset` to identify source: `SYM1`, `SYM2`, `PXPN`\n",
    "- add `panic_label` (boolean)\n",
    "- keep `panic` column instead of removing it (`20250617`)\n",
    "> If using `panic` column as a label this must be removed as a feature from final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_init = create_empty_df()\n",
    "data_pre_init = scraped_data.copy()\n",
    "\n",
    "# Add 'entry_id' column: unique identifier for each row\n",
    "data_pre_init['entry_id'] = data_pre_init['ID'] + '_' + data_pre_init['date'].astype(str)\n",
    "instance_id_unique = data_pre_init['entry_id'].unique()\n",
    "move_column(data_pre_init, 'entry_id', 0)\n",
    "print(\"Number of unique entry IDs:\", len(instance_id_unique))\n",
    "# Check if 'entry_id' is unique\n",
    "if data_pre_init['entry_id'].duplicated().any():\n",
    "\t# return the rows with duplicate 'entry_id'\n",
    "\tduplicates = data_pre_init[data_pre_init['entry_id'].duplicated(keep=False)]\n",
    "\tdisplay(duplicates.head(5))\n",
    "\tsave_as_csv(duplicates, TMP_PATH, f\"duplicates_{scraped_data_filename}\")\n",
    "\traise ValueError(\"Duplicate 'entry_id' found in the data. Please resolve this issue before proceeding.\")\n",
    "\n",
    "# Add 'dataset' column: source of data\n",
    "data_pre_init['dataset'] = data_pre_init['ID'].str.split('_').str[0]\n",
    "data_pre_init['dataset'] = data_pre_init['dataset'].str.split('-').str[0]\n",
    "move_column(data_pre_init, 'dataset', 1)\n",
    "\n",
    "# Add panic_label column\n",
    "data_pre_init['panic_label'] = data_pre_init['panic'].apply(lambda x: 1 if x == 2 else 0)\n",
    "\n",
    "# Update the features_dict\n",
    "if 'entry_id' not in features_dict['id']:\n",
    "\tfeatures_dict['id'].insert(0, 'entry_id')\n",
    "if 'dataset' not in features_dict['id']:\n",
    "\tfeatures_dict['id'].append('dataset')\n",
    "if 'panic_label' not in features_dict['label']:\n",
    "\tfeatures_dict['label'].append('panic_label')\n",
    "# Remove 'panic' from dailylog features (as it is a label) #NOTE: Need to remove as panic null values were filled with 0 in scraped_data\n",
    "if 'panic' in features_dict['dailylog']:\n",
    "\tfeatures_dict['dailylog'].remove('panic')\n",
    "\n",
    "# print scraped_data shape\n",
    "print(f\"Scraped data shape: {scraped_data.shape}\")\n",
    "print(f\"Initialized preprocessed data shape: {data_pre_init.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_pre_init.head(2))\n",
    "print(\"Unique sources in metadata_ljy: \", data_pre_init['dataset'].unique())\n",
    "print(\"Number of entries in metadata_ljy:\", data_pre_init.shape[0])\n",
    "sym1_n = data_pre_init[data_pre_init['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre_init[data_pre_init['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre_init[data_pre_init['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in metadata_ljy:\", len(data_pre_init['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre_init[data_pre_init['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre_init[data_pre_init['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre_init[data_pre_init['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (panic_label = 1):\", data_pre_init[data_pre_init['panic_label'] == 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cac54",
   "metadata": {},
   "source": [
    "## Initialize Metadata\n",
    "\n",
    "initialize `metadata` by adding\n",
    "- `demography_data` : whether demography data exists in the entry (`boolean`)\n",
    "- `dailylog_data`, `lifelog_data`, `questionnaire_data` : whether each data group exists in the entry (`boolean`)\n",
    "- `dtype_n` : how many of the 3 `state` groups exists in the entry (`int`)\n",
    "- `diary_data`: whether panic diary data group exists in the entry (`boolean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed181f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_init = create_empty_df()\n",
    "metadata_init = data_pre_init.copy()\n",
    "\n",
    "metadata_init['demography_data'] = metadata_init[features_dict['demography']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['dailylog_data'] = metadata_init[features_dict['dailylog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['lifelog_data'] = metadata_init[features_dict['lifelog']].notnull().any(axis=1).astype(int)\n",
    "metadata_init['questionnaire_data'] = metadata_init[features_dict['questionnaire']].notnull().any(axis=1).astype(int)\n",
    "\n",
    "metadata_init['dtype_n'] = metadata_init['dailylog_data'] + metadata_init['lifelog_data'] + metadata_init['questionnaire_data']\n",
    "move_column(metadata_init, 'dtype_n', 8)\n",
    "\n",
    "add_list = ['dailylog_data', 'lifelog_data', 'questionnaire_data', 'dtype_n']\n",
    "for item in add_list:\n",
    "\tif item not in features_dict['metadata']:\n",
    "\t\tfeatures_dict['metadata'].append(item)\n",
    "del add_list\n",
    "\n",
    "display(metadata_init.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccf3dc",
   "metadata": {},
   "source": [
    "## Extract Demography Data\n",
    "\n",
    "- All patients within the scraped data were confirmed to have demographic data (`demography_data` = `True`)\n",
    "- as such demography_data will not be included in the `metadata`\n",
    "- demographic features were removed from preprocessed data (`data_pre`)\n",
    "- Demography data was extracted and saved as `demography.csv` to the `output` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_matrix = [\n",
    "\t('gender_n', 'gender', 'nunique'),\n",
    "\t('age_n', 'age', 'nunique'),\n",
    "\t('marriage_n', 'marriage', 'nunique'),\n",
    "\t('job_n', 'job', 'nunique'),\n",
    "\t('smkHx_n', 'smkHx', 'nunique'),\n",
    "\t('drinkHx_n', 'drinkHx', 'nunique'),\n",
    "\t('suicideHx_n', 'suicideHx', 'nunique'),\n",
    "\t('suicide_need_n', 'suicide_need', 'nunique'),\n",
    "    ('gender', 'gender', 'first'),\n",
    "\t('age', 'age', 'first'),\n",
    "\t('marriage', 'marriage', 'first'),\n",
    "\t('job', 'job', 'first'),\n",
    "\t('smkHx', 'smkHx', 'first'),\n",
    "\t('drinkHx', 'drinkHx', 'first'),\n",
    "\t('suicideHx', 'suicideHx', 'first'),\n",
    "\t('suicide_need', 'suicide_need', 'first'),\n",
    "]\n",
    "demo_data = create_empty_df()\n",
    "demo_data = aggregate_by_column(metadata_init, 'ID', agg_matrix)\n",
    "\n",
    "# check if the length of each unique value is 1\n",
    "non_unique_cols = []\n",
    "for col in features_dict['demography']:\n",
    "\tif demo_data[col+'_n'].apply(lambda x: x > 1).any():\n",
    "\t\tnon_unique_cols.append(col)\n",
    "if non_unique_cols:\n",
    "\traise ValueError(f\"Demographic columns {non_unique_cols} are not unique for each ID in demo_data.\")\n",
    "else:\n",
    "\tprint(\"All demographic columns are unique for each ID in demo_data.\")\n",
    "\n",
    "for col in features_dict['demography']:\n",
    "\tremove_columns(demo_data, [col+'_n'])\n",
    "print(f\"Number of rows in demo_data: {demo_data.shape[0]}\")\n",
    "display(demo_data.head(5))\n",
    "\n",
    "save_as_csv(demo_data, OUTPUT_PATH, f\"panic_demography_data{file_desc}\")\n",
    "\n",
    "# Remove demographic features from data_proc\n",
    "remove_columns(data_pre_init, features_dict['demography'], ignore_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52f179",
   "metadata": {},
   "source": [
    "## Construct Intermediate Metadata\n",
    "- the current `metadata` (`metadata_init`) was filtered to include only columns for identification, added columns for metadata, and labels\n",
    "- the `metadata` was also filtered to get rid of all entries that only have demography data (`dtype_n` = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b509aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_int = create_empty_df()\n",
    "metadata_int = metadata_init.copy()\n",
    "\n",
    "metadata_int = metadata_int[features_dict['id'] + features_dict['metadata'] + features_dict['label']]\n",
    "move_column(metadata_int, 'severity', -1)\n",
    "move_column(metadata_int, 'panic_label', -1)\n",
    "metadata_int = metadata_int[metadata_int['dtype_n'] > 0]\n",
    "metadata_int = metadata_int[metadata_int['date'].notnull()]\n",
    "display(metadata_int.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c967c9",
   "metadata": {},
   "source": [
    "## Filter Preprocessed Data\n",
    "\n",
    "- the data was filtered to remove entries with only demgraphic data\n",
    "- the removed IDs were checked to see if no relevant entries were discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = create_empty_df()\n",
    "data_pre = data_pre_init.copy()\n",
    "\n",
    "# Filter data_proc to keep only rows with entry IDs present in metadata_int\n",
    "metadata_int_unique_ids = metadata_int['entry_id'].unique()\n",
    "data_pre = data_pre[data_pre['entry_id'].isin(metadata_int_unique_ids)]\n",
    "\n",
    "# remove rows with null dates\n",
    "data_pre = data_pre[data_pre['date'].notnull()]\n",
    "\n",
    "# Move label columns to the end\n",
    "move_column(data_pre, 'panic', -1)\n",
    "move_column(data_pre, 'severity', -1)\n",
    "move_column(data_pre, 'panic_label', -1)\n",
    "\n",
    "display(data_pre.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc84e1",
   "metadata": {},
   "source": [
    "## ðŸ’¾ | Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_pre to CSV\n",
    "save_as_csv(data_pre, OUTPUT_PATH, f\"panic_pre_data{file_desc}\")\n",
    "\n",
    "display(data_pre.head(3))\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Total entries in original: \", data_pre_init.shape[0])\n",
    "sym1_n = data_pre_init[data_pre_init['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre_init[data_pre_init['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre_init[data_pre_init['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in original:\", len(data_pre_init['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre_init[data_pre_init['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre_init[data_pre_init['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre_init[data_pre_init['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (panic_label = 1):\", data_pre_init[data_pre_init['panic_label'] == 1].shape[0])\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Total entries in filtered: \", data_pre.shape[0])\n",
    "sym1_n = data_pre[data_pre['dataset'] == 'SYM1'].shape[0]\n",
    "sym2_n = data_pre[data_pre['dataset'] == 'SYM2'].shape[0]\n",
    "print(\"    SYM entries:\", sym1_n+sym2_n)\n",
    "print(\"    PXPN entries:\", data_pre[data_pre['dataset'] == 'PXPN'].shape[0])\n",
    "print(\"Number of unique IDs in filtered:\", len(data_pre['ID'].unique()))\n",
    "# find the unique IDs for SYM1 and SYM2\n",
    "sym1_ids = data_pre[data_pre['dataset'] == 'SYM1']['ID'].unique()\n",
    "sym2_ids = data_pre[data_pre['dataset'] == 'SYM2']['ID'].unique()\n",
    "print(\"    SYM IDs: \", len(sym1_ids)+len(sym2_ids))\n",
    "print(\"    PXPN IDs: \", len(data_pre[data_pre['dataset'] == 'PXPN']['ID'].unique()))\n",
    "print(\"Number of panic events (panic_label = 1):\", data_pre[data_pre['panic_label'] == 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e6270",
   "metadata": {},
   "source": [
    "# ðŸ“– | Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a16be3",
   "metadata": {},
   "source": [
    "**Description**\n",
    "- `entry_id`: ID for each entry `'ID'_'date'`\n",
    "- `ID`: ID for each patient\n",
    "- `date`: logging date of each entry\n",
    "- `dataset`: source of entry (`SYM1`, `SYM2`, `PXPN`)\n",
    "- `dailylog_data`: whether daily log data exists in the entry (`boolean`)\n",
    "- `lifelog_data`: whether life log data exists in the entry (`boolean`)\n",
    "- `questionnaire_data`: whether questionnaire data exists in the entry (`boolean`)\n",
    "- `dtype_n`: how many of the 3 `state` groups exists in the entry (`int`)\n",
    "- `diary_data`: whether panic diary data exists in the entry (`boolean`)\n",
    "- `dbp`: number of consecutive days prior to panic. i.e. panic day = 0; 1 day prior = 1; etc. (up to 3)\n",
    "- `n_prior_data`: number of existing consecutive prior (days) entries\n",
    "- `ref_event_id`: the `entry_id` to which days before panic (`dbp`) is referencing\n",
    "- `valid_entry_3`: whether the entry has 3 consecutive days of prior data (`n_prior_data`)\n",
    "- `valid_entry_2`: whether the entry has 2 consecutive days of prior data (`n_prior_data`)\n",
    "- `valid_entry_1`: whether the entry has 1 consecutive days of prior data (`n_prior_data`)\n",
    "- `panic_label`: whether a panic occured in the entry (`boolean`)\n",
    "- `severity`: severity of the panic (1 ~ 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e7f97",
   "metadata": {},
   "source": [
    "## Calculate Days Before Panic (``dbp``) and Prior Consecutive Days (``n_prior_data``)\n",
    "\n",
    "- calculate the consecutive 'days before panic' (`dbp`):\n",
    "  - day when panic occured -> `dbp` = 0\n",
    "  - 1 day before panic -> `dbp` = 1\n",
    "  - 2 day before panic -> `dbp` = 2\n",
    "  - 3 day before panic -> `dbp` = 3 (etc)\n",
    "  - stop calculating at a set limit (`delta_days`) or if a panic occurred within the limit\n",
    "- calculate the number of existing prior consecutive (days) entries (`n_prior_data`) (Default: 3)\n",
    "  - stop calculating at a certain limit (`lookback_limit`) (Default: 7)\n",
    "\n",
    "> May take ~ 1 to 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preproc_utils import process_calculate_days_before_panic\n",
    "\n",
    "metadata_calc = create_empty_df()\n",
    "metadata_calc = metadata_int.copy()\n",
    "\n",
    "metadata_calc['n_prior_data']    = None\n",
    "metadata_calc['ref_event_id']    = None\n",
    "metadata_calc['dbp'] = None  # days before panic\n",
    "move_column(metadata_calc, 'panic_label', -1)\n",
    "move_column(metadata_calc, 'severity', -1)\n",
    "metadata_calc.sort_values(by=['ID', 'date'], ascending=False, inplace=True)\n",
    "\n",
    "d_days = 3\n",
    "l_back_lim = 7\n",
    "\n",
    "metadata_int = process_calculate_days_before_panic(metadata_calc, delta_days=d_days, lookback_limit=l_back_lim)\n",
    "\n",
    "# update features_dict with metadata columns\n",
    "if 'ref_event_id' not in features_dict['metadata']:\n",
    "\tfeatures_dict['metadata'].append('ref_event_id')\n",
    "if 'n_prior_data' not in features_dict['metadata']:\n",
    "\tfeatures_dict['metadata'].append('n_prior_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded18a26",
   "metadata": {},
   "source": [
    "## Find Valid Entries\n",
    "- add `valid_entry_3`: whether the entry has 3 consecutive days of prior data (`n_prior_data`)\n",
    "- add `valid_entry_2`: whether the entry has 2 consecutive days of prior data (`n_prior_data`)\n",
    "- add `valid_entry_1`: whether the entry has 1 consecutive days of prior data (`n_prior_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_int['valid_entry_3'] = metadata_int.apply(\n",
    "\tlambda row: 1 if row['n_prior_data'] >= 3 else 0,\n",
    "\taxis=1\n",
    ")\n",
    "metadata_int['valid_entry_2'] = metadata_int.apply(\n",
    "\tlambda row: 1 if row['n_prior_data'] >= 2 else 0,\n",
    "\taxis=1\n",
    ")\n",
    "metadata_int['valid_entry_1'] = metadata_int.apply(\n",
    "\tlambda row: 1 if row['n_prior_data'] >= 1 else 0,\n",
    "\taxis=1\n",
    ")\n",
    "move_column(metadata_int, 'ref_event_id', -1)\n",
    "move_column(metadata_int, 'panic_label', -1)\n",
    "move_column(metadata_int, 'severity', -1)\n",
    "display(metadata_int.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ff9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for panic labeling consistency\n",
    "# Panic events should have dbp = 0, panic = 2\n",
    "test_panic_dbpnot0 = metadata_int[(metadata_int['panic'] == 2) & (metadata_int['dbp'] != 0)]['entry_id'].unique()\n",
    "test_panic_dbp1 = metadata_int[(metadata_int['panic'] == 1) & (metadata_int['dbp'] != 1)]['entry_id'].unique()\n",
    "if len(test_panic_dbpnot0) != 0:\n",
    "\traise ValueError(\"Entries found with dbp != 0 for panic events. Please check the data.\")\n",
    "if len(test_panic_dbp1) != 0:\n",
    "\traise ValueError(\"Entries found with dbp != 1 for panic = 1. Please check the data.\")\n",
    "del test_panic_dbpnot0, test_panic_dbp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dea337",
   "metadata": {},
   "source": [
    "## ðŸ’¾ | Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69eeb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = create_empty_df()\n",
    "metadata = metadata_int.copy()\n",
    "\n",
    "save_as_csv(metadata, OUTPUT_PATH, f\"panic_metadata{file_desc}\")\n",
    "save_dict_to_file(features_dict, OUT_FILE_PATH, \"panic_features_dict\")\n",
    "\n",
    "display(metadata.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489556bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_file(current_config, OUT_PATH, \"current_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5553a",
   "metadata": {},
   "source": [
    "# ðŸ” | Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_unique_ids = scraped_data['ID'].unique()\n",
    "data_pre_unique_ids = data_pre['ID'].unique()\n",
    "print(f\"Scraped Unique IDs: {len(scraped_unique_ids)} -> {len(data_pre_unique_ids)} after preprocessing. discarded {len(scraped_unique_ids) - len(data_pre_unique_ids)} IDs.\")\n",
    "scraped_data_n = len(scraped_data)\n",
    "data_pre_entry_ids = data_pre['entry_id'].unique()\n",
    "print(f\"Scraped Entries: {scraped_data_n} -> {len(data_pre_entry_ids)} after preprocessing. discarded {scraped_data_n - len(data_pre_entry_ids)} entries.\")\n",
    "scraped_panic_events = scraped_data[scraped_data['panic'] == 2].shape[0]\n",
    "data_pre_panic_events = data_pre[data_pre['panic'] == 2].shape[0]\n",
    "data_pre_dbp_panic_events = metadata[metadata['dbp'] == 0].shape[0]\n",
    "data_pre_label_panic_events = data_pre[data_pre['panic_label'] == 1].shape[0]\n",
    "if data_pre_dbp_panic_events != data_pre_panic_events:\n",
    "\traise ValueError(\"Mismatch in panic events count: dbp panic events and panic events do not match.\")\n",
    "if data_pre_label_panic_events != data_pre_panic_events:\n",
    "\traise ValueError(\"Mismatch in panic events count: label panic events and panic events do not match.\")\n",
    "print(f\"Scraped Panic Events: {scraped_panic_events} -> {data_pre_panic_events} after preprocessing. discarded {scraped_panic_events - data_pre_panic_events} panic events.\")\n",
    "\n",
    "if save_removed_data:\n",
    "\t# find the entry_ids in scraped_data that are not in pre_data\n",
    "\tmissing_entry_ids = set(data_pre_init['entry_id']) - set(data_pre['entry_id'])\n",
    "\tif len(missing_entry_ids) > 0:\n",
    "\t\tlogging.info(f\"Saving removed data... {len(missing_entry_ids)} missing entry IDs\")\n",
    "\t\tremoved_data = data_pre_init[data_pre_init['entry_id'].isin(missing_entry_ids)]\n",
    "\t\tsave_as_csv(removed_data, TMP_PATH, f\"removed_data_{scraped_data_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb68775",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_entry_ids = data_pre['entry_id'].unique()\n",
    "print(f\"Total number of daily log entries: {metadata[metadata['dailylog_data'] == 1].shape[0]} / {len(data_pre_entry_ids)} ({metadata[metadata['dailylog_data'] == 1].shape[0] / len(data_pre_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of life log entries: {metadata[metadata['lifelog_data'] == 1].shape[0]} / {len(data_pre_entry_ids)} ({metadata[metadata['lifelog_data'] == 1].shape[0] / len(data_pre_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of questionnaire entries: {metadata[metadata['questionnaire_data'] == 1].shape[0]} / {len(data_pre_entry_ids)} ({metadata[metadata['questionnaire_data'] == 1].shape[0] / len(data_pre_entry_ids) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28531a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "panic_patients = metadata[metadata['panic'] == 2]['ID'].unique()\n",
    "print(f\"Total number of panic patients: {len(panic_patients)}\")\n",
    "panic_1_entries = metadata[metadata['panic'] == 1].shape[0]\n",
    "dbp_1_entries = metadata[metadata['dbp'] == 1].shape[0]\n",
    "if panic_1_entries != dbp_1_entries:\n",
    "\traise ValueError(\"Mismatch in panic entries count: panic entries and dbp entries do not match.\")\n",
    "if len(metadata) != len(data_pre):\n",
    "    raise ValueError(\"Error\")\n",
    "print(f\"Total number of entries with dbp = 1 (panic = 1): {panic_1_entries} / {len(data_pre_entry_ids)} ({panic_1_entries / len(data_pre_entry_ids) * 100:.2f}%)\")\n",
    "dbp_2_entries = metadata[metadata['dbp'] == 2].shape[0]\n",
    "print(f\"Total number of entries with dbp = 2: {dbp_2_entries} / {len(data_pre_entry_ids)} ({dbp_2_entries / len(data_pre_entry_ids) * 100:.2f}%)\")\n",
    "dbp_3_entries = metadata[metadata['dbp'] == 3].shape[0]\n",
    "print(f\"Total number of entries with dbp = 3: {dbp_3_entries} / {len(data_pre_entry_ids)} ({dbp_3_entries / len(data_pre_entry_ids) * 100:.2f}%)\")\n",
    "panic_0_entries = metadata[metadata['panic'] == 0].shape[0]\n",
    "print(f\"Total number of entries with panic = 0: {panic_0_entries} / {len(data_pre_entry_ids)} ({panic_0_entries / len(data_pre_entry_ids) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ff081",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scraped data shape:\", scraped_data.shape)\n",
    "display(scraped_data.head(2))\n",
    "print(\"Data preprocessed shape:\", data_pre.shape)\n",
    "display(data_pre.head(2))\n",
    "print(\"Metadata shape:\", metadata.shape)\n",
    "display(metadata.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
