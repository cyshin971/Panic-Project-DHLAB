{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f4fd25",
   "metadata": {},
   "source": [
    "# Panic Project (DHLAB) - Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5634a08",
   "metadata": {},
   "source": [
    "author:  `@cyshin971`  \n",
    "\n",
    "date:    `2025-07-xx`  \n",
    "\n",
    "version: `3.0`\n",
    " \n",
    "> version `2.0`: imputation separated from `data_analysis.ipynb` (version `1.3`)  \n",
    "> version `3.0`: Release Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '3-0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42654e1",
   "metadata": {},
   "source": [
    "# üìö | Import Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2014ac0",
   "metadata": {},
   "source": [
    "Required Packages:\n",
    "- `python` (`3.10`)\n",
    "- `pandas`  \n",
    "- `numpy`\n",
    "- `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from library.pandas_utils import create_empty_df, read_csv\n",
    "from library.text_utils import save_as_csv\n",
    "from library.json_utils import save_dict_to_file, load_dict_from_file\n",
    "from library.path_utils import get_file_path\n",
    "\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a73f4",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è | Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_scraped_data_filename = None # Keep as None if you don't want to manually specify a file\n",
    "\n",
    "use_growing_avg = True # Default: True\n",
    "# True: will use a growing average for the patient, i.e. the average of all previous entries\n",
    "# False: will use a fixed average for the patient, i.e. the average of all entries for that patient\n",
    "\n",
    "null_default_zero = True # Default: True\n",
    "# True: will replace null values with no existing entries for that patient with 0\n",
    "# False: will replace null values with no existing entries for that patient with the patient average or global average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa403ca",
   "metadata": {},
   "source": [
    "# üìÅ | Path Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_PATH = \"./_tmp\"\n",
    "OUT_PATH = \"./_output\"\n",
    "\n",
    "try:\n",
    "\tfeatures_dict = load_dict_from_file(OUT_PATH, 'panic_features_dict')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {get_file_path(OUT_PATH, 'panic_features_dict')}. Please run data_preprocessing.ipynb first.\")\n",
    "print(f\"Loaded features dict with {len(features_dict)} keys:\")\n",
    "scraped_data_filename = None\n",
    "for k, v in features_dict.items():\n",
    "    if k == 'scraped_data_filename':\n",
    "        print(f\"  {k}: {v}.csv\")\n",
    "        scraped_data_filename = v\n",
    "    elif k == 'preproc_version':\n",
    "        preproc_version = v\n",
    "        print(f\"  {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "if scraped_data_filename is None:\n",
    "\traise ValueError(\"scraped_data_filename not found in features_dict. Please ensure that data_preprocessing.ipynb has been run successfully before running this notebook.\")\n",
    "if manual_scraped_data_filename is not None:\n",
    "    logging.warning(f\"Using manually specified scraped_data_filename: {manual_scraped_data_filename}. If this is not intended, please set it to None.\")\n",
    "    scraped_data_filename = manual_scraped_data_filename\n",
    "\n",
    "features_dict['imputation_version'] = version\n",
    "features_dict['use_growing_avg'] = use_growing_avg\n",
    "features_dict['null_default_zero'] = null_default_zero\n",
    "save_dict_to_file(features_dict, OUT_PATH, 'panic_features_dict')\n",
    "\n",
    "PREPROC_PATH = f\"{OUT_PATH}/{scraped_data_filename}/preprocessed\"\n",
    "OUTPUT_PATH = f\"{OUT_PATH}/{scraped_data_filename}/imputed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024277b",
   "metadata": {},
   "source": [
    "# ‚öíÔ∏è | Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1f06",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = read_csv(get_file_path(PREPROC_PATH, f'panic_pre_data_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(pre_data.head(2))\n",
    "metadata = read_csv(get_file_path(PREPROC_PATH, f'panic_metadata_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(metadata.head(2))\n",
    "demography_data = read_csv(get_file_path(PREPROC_PATH, f'panic_demography_data_{preproc_version}({scraped_data_filename}).csv'))\n",
    "display(demography_data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdce151",
   "metadata": {},
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9355d",
   "metadata": {},
   "source": [
    "## Questionnaire\n",
    "\n",
    "- The null values were forward filled from the first value in the scraped data\n",
    "- the entries with null values prior to the first value was backward filled\n",
    "- Subsequent entries were forward filled\n",
    "- other null values were set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_filled_questionnaire = create_empty_df()\n",
    "pre_data_filled_questionnaire = pre_data.copy()\n",
    "\n",
    "pre_data_filled_questionnaire.sort_values(by=['ID', 'date'], inplace=True)\n",
    "\n",
    "for qcol in features_dict['questionnaire']:\n",
    "    pre_data_filled_questionnaire[qcol] = (\n",
    "        pre_data_filled_questionnaire.groupby('ID')[qcol]\n",
    "            .apply(lambda s: s.ffill().bfill())\n",
    "            .fillna(0)\n",
    "            .values\n",
    "    )\n",
    "\n",
    "pre_data_filled_questionnaire.sort_values(by=['ID', 'date'], inplace=True)\n",
    "disp_df = pre_data_filled_questionnaire[pre_data_filled_questionnaire['ID'] == 'SYM1-1-380'].copy()\n",
    "display(disp_df[features_dict['id']+features_dict['questionnaire']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d45e2",
   "metadata": {},
   "source": [
    "## Growing Average Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53ac46",
   "metadata": {},
   "source": [
    "- Initial fill: All NaNs get 0.\n",
    "- By ID: For each ID, process their entries in chronological order (date).\n",
    "- Running average: Each time a valid value is encountered, add it to the running sum/count.\n",
    "- For every NaN (now 0, but you detect it with the original mask), fill with running average so far.\n",
    "- No values for that ID: Remain at 0 (since running_count stays at 0).\n",
    "- After each value: The average is updated so the imputation logic matches your requirements.\n",
    "- No backward fill: Only forward imputation based on past data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3281c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imputation_utils import growing_average_impute\n",
    "\n",
    "pre_data_filled_avg = create_empty_df()\n",
    "pre_data_filled_avg = pre_data_filled_questionnaire.copy()\n",
    "\n",
    "avg_features = features_dict['lifelog'] + features_dict['mood']\n",
    "\n",
    "if use_growing_avg:\n",
    "\tlogging.info(\"Using growing average imputation.\")\n",
    "\tpre_data_filled_avg = growing_average_impute(\n",
    "\t\tpre_data_filled_avg, 'ID', avg_features, default_fill_zero=null_default_zero\n",
    "\t)\n",
    "else: \n",
    "\tlogging.info(\"Using fixed average imputation.\")\n",
    "\tfor col in avg_features:\n",
    "\t\tpre_data_filled_avg[col] = pre_data_filled_avg.groupby('ID')[col].transform(\n",
    "\t\t\tlambda x: x.fillna(x.mean())\n",
    "\t\t)\n",
    "\t\tif null_default_zero:\n",
    "\t\t\tpre_data_filled_avg[col].fillna(0, inplace=True)\n",
    "\t\telse:\n",
    "\t\t\tpre_data_filled_avg[col].fillna(pre_data_filled_avg[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d404d",
   "metadata": {},
   "source": [
    "## Null Value 0\n",
    "\n",
    "- Null values set to `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d609e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_null_features = features_dict['dailylog_life']\n",
    "\n",
    "pre_data_filled_zero_null = create_empty_df()\n",
    "pre_data_filled_zero_null = pre_data_filled_avg.copy()\n",
    "\n",
    "# Fill Null Values\n",
    "pre_data_filled_zero_null.loc[:, zero_null_features] = pre_data_filled_zero_null.loc[:, zero_null_features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c8ada",
   "metadata": {},
   "source": [
    "## üíæ | Save Filled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c34026",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_filled = create_empty_df()\n",
    "pre_data_filled = pre_data_filled_zero_null.copy()\n",
    "\n",
    "save_as_csv(pre_data_filled, OUTPUT_PATH, f\"panic_pre_data_filled_{version}({scraped_data_filename})_{('grw' if use_growing_avg else 'avg')}_{('zero' if null_default_zero else 'global')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb7bee",
   "metadata": {},
   "source": [
    "## Filled Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df884ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filled = create_empty_df()\n",
    "metadata_filled = metadata.copy()\n",
    "if len(pre_data_filled) != len(metadata):\n",
    "    raise ValueError(f\"Length of pre_data_filled ({len(pre_data_filled)}) does not match length of metadata ({len(metadata)}). Please check the data consistency.\")\n",
    "none_columns = ['dailylog_data', 'lifelog_data', 'questionnaire_data', 'dtype_n']\n",
    "for col in none_columns:\n",
    "    metadata_filled[col] = None\n",
    "\n",
    "metadata_filled['dailylog_data'] = pre_data_filled[features_dict['dailylog']].notnull().any(axis=1).astype(int)\n",
    "metadata_filled['lifelog_data'] = pre_data_filled[features_dict['lifelog']].notnull().any(axis=1).astype(int)\n",
    "metadata_filled['questionnaire_data'] = pre_data_filled[features_dict['questionnaire']].notnull().any(axis=1).astype(int)\n",
    "\n",
    "# TODO: Diary data is not used in the current analysis, but can be useful for future reference\n",
    "metadata_filled['dtype_n'] = metadata_filled['dailylog_data'] + metadata_filled['lifelog_data'] + metadata_filled['questionnaire_data']\n",
    "\n",
    "save_as_csv(metadata_filled, OUTPUT_PATH, f\"panic_metadata_filled_{version}({scraped_data_filename})_{('grw' if use_growing_avg else 'avg')}_{('zero' if null_default_zero else 'global')}\")\n",
    "display(metadata_filled.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdba2a",
   "metadata": {},
   "source": [
    "# üìí | Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f58181",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filled_ids =  pre_data_filled['ID'].unique()\n",
    "print(f\"Total number of unique IDs in pre_data_filled: {len(unique_filled_ids)}\")\n",
    "print(f\"Total number of entries in pre_data_filled: {len(pre_data_filled)}\")\n",
    "panic_entries = pre_data_filled[pre_data_filled['panic_label'] == 1]\n",
    "print(f\"Total number of panic entries in pre_data_filled: {len(panic_entries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_entry_ids = pre_data_filled['entry_id'].unique()\n",
    "print(f\"Total number of daily log entries: {metadata_filled[metadata_filled['dailylog_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata_filled[metadata_filled['dailylog_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of life log entries: {metadata_filled[metadata_filled['lifelog_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata_filled[metadata_filled['lifelog_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")\n",
    "print(f\"Total number of questionnaire entries: {metadata_filled[metadata_filled['questionnaire_data'] == 1].shape[0]} / {len(filled_entry_ids)} ({metadata_filled[metadata_filled['questionnaire_data'] == 1].shape[0] / len(filled_entry_ids) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "panic_patients = metadata_filled[metadata_filled['panic_label'] == 1]['ID'].unique()\n",
    "print(f\"Total number of patients with panic events: {len(panic_patients)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhlab-panic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
